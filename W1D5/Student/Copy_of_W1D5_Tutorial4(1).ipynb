{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W1D5_Tutorial4",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4af6fceWDA7"
      },
      "source": [
        "# Neuromatch Academy: Week 1, Day 5, Tutorial 4\n",
        "# Dimensionality Reduction: Nonlinear dimensionality reduction\n",
        "\n",
        "__Content creators:__ Alex Cayco Gajic, John Murray\n",
        "\n",
        "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnVe1mWoWDA7"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "In this notebook we'll explore how dimensionality reduction can be useful for visualizing and inferring structure in your data. To do this, we will compare PCA with t-SNE, a nonlinear dimensionality reduction method.\n",
        "\n",
        "Overview:\n",
        "- Visualize MNIST in 2D using PCA.\n",
        "- Visualize MNIST in 2D using t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Qvh5DaSSWDA8"
      },
      "source": [
        "# @title Video 1: PCA Applications\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"2Zb93aOWioM\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1wj2EWiQooI"
      },
      "source": [
        "# Summary of Video 1:\n",
        "\n",
        "- nonlinear methods of dimensionality reduction techniques.\n",
        "\n",
        "the big picture of PCA\n",
        "is that it finds a low dimensional basis that describes most of the variability in your data. We often call the subspace \n",
        "that's generated by this basis the latent subspace and after the transformation we often call this the latent representation.\n",
        "Latent means hidden because these components are not directly observed in the data, but they're inferred from the structure.\n",
        "\n",
        "PCA has a number of different applications:\n",
        "1. can be used for compression.\n",
        "That's related to the fact that PCA has an alternate formulation as finding the low dimensional basis that minimizes the\n",
        "reconstruction error. So instead of saving your full data matrix X you can save the truncated scores matrix and the corresponding coefficients.\n",
        "\n",
        "2. PCA can be useful for denoising. When you project onto the low\n",
        "dimensional subspace, you're basically removing any noise that's orthogonal to that subspace.\n",
        "\n",
        "3. PCA can be very useful for whitening, which is a procedure in which you decorrelate your data, and you standardize it meaning that\n",
        "you rescale it so that the\n",
        "individual\n",
        "components have unit variance. \n",
        "PCA finds transformation so that the new components are uncorrelated, and we know that the variance of each component is given by the eigen value.\n",
        "So we can just rescale this so that the new components are uncorrelated and have unit variance.\n",
        "So these first three properties that PCA can be used for- compression, denoising and whitening are \n",
        "extremely useful in a very wide variety of settings,\n",
        "and actually if you want to do a more complicated nonlinear dimensionality reduction technique\n",
        "it's often useful to use PCA as a pre-processing step for exactly these reasons. \n",
        "\n",
        "4. visualization, maybe one of the most common applications of PCA.\n",
        "\n",
        "\n",
        "If we have a sample\n",
        "that's a 0\n",
        "and so when you take the dot product you'll get a very\n",
        "high value for the score\n",
        "corresponding to the first principal component.\n",
        "\n",
        "On the other hand, if you have a sample that says 0 \n",
        "So you'll get a very low value of the first principal component score.\n",
        "So that means that we already expect that there should be some kind of clustering\n",
        "aligned with the first principal component of ones and zeros.\n",
        "\n",
        "But this doesn't really say anything for what will happen for the digits between 2 & 9. \n",
        "\n",
        "So you will use PCA in order to visualize for full MNIST set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTZhjF3NWDA8"
      },
      "source": [
        "---\n",
        "# Setup\n",
        "Run these cells to get the tutorial started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjR7LAcBWDA8"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DYGtwRHcWDA8"
      },
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wgflVCBsWDA8"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "\n",
        "def visualize_components(component1, component2, labels, show=True):\n",
        "  \"\"\"\n",
        "  Plots a 2D representation of the data for visualization with categories\n",
        "  labelled as different colors.\n",
        "\n",
        "  Args:\n",
        "    component1 (numpy array of floats) : Vector of component 1 scores\n",
        "    component2 (numpy array of floats) : Vector of component 2 scores\n",
        "    labels (numpy array of floats)     : Vector corresponding to categories of\n",
        "                                         samples\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  cmap = plt.cm.get_cmap('tab10')\n",
        "  plt.scatter(x=component1, y=component2, c=labels, cmap=cmap)\n",
        "  plt.xlabel('Component 1')\n",
        "  plt.ylabel('Component 2')\n",
        "  plt.colorbar(ticks=range(10))\n",
        "  plt.clim(-0.5, 9.5)\n",
        "  if show:\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKj6TEfzQBOc"
      },
      "source": [
        "# Discussion of helper functions:\n",
        "\n",
        "*visualize_components*:\n",
        "Plots a 2D representation of the data for visualization with categories labelled as different colors with x-axis: component 1 and y-axis: component 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZaMZ9LbWDA8"
      },
      "source": [
        "---\n",
        "# Section 1: Visualize MNIST in 2D using PCA\n",
        "\n",
        "In this exercise, we'll visualize the first few components of the MNIST dataset to look for evidence of structure in the data. But in this tutorial, we will also be interested in the label of each image (i.e., which numeral it is from 0 to 9). Start by running the following cell to reload the MNIST dataset (this takes a few seconds). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7F--ZEKWDA8"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml(name='mnist_784')\n",
        "X = mnist.data\n",
        "labels = [int(k) for k in mnist.target]\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u73Gi8SHWDA8"
      },
      "source": [
        "To perform PCA, we now will use the method implemented in sklearn. Run the following cell to set the parameters of PCA - we will only look at the top 2 components because we will be visualizing the data in 2D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACVBntQWDA9"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_model = PCA(n_components=2) # Initializes PCA\n",
        "pca_model.fit(X) # Performs PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tex_ooBnWDA9"
      },
      "source": [
        "## Exercise 1: Visualization of MNIST in 2D using PCA\n",
        "\n",
        "Fill in the code below to perform PCA and visualize the top two  components. For better visualization, take only the first 2,000 samples of the data (this will also make t-SNE much faster in the following section of the tutorial so don't skip this step!)\n",
        "\n",
        "**Suggestions:**\n",
        "- Truncate the data matrix at 2,000 samples. You will also need to truncate the array of labels.\n",
        "- Perform PCA on the truncated data.\n",
        "- Use the function `visualize_components` to plot the labelled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj1X-njtWDA9"
      },
      "source": [
        "help(visualize_components)\n",
        "help(pca_model.transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqa8jd3qWDA-"
      },
      "source": [
        "#################################################\n",
        "## TODO for students: take only 2,000 samples and perform PCA\n",
        "#################################################\n",
        "\n",
        "# Take only the first 2000 samples with the corresponding labels\n",
        "# X, labels = ...\n",
        "# Perform PCA\n",
        "# scores = pca_model.transform(X)\n",
        "\n",
        "# Plot the data and reconstruction\n",
        "# visualize_components(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "26eb6551-87e4-463f-a503-2f8d4c3529fa",
        "id": "qzcs49NTWDA_"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial4_Solution_e53bd4fb.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=524 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial4_Solution_e53bd4fb_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-wy463qWDA_"
      },
      "source": [
        "## Think!\n",
        "- What do you see? Are different samples corresponding to the same numeral clustered together? Is there much overlap?\n",
        "- Do some pairs of numerals appear to be more distinguishable than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgE7ozDSRwWJ"
      },
      "source": [
        "*hint*:\n",
        "1. think in terms of structure.\n",
        "2. think in terms of distinguishability - could help to start with crude color based analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWDWZiVaWDA_"
      },
      "source": [
        "---\n",
        "# Section 2: Visualize MNIST in 2D using t-SNE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "C_GPJwJJWDA_"
      },
      "source": [
        "# @title Video 2: Nonlinear Methods\n",
        "video = YouTubeVideo(id=\"5Xpb0YaN5Ms\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "042pXNhKTZ7U"
      },
      "source": [
        "# Summary of Video 2:\n",
        "\n",
        "PCA is an example of linear dimensionality reduction. That means it finds a linear transformation to a low dimensional representation.\n",
        "There are many such linear dimensionality reduction methods and they differ in terms of the assumptions that they make on the data.\n",
        "\n",
        "1. probabilistic PCA which is similar to PCA, but it includes an explicit noise model. It assumes that the noise is gaussian with the same variance in each direction.\n",
        "\n",
        "2. factor analysis, which is similar to PPCA, \n",
        "but it allows the variance to be different in different directions.\n",
        "\n",
        "So different methods can also have different features of the data that they're looking for.\n",
        "\n",
        "1. linear discriminant analysis looks for a low dimensional subspace that preserves class discriminatory information.\n",
        "This is useful when you have labeled data\n",
        "\n",
        "2. supervised dimensionality reduction in contrast to PCA \n",
        "\n",
        "\n",
        "- running PCA you would find direction that captures\n",
        "most of the variance in the full data set, and if you\n",
        "run LDA, you would find the direction that captures information about the two stimuli.\n",
        "So LDA can be very useful when looking for directions that represents\n",
        "Information about different stimuli or different behavior in your neural data set.\n",
        "\n",
        "These methods are related to another class of methods that are used to solve blind source separation problems.\n",
        "\n",
        "These are problems where you have some data that are mixtures of\n",
        "different signals and you want to recover those signals by demixing your data?\n",
        "\n",
        "For example, this can be useful in pre-processing neural data like an EEG.\n",
        "The canonical example is independent components analysis, which finds components that are statistically independent.\n",
        "\n",
        "This is a stronger condition than them being uncorrelated which is what PCA finds.\n",
        "\n",
        "non-negative matrix factorization,\n",
        "which is useful when you have positive data because it assumes that your weights and components are positive.\n",
        "This can also make your results much more interpretable.\n",
        "Geometrically speaking. It finds basis vectors that are on the edges of a cone that contains all of your data points.\n",
        "\n",
        "And if you run PCA then you'll find components that have positive and negative\n",
        "values that could be very difficult to interpret.\n",
        "Again the basis vectors here are not necessarily orthogonal.\n",
        "\n",
        "When are linear methods not enough? \n",
        "- dimension is curved. So if you run PCA you will not find that\n",
        "S-shaped curve because it's explicitly looking for a linear representation.\n",
        "Instead what you want to find is this S shape in which the data points are embedded. In neuroscience\n",
        "we often call this a neural manifold. A neural manifold is a\n",
        "smooth low dimensional structure in which your data points are embedded in your high dimensional space.\n",
        "This is where nonlinear methods kick in. They don't usually find the equations for the manifold,\n",
        "but they find a mapping\n",
        "to a low dimensional embedding, and this mapping is chosen in order to preserve\n",
        "some information in the structure of the data. For example information about the locality of\n",
        "different data points. In particular many of these methods\n",
        "try to map data points that are close together in your high dimensional space,\n",
        "to also be close together and your low dimensional embedding.\n",
        "\n",
        "There are many different types of nonlinear dimensionality reduction methods, all with their own \n",
        "positives and negatives. \n",
        "\n",
        "in this final exercise, you'll use one called T distributed stochastic neighbor embedding or t-SNE.\n",
        "\n",
        "t-SNE is very useful for visualization of high dimensional data in two or three dimensions.\n",
        "You also can't reconstruct the data in the same way that you can in PCA.\n",
        "\n",
        "And finally t-SNE has a free parameter called the perplexity which roughly speaking\n",
        "balances the local versus global information that's considered in finding this embedding. \n",
        "this perplexity parameter can have a large impact on the\n",
        "results that you see. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohKZyvWVWDA_"
      },
      "source": [
        "Next we will analyze the same data using t-SNE, a nonlinear dimensionality reduction method that is useful for visualizing high dimensional data in 2D or 3D. Run the cell below to get started. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IjU4v1WDA_"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne_model = TSNE(n_components=2, perplexity=30, random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvVkHNLJWDA_"
      },
      "source": [
        "## Exercise 2: Apply t-SNE on MNIST\n",
        "First, we'll run t-SNE on the data to explore whether we can see more structure. The cell above defined the parameters that we will use to find our embedding (i.e, the low-dimensional representation of the data) and stored them in `model`. To run t-SNE on our data, use the function `model.fit_transform`.\n",
        "\n",
        "**Suggestions:**\n",
        "- Run t-SNE using the function `model.fit_transform`.\n",
        "- Plot the result data using `visualize_components`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB_GL42PWDA_"
      },
      "source": [
        "help(tsne_model.fit_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B9167pPWDA_"
      },
      "source": [
        "#################################################\n",
        "## TODO for students: perform tSNE and visualize the data\n",
        "#################################################\n",
        "\n",
        "# perform t-SNE\n",
        "embed = ...\n",
        "\n",
        "# Visualize the data\n",
        "# visualize_components(..., ..., labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "949bfb79-83f9-4809-8362-75872fce2246",
        "id": "J89iQw-WWDA_"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial4_Solution_a989b6ef.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=522 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial4_Solution_a989b6ef_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VGth3hVWDA_"
      },
      "source": [
        "## Exercise 3: Run t-SNE with different perplexities\n",
        "\n",
        "Unlike PCA, t-SNE has a free parameter (the perplexity) that roughly determines how global vs. local information is weighted. Here we'll take a look at how the perplexity affects our interpretation of the results. \n",
        "\n",
        "**Steps:**\n",
        "- Rerun t-SNE (don't forget to re-initialize using the function `TSNE` as above) with a perplexity of 50, 5 and 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVh16ehBWDBA"
      },
      "source": [
        "def explore_perplexity(values):\n",
        "  \"\"\"\n",
        "  Plots a 2D representation of the data for visualization with categories\n",
        "  labelled as different colors using different perplexities.\n",
        "\n",
        "  Args:\n",
        "    values (list of floats) : list with perplexities to be visualized\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "  for perp in values:\n",
        "\n",
        "    #################################################\n",
        "    ## TO DO for students: Insert your code here to redefine the t-SNE \"model\"\n",
        "    ## while setting the perplexity perform t-SNE on the data and plot the\n",
        "    ## results for perplexity = 50, 5, and 2 (set random_state to 2020\n",
        "    # Comment these lines when you complete the function\n",
        "    raise NotImplementedError(\"Student Exercise! Explore t-SNE with different perplexity\")\n",
        "    #################################################\n",
        "\n",
        "    # perform t-SNE\n",
        "    tsne_model = ...\n",
        "\n",
        "    embed = tsne_model.fit_transform(X)\n",
        "    visualize_components(embed[:, 0], embed[:, 1], labels, show=False)\n",
        "    plt.title(f\"perplexity: {perp}\")\n",
        "\n",
        "\n",
        "# Uncomment when you complete the function\n",
        "# values = [50, 5, 2]\n",
        "# explore_perplexity(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "bcdc9dda-f25e-4ac5-f2fc-6975a52a9756",
        "id": "nOGy39ObWDBA"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial4_Solution_e3519b37.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=521 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial4_Solution_e3519b37_0.png>\n",
        "\n",
        "<img alt='Solution hint' align='left' width=521 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial4_Solution_e3519b37_1.png>\n",
        "\n",
        "<img alt='Solution hint' align='left' width=522 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial4_Solution_e3519b37_2.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS_9r-uoWDBA"
      },
      "source": [
        "## Think!\n",
        "\n",
        "- What changes compared to your previous results using perplexity equal to 50? Do you see any clusters that have a different structure than before? \n",
        "- What changes in the embedding structure for perplexity equals to 5 or 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43GEksYSRUo"
      },
      "source": [
        "*hint*:\n",
        "\n",
        "1. think location and structure. \n",
        "2. think overall structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGDNpOazWDBA"
      },
      "source": [
        "---\n",
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdJxXe1WDBA"
      },
      "source": [
        "* We learned the difference between linear and nonlinear dimensionality reduction. While nonlinear methods can be more powerful, they can also be senseitive to noise. In contrast, linear methods are useful for their simplicity and robustness.\n",
        "* We compared PCA and t-SNE for data visualization. Using t-SNE, we could visualize clusters in the data corresponding to different digits. While PCA was able to separate some clusters (e.g., 0 vs 1), it performed poorly overall.\n",
        "* However, the results of t-SNE can change depending on the choice of perplexity. To learn more, we recommend this [Distill paper](https://distill.pub/2016/misread-tsne/).\n"
      ]
    }
  ]
}