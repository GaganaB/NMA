{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W1D5_Tutorial1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weximBykeB8B"
      },
      "source": [
        "# Neuromatch Academy: Week 1, Day 5, Tutorial 1\n",
        "# Dimensionality Reduction: Geometric view of data\n",
        "\n",
        "__Content creators:__ Alex Cayco Gajic, John Murray\n",
        "\n",
        "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piEz7S98eB8B"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "In this notebook we'll explore how multivariate data can be represented in different orthonormal bases. This will help us build intuition that will be helpful in understanding PCA in the following tutorial. \n",
        "\n",
        "Overview:\n",
        " - Generate correlated multivariate data.\n",
        " - Define an arbitrary orthonormal basis. \n",
        " - Project the data onto the new basis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jWR3v_peeB8C"
      },
      "source": [
        "# @title Video 1: Geometric view of data\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"THu9yHnpq9I\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyknDFwIPPAN"
      },
      "source": [
        "# Summary of Video 1:\n",
        "\n",
        "- first tutorial: conceptualize multivariate data from a geometric point of view (useful for understanding how principal component analysis works)\n",
        "- second tutorial: PCA\n",
        "- third tutorial: use PCA for dimensionality reduction in high dimensional data.\n",
        "- final tutorial: nonlinear dimensionality reduction technique called t-SNE.\n",
        "\n",
        "Geometric view of data:\n",
        "- Multivariate means many variables \n",
        "- each Xi represents the firing rate of a particular neuron, from the whole population of neurons that you're recording from.\n",
        "(each Xi could represent a behavioral variable for a particular behavioral task.)\n",
        "So at each time point in the experiment, collect a new vector corresponding to the firing rates of all of these neurons. \n",
        "\n",
        "Data representation:\n",
        "- One way is to just stack all of the vectors together into the data matrix X. In this matrix, each row corresponds to a single sample of all of the variables. So all of the firing rates of all neurons at a single time point. And each column corresponds to all of the samples of a single variable \n",
        "- Another way to represent this data is as a point in a high dimensional vector space described by the coordinate axes each of which represents the firing rates for a different neuron.\n",
        "\n",
        "- get multiple data points => will lie on different locations in this vector space and that's because there is typically a lot of variability in neural data. \n",
        "But there's different flavors of variability, and one possibility is you may see data that looks uncorrelated (~variability of neuron 1 is pretty much the same as the variability of neuron 2). On the other hand, you might see uncorrelated data where one neuron has much higher variability than the other neuron.\n",
        "On the other hand, you might see that your data is very correlated. And in this case notice that the direction corresponding to most of the variance in the data is\n",
        "aligned neither with neuron 1's coordinate axis nor with axis of neuron 2. Essentially the goal of PCA is to find this direction.\n",
        "\n",
        "Typically in neural data, even when we're recording from very large numbers of neurons. We don't observe that the neural activity patterns \n",
        "are uniformly distributed across the entire vector space of possible activity patterns. Usually we see that they are constrained to lie on a lower dimensional subspace,\n",
        "and the goal of PCA is to find this low dimensional subspace by looking for directions of maximum variance in the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X__vYYSueB8C"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxcpXHJKeB8C"
      },
      "source": [
        "# Import\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "uf_xGCZVeB8D"
      },
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets  # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FtT6mJcCeB8D"
      },
      "source": [
        "# @title Helper functions\n",
        "\n",
        "\n",
        "def get_data(cov_matrix):\n",
        "  \"\"\"\n",
        "  Returns a matrix of 1000 samples from a bivariate, zero-mean Gaussian.\n",
        "\n",
        "  Note that samples are sorted in ascending order for the first random variable\n",
        "\n",
        "  Args:\n",
        "    cov_matrix (numpy array of floats): desired covariance matrix\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats) : samples from the bivariate Gaussian, with each\n",
        "                              column corresponding to a different random\n",
        "                              variable\n",
        "  \"\"\"\n",
        "\n",
        "  mean = np.array([0, 0])\n",
        "  X = np.random.multivariate_normal(mean, cov_matrix, size=1000)\n",
        "  indices_for_sorting = np.argsort(X[:, 0])\n",
        "  X = X[indices_for_sorting, :]\n",
        "\n",
        "  return X\n",
        "\n",
        "\n",
        "def plot_data(X):\n",
        "  \"\"\"\n",
        "  Plots bivariate data. Includes a plot of each random variable, and a scatter\n",
        "  plot of their joint activity. The title indicates the sample correlation\n",
        "  calculated from the data.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) :   Data matrix each column corresponds to a\n",
        "                                  different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax1 = fig.add_subplot(gs[0, 0])\n",
        "  ax1.plot(X[:, 0], color='k')\n",
        "  plt.ylabel('Neuron 1')\n",
        "  plt.title('Sample var 1: {:.1f}'.format(np.var(X[:, 0])))\n",
        "  ax1.set_xticklabels([])\n",
        "  ax2 = fig.add_subplot(gs[1, 0])\n",
        "  ax2.plot(X[:, 1], color='k')\n",
        "  plt.xlabel('Sample Number')\n",
        "  plt.ylabel('Neuron 2')\n",
        "  plt.title('Sample var 2: {:.1f}'.format(np.var(X[:, 1])))\n",
        "  ax3 = fig.add_subplot(gs[:, 1])\n",
        "  ax3.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
        "           markeredgewidth=0)\n",
        "  ax3.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(X[:, 0], X[:, 1])[0, 1]))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_basis_vectors(X, W):\n",
        "  \"\"\"\n",
        "  Plots bivariate data as well as new basis vectors.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) :   Data matrix each column corresponds to a\n",
        "                                  different random variable\n",
        "    W (numpy array of floats) :   Square matrix representing new orthonormal\n",
        "                                  basis each column represents a basis vector\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure(figsize=[4, 4])\n",
        "  plt.plot(X[:, 0], X[:, 1], '.', color=[.5, .5, .5], label='Data')\n",
        "  plt.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  plt.plot([0, W[0, 0]], [0, W[1, 0]], color='r', linewidth=3,\n",
        "           label='Basis vector 1')\n",
        "  plt.plot([0, W[0, 1]], [0, W[1, 1]], color='b', linewidth=3,\n",
        "           label='Basis vector 2')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_data_new_basis(Y):\n",
        "  \"\"\"\n",
        "  Plots bivariate data after transformation to new bases.\n",
        "  Similar to plot_data but with colors corresponding to projections onto\n",
        "  basis 1 (red) and basis 2 (blue). The title indicates the sample correlation\n",
        "  calculated from the data.\n",
        "\n",
        "  Note that samples are re-sorted in ascending order for the first\n",
        "  random variable.\n",
        "\n",
        "  Args:\n",
        "    Y (numpy array of floats):   Data matrix in new basis each column\n",
        "                                 corresponds to a different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax1 = fig.add_subplot(gs[0, 0])\n",
        "  ax1.plot(Y[:, 0], 'r')\n",
        "  plt.xlabel\n",
        "  plt.ylabel('Projection \\n basis vector 1')\n",
        "  plt.title('Sample var 1: {:.1f}'.format(np.var(Y[:, 0])))\n",
        "  ax1.set_xticklabels([])\n",
        "  ax2 = fig.add_subplot(gs[1, 0])\n",
        "  ax2.plot(Y[:, 1], 'b')\n",
        "  plt.xlabel('Sample number')\n",
        "  plt.ylabel('Projection \\n basis vector 2')\n",
        "  plt.title('Sample var 2: {:.1f}'.format(np.var(Y[:, 1])))\n",
        "  ax3 = fig.add_subplot(gs[:, 1])\n",
        "  ax3.plot(Y[:, 0], Y[:, 1], '.', color=[.5, .5, .5])\n",
        "  ax3.axis('equal')\n",
        "  plt.xlabel('Projection basis vector 1')\n",
        "  plt.ylabel('Projection basis vector 2')\n",
        "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(Y[:, 0], Y[:, 1])[0, 1]))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm3_QDvnV0I_"
      },
      "source": [
        "# Discussion of Helper functions:\n",
        "\n",
        "*get_data*:\n",
        "takes the covariance matrix and returns the data matrix (1000 samples from bivariate, zero mean gaussian - each column corresponding to different random variable) sorted in ascending order for the first random variable.  \n",
        "\n",
        "*plot_data*:\n",
        "Plots bivariate data. Includes a plot of each random variable neuron1 (ylabel) and neuron2 (ylabel) with sample number (xaxis) : a scatter\n",
        "  plot of their activity indicating the sample correlation calculated from the data.\n",
        "\n",
        "*plot_basis_vectors*:\n",
        "plots bivariable data with new basis vectors using orthonormal basis matrix (each column represents basis vector) and data matrix where each column corresponds to different random variable.\n",
        "\n",
        "*plot_data_new_basis*:\n",
        "Plots bivariate data after transformation to new bases with colors corresponding to projections onto basis 1 and basis 2 indicating the sample correlation calculated from the data matrix in new basis with each column corresponding to a different random variable. Samples are re-sorted in ascending order for the first random variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vglBoC38eB8D"
      },
      "source": [
        "---\n",
        "# Section 1: Generate correlated multivariate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "S7zG4gzFeB8D"
      },
      "source": [
        "# @title Video 2: Multivariate data\n",
        "video = YouTubeVideo(id=\"jcTq2PgU5Vw\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBzg8PdEWK7G"
      },
      "source": [
        "# Summary of Video 2:\n",
        "\n",
        "To measure the variability of each neuron individually, we can use the variance, which describes the spread of the distribution of firing rates around it's average value.\n",
        "\n",
        "if you measure the firing rates in Hertz, then the covariance would be in Hertz squared. So you can get rid of this, by dividing by the product of the standard deviations and this also normalizes. We call this the correlation coefficient.\n",
        "All of this information can be summarized in the covariance matrix, which is usually called capital sigma. This is an N by N matrix whose ij element is the covariance of xi and xj. That means that the ji element is also the covariance of xi and xj. So the covariance matrix is symmetric (sigma is equal to its own transpose) across its diagonal and the ii element is given by the variance.\n",
        "\n",
        "This affords the covariance matrix with some important quantities that will be used in PCA.\n",
        "In the first exercise model neural data as coming from a multivariate normal distribution. This is a generalization of a one-dimensional normal distribution to N dimensions with a given covariance structure. And if you look at the marginal distribution of each neurons separately, then you recover the original one dimensional Gaussian. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICLXqxWLeB8D"
      },
      "source": [
        "To gain intuition, we will first use a simple model to generate multivariate data. Specifically, we will draw random samples from a *bivariate normal distribution*. This is an extension of the one-dimensional normal distribution to two dimensions, in which each $x_i$ is marginally normal with mean $\\mu_i$ and variance $\\sigma_i^2$:\n",
        "\n",
        "\\begin{align}\n",
        "x_i \\sim \\mathcal{N}(\\mu_i,\\sigma_i^2).\n",
        "\\end{align}\n",
        "\n",
        "Additionally, the joint distribution for $x_1$ and $x_2$ has a specified correlation coefficient $\\rho$. Recall that the correlation coefficient is a normalized version of the covariance, and ranges between -1 and +1:\n",
        "\n",
        "\\begin{align}\n",
        "\\rho = \\frac{\\text{cov}(x_1,x_2)}{\\sqrt{\\sigma_1^2 \\sigma_2^2}}.\n",
        "\\end{align}\n",
        "\n",
        "For simplicity, we will assume that the mean of each variable has already been subtracted, so that $\\mu_i=0$. The remaining parameters can be summarized in the covariance matrix, which for two dimensions has the following form:\n",
        "\n",
        "\\begin{equation*}\n",
        "{\\bf \\Sigma} = \n",
        "\\begin{pmatrix}\n",
        " \\text{var}(x_1) & \\text{cov}(x_1,x_2) \\\\\n",
        " \\text{cov}(x_1,x_2) &\\text{var}(x_2)\n",
        "\\end{pmatrix}.\n",
        "\\end{equation*}\n",
        "\n",
        "In general, $\\bf \\Sigma$ is a symmetric matrix with the variances $\\text{var}(x_i) = \\sigma_i^2$ on the diagonal, and the covariances on the off-diagonal. Later, we will see that the covariance matrix plays a key role in PCA.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7TFIKNeB8D"
      },
      "source": [
        "\n",
        "## Exercise 1: Draw samples from a distribution\n",
        "\n",
        "We have provided code to draw random samples from a zero-mean bivariate normal distribution. Throughout this tutorial, we'll imagine these samples represent the activity (firing rates) of two recorded neurons on different trials. Fill in the function below to calculate the covariance matrix given the desired variances and correlation coefficient. The covariance can be found by rearranging the equation above:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{cov}(x_1,x_2) = \\rho \\sqrt{\\sigma_1^2 \\sigma_2^2}.\n",
        "\\end{align}\n",
        "\n",
        "Use these functions to generate and plot data while varying the parameters. You should get a feel for how changing the correlation coefficient affects the geometry of the simulated data.\n",
        "\n",
        "**Steps**\n",
        "* Fill in the function `calculate_cov_matrix` to calculate the desired covariance.\n",
        "* Generate and plot the data for $\\sigma_1^2 =1$, $\\sigma_1^2 =1$, and $\\rho = .8$. Try plotting the data for different values of the correlation coefficient: $\\rho = -1, -.5, 0, .5, 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqes_Qi0eB8D"
      },
      "source": [
        "help(plot_data)\n",
        "help(get_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXCsrdALeB8D"
      },
      "source": [
        "def calculate_cov_matrix(var_1, var_2, corr_coef):\n",
        "  \"\"\"\n",
        "  Calculates the covariance matrix based on the variances and correlation\n",
        "  coefficient.\n",
        "\n",
        "  Args:\n",
        "    var_1 (scalar)          : variance of the first random variable\n",
        "    var_2 (scalar)          : variance of the second random variable\n",
        "    corr_coef (scalar)      : correlation coefficient\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats) : covariance matrix\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: calculate the covariance matrix\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: calculate the covariance matrix!\")\n",
        "  #################################################\n",
        "\n",
        "  # Calculate the covariance from the variances and correlation\n",
        "  cov = ...\n",
        "\n",
        "  cov_matrix = np.array([[var_1, cov], [cov, var_2]])\n",
        "\n",
        "  return cov_matrix\n",
        "\n",
        "\n",
        "###################################################################\n",
        "## TO DO for students: generate and plot bivariate Gaussian data with variances of 1\n",
        "## and a correlation coefficients of: 0.8\n",
        "## repeat while varying the correlation coefficient from -1 to 1\n",
        "###################################################################\n",
        "np.random.seed(2020)  # set random seed\n",
        "variance_1 = 1\n",
        "variance_2 = 1\n",
        "corr_coef = 0.8\n",
        "\n",
        "# Uncomment to test your code and plot\n",
        "# cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
        "# X = get_data(cov_matrix)\n",
        "# plot_data(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "87407e56-4bd4-4cf0-d040-57381772daad",
        "id": "cBqtSwfOeB8D"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial1_Solution_a068bc84.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=570 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial1_Solution_a068bc84_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSUr5rc-eB8D"
      },
      "source": [
        "---\n",
        "# Section 2: Define a new orthonormal basis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Y3fD-qIFeB8D"
      },
      "source": [
        "# @title Video 3: Orthonormal bases\n",
        "video = YouTubeVideo(id=\"PC1RZELnrIg\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uai0R6bZ61G"
      },
      "source": [
        "# Summary of Video 3:\n",
        "\n",
        "Remember that the goal for dimensionality reduction is to find a lower dimensional subspace that you can use to capture much of the structure of your original data.\n",
        "We need to understand how you can represent data in multiple different coordinate systems.\n",
        "\n",
        "In fact there are many ways to represent multivariate data.\n",
        "We can define a basis as a set of N vectors that you can use to construct any point in an N-dimensional vector space.\n",
        "(specific choice of basis called the standard basis in which your first basis vector is one unit in the direction of the\n",
        "activity for neuron one and the second basis vector is one unit in the direction of neuron two.)\n",
        "One could rotate these basis vectors and therefore represent this point as a completely different set of coefficients.\n",
        "You can also rescale your basis \n",
        "\n",
        "By orthonormal, we mean that the basis is orthogonal and all basis vectors have a length of 1, \n",
        "and we can measure the length as the square root of the sum of the squared values of the entries.\n",
        "It's worth noting that if you already have an orthogonal basis, then you can very easily normalize it by\n",
        "dividing each basis vector by its magnitude.\n",
        "\n",
        "So if they're perpendicular or orthogonal then the cosine is equal to zero,\n",
        "so that means that the dot product is zero. And in fact two vectors are defined as orthogonal\n",
        "exactly when the dot product is equal to zero.\n",
        "\n",
        "Another way to find if two vectors are orthogonal - \n",
        "- u dot w can also be written as the sum of ui times wi.\n",
        "- So if u and w are both column vectors then u dot w is u transpose times w."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soGnPxsIeB8D"
      },
      "source": [
        "Next, we will define a new orthonormal basis of vectors ${\\bf u} = [u_1,u_2]$ and ${\\bf w} = [w_1,w_2]$. As we learned in the video, two vectors are orthonormal if: \n",
        "\n",
        "1.   They are orthogonal (i.e., their dot product is zero):\n",
        "\\begin{equation}\n",
        "{\\bf u\\cdot w} = u_1 w_1 + u_2 w_2 = 0\n",
        "\\end{equation}\n",
        "2.   They have unit length:\n",
        "\\begin{equation}\n",
        "||{\\bf u} || = ||{\\bf w} || = 1\n",
        "\\end{equation}\n",
        "\n",
        "In two dimensions, it is easy to make an arbitrary orthonormal basis. All we need is a random vector ${\\bf u}$, which we have normalized. If we now define the second basis vector to be ${\\bf w} = [-u_2,u_1]$, we can check that both conditions are satisfied: \n",
        "\\begin{equation}\n",
        "{\\bf u\\cdot w} = - u_1 u_2 + u_2 u_1 = 0\n",
        "\\end{equation}\n",
        "and \n",
        "\\begin{equation}\n",
        "{|| {\\bf w} ||} = \\sqrt{(-u_2)^2 + u_1^2} = \\sqrt{u_1^2 + u_2^2} = 1,\n",
        "\\end{equation}\n",
        "where we used the fact that ${\\bf u}$ is normalized. So, with an arbitrary input vector, we can define an orthonormal basis, which we will write in matrix by stacking the basis vectors horizontally:\n",
        "\n",
        "\\begin{equation}\n",
        "{{\\bf W} } =\n",
        "\\begin{pmatrix}\n",
        " u_1 & w_1 \\\\\n",
        " u_2 & w_2\n",
        "\\end{pmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ffts7TEeB8D"
      },
      "source": [
        "## Exercise 2: Find an orthonormal basis\n",
        "\n",
        "In this exercise you will fill in the function below to define an orthonormal basis, given a single arbitrary 2-dimensional vector as an input.\n",
        "\n",
        "**Steps**\n",
        "* Modify the function `define_orthonormal_basis` to first normalize the first basis vector $\\bf u$.\n",
        "* Then complete the function by finding a basis vector $\\bf w$ that is orthogonal to $\\bf u$.\n",
        "* Test the function using initial basis vector ${\\bf u} = [3,1]$. Plot the resulting basis vectors on top of the data scatter plot using the function `plot_basis_vectors`. (For the data, use  $\\sigma_1^2 =1$, $\\sigma_2^2 =1$, and $\\rho = .8$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd-syz8oeB8D"
      },
      "source": [
        "help(plot_basis_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA6G-17seB8D"
      },
      "source": [
        "def define_orthonormal_basis(u):\n",
        "  \"\"\"\n",
        "  Calculates an orthonormal basis given an arbitrary vector u.\n",
        "\n",
        "  Args:\n",
        "    u (numpy array of floats) : arbitrary 2-dimensional vector used for new\n",
        "                                basis\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : new orthonormal basis\n",
        "                                columns correspond to basis vectors\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: calculate the orthonormal basis\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: implement the orthonormal basis function\")\n",
        "  #################################################\n",
        "\n",
        "  # normalize vector u\n",
        "  u = ...\n",
        "  # calculate vector w that is orthogonal to w\n",
        "  w = ...\n",
        "\n",
        "  W = np.column_stack([u, w])\n",
        "\n",
        "  return W\n",
        "\n",
        "\n",
        "np.random.seed(2020)  # set random seed\n",
        "variance_1 = 1\n",
        "variance_2 = 1\n",
        "corr_coef = 0.8\n",
        "\n",
        "cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
        "X = get_data(cov_matrix)\n",
        "u = np.array([3, 1])\n",
        "\n",
        "# Uncomment and run below to plot the basis vectors\n",
        "# W = define_orthonormal_basis(u)\n",
        "# plot_basis_vectors(X, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "692ac8a7-45b5-4a00-ad38-4522f4c0ecdd",
        "id": "0btLD97XeB8E"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial1_Solution_85e5b50f.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=274 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial1_Solution_85e5b50f_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QkGkWuZeB8E"
      },
      "source": [
        "---\n",
        "# Section 3: Project data onto new basis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0rJPJfbgeB8E"
      },
      "source": [
        "# @title Video 4: Change of basis\n",
        "video = YouTubeVideo(id=\"Mj6BRQPKKUc\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3pDwBhabpZ8"
      },
      "source": [
        "#Summary of Video 4:\n",
        "\n",
        "how can you transform from your old basis to a new basis?\n",
        "- easy if we have a new orthonormal basis. \n",
        "- project and find magnitude of that projection (hypotenuse and its' adjacent).\n",
        "So that boils down to the magnitude of projected_vector times the cosine of theta or dot product if vector you project onto is normalised.\n",
        "So for an orthonormal basis your new coordinates are the old coordinates dot the new basis vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8di1IrMeB8E"
      },
      "source": [
        "\n",
        "   \n",
        "Finally, we will express our data in the new basis that we have just found. Since $\\bf W$ is orthonormal, we can project the data into our new basis using simple matrix multiplication :\n",
        "\n",
        "\\begin{equation}\n",
        "{\\bf Y = X W}.\n",
        "\\end{equation}\n",
        "\n",
        "We will explore the geometry of the transformed data $\\bf Y$ as we vary the choice of basis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY8uIs6NeB8E"
      },
      "source": [
        "## Exercise 3: Define an orthonormal basis\n",
        "In this exercise you will fill in the function below to define an orthonormal basis, given a single arbitrary vector as an input.\n",
        "\n",
        "**Steps**\n",
        "* Complete the function `change_of_basis` to project the data onto the new basis.\n",
        "* Plot the projected data using the function `plot_data_new_basis`. \n",
        "* What happens to the correlation coefficient in the new basis? Does it increase or decrease? \n",
        "* What happens to variance? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0spOaQrDeB8E"
      },
      "source": [
        "def change_of_basis(X, W):\n",
        "  \"\"\"\n",
        "  Projects data onto new basis W.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
        "                                basis vectors\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)    : Data matrix expressed in new basis\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: project the data onto o new basis W\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: implement change of basis\")\n",
        "  #################################################\n",
        "\n",
        "  # project data onto new basis described by W\n",
        "  Y = ...\n",
        "\n",
        "  return Y\n",
        "\n",
        "\n",
        "# Unomment below to transform the data by projecting it into the new basis\n",
        "# Y = change_of_basis(X, W)\n",
        "# plot_data_new_basis(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "6ae7008b-0f5c-4d43-eb19-45787a995387",
        "id": "9KKzqZTBeB8E"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial1_Solution_a1124bbc.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=560 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial1_Solution_a1124bbc_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_YnUineB8E"
      },
      "source": [
        "## Interactive Demo: Play with the basis vectors\n",
        "To see what happens to the correlation as we change the basis vectors, run the cell below. The parameter $\\theta$ controls the angle of $\\bf u$ in degrees. Use the slider to rotate the basis vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oKXFpmCWeB8E"
      },
      "source": [
        "# @title\n",
        "\n",
        "# @markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "\n",
        "def refresh(theta=0):\n",
        "  u = [1, np.tan(theta * np.pi / 180)]\n",
        "  W = define_orthonormal_basis(u)\n",
        "  Y = change_of_basis(X, W)\n",
        "  plot_basis_vectors(X, W)\n",
        "  plot_data_new_basis(Y)\n",
        "\n",
        "\n",
        "_ = widgets.interact(refresh, theta=(0, 90, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-m5xRCheB8E"
      },
      "source": [
        "## Questions\n",
        "\n",
        "* What happens to the projected data as you rotate the basis? \n",
        "* How does the correlation coefficient change? How does the variance of the projection onto each basis vector change?\n",
        "* Are you able to find a basis in which the projected data is **uncorrelated**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6-PW4AWcebP"
      },
      "source": [
        "*hint*:\n",
        "1. think of effect on sample correlation value. \n",
        "2. think of relationship between variance, correlation matrix, and theta. \n",
        "3. think of covariance values that cause projected data to be uncorrelated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1oRsCJ6eB8E"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "- In this tutorial, we learned that multivariate data can be visualized as a cloud of points in a high-dimensional vector space. The geometry of this cloud is shaped by the covariance matrix.\n",
        "\n",
        "- Multivariate data can be represented in a new orthonormal basis using the dot product. These new basis vectors correspond to specific mixtures of the original variables - for example, in neuroscience, they could represent different ratios of activation  across a population of neurons.\n",
        "\n",
        "- The projected data (after transforming into the new basis) will generally have a different geometry from the original data. In particular, taking basis vectors that are aligned with the spread of cloud of points decorrelates the data.\n",
        "\n",
        "* These concepts - covariance, projections, and orthonormal bases - are key for understanding PCA, which will be our focus in the next tutorial."
      ]
    }
  ]
}