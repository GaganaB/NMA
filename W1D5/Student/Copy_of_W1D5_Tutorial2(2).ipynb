{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W1D5_Tutorial2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA_sYV22pniJ"
      },
      "source": [
        "# Neuromatch Academy: Week 1, Day 5, Tutorial 2\n",
        "# Dimensionality Reduction: Principal component analysis\n",
        "\n",
        "__Content creators:__ Alex Cayco Gajic, John Murray\n",
        "\n",
        "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwj-g8kpniJ"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "In this notebook we'll learn how to perform PCA by projecting the data onto the eigenvectors of its covariance matrix. \n",
        "\n",
        "Overview:\n",
        "- Calculate the eigenvectors of the sample covariance matrix.\n",
        "- Perform PCA by projecting data onto the eigenvectors of the covariance matrix. \n",
        "- Plot and explore the eigenvalues.\n",
        "\n",
        "To quickly refresh your knowledge of eigenvalues and eigenvectors, you can watch this [short video](https://www.youtube.com/watch?v=kwA3qM0rm7c) (4 minutes) for a geometrical explanation. For a deeper understanding, this [in-depth video](https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=14) (17 minutes) provides an excellent basis and is beautifully illustrated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "JutYjFdnpniJ"
      },
      "source": [
        "# @title Video 1: PCA\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"-f6T9--oM0E\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcI31IoUifY8"
      },
      "source": [
        "# Summary of video 1:\n",
        "\n",
        "goal of dimensionality reduction, which is to take your original data, which is made of many samples of a large number of variables that you're recording from directly, and to reduce that to a much smaller number of features that you're not directly observing, but that you're inferring, in order to capture much of the structure of the original data. \n",
        "Which features? much of the structure of the data can be revealed by the covariance and\n",
        "We might expect that the directions that have large variation probably represents signal or other interesting features in the data. \n",
        "Whereas directions of small variation probably represent measurement noise.\n",
        "\n",
        "The goal of pca is to find these directions of maximum variance and to use them to form the new orthonormal basis to represent your data.\n",
        "the projected variance is largest when the basis is aligned with a covariance direction, and this also makes the correlation 0. \n",
        "\n",
        "So the first step is to find the vector that has the highest projected variance (just sweep over all possible vectors in your vector space until you find one\n",
        "where the variance of the projected data is maximized. This is called the first principal component. And then after this,\n",
        "we can find the second principal component by finding the vector that has the highest projected variance while being orthogonal to the first\n",
        "principal component. and if we continue this at each step we're maximizing the projected variance \n",
        "Constrained to the vectors that are orthogonal to all of the previous principal components that we found)\n",
        "or mathematically - vectors are the eigen vectors of the covariance matrix.\n",
        "\n",
        "sometimes you observe that there are some vectors that after you transform them, \n",
        "they're still pointing in the same direction, \n",
        "but they have a different magnitude. These are called eigen vectors. \n",
        "So the eigen vector is a vector so that the linear transformation\n",
        "is stretching or shrinking in the same direction and the stretching or shrinking amount, which is lambda. We call the eigenvalue.\n",
        "\n",
        "goal of PCA - do the constraint optimization problem, then you get out that the\n",
        "solution is the eigen vectors of the covariance matrix.\n",
        "\n",
        "But how do you know which of the eigenvectors correspond to the maximal projected variance?\n",
        "The projected variance onto each of these vectors is actually just given by the corresponding eigenvalue lambda i,\n",
        "so you don't actually have to calculate the projected variance of the data.\n",
        "This also gives a natural ordering of the eigen vectors of the covariance matrix \n",
        "in order of how much of the variance of the data they're able to explain.\n",
        "\n",
        "algorithm. \n",
        "- The first step is to subtract the mean so for each neuron for example subtract its average firing rate across all Of these samples and this makes it easier to calculate the covariance matrix\n",
        "- The second step is to calculate the eigen vectors of the covariance matrix of the data and order them by their corresponding eigen value, which again is the\n",
        "projection variance. \n",
        "- And then the final step is to project the data onto the new basis since the new basis is orthonormal.\n",
        "\n",
        "the vectors corresponding to the eigen vectors with a new\n",
        "orthonormal basis are usually called loadings or weights, and the projected data is\n",
        "usually described as a scores. In the case that we're using PCA for dimensionality reduction, they may also be called latent variables.\n",
        "\n",
        "And the key properties that you should always remember about PCA are that\n",
        "the\n",
        "the basis vectors Wi are orthogonal by construction the scores Si are uncorrelated and\n",
        "the eigen values lambda i are equal to the projected variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TadoEygvpniJ"
      },
      "source": [
        "---\n",
        "# Setup\n",
        "Run these cells to get the tutorial started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdOecYKGpniJ"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lA0y6jqspniJ"
      },
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets  # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVmfUgmJpniK"
      },
      "source": [
        "# @title Helper functions\n",
        "\n",
        "\n",
        "def plot_eigenvalues(evals):\n",
        "  \"\"\"\n",
        "  Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "      (numpy array of floats) : Vector of eigenvalues\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(4, 4))\n",
        "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
        "  plt.xlabel('Component')\n",
        "  plt.ylabel('Eigenvalue')\n",
        "  plt.title('Scree plot')\n",
        "  plt.xticks(np.arange(1, len(evals) + 1))\n",
        "  plt.ylim([0, 2.5])\n",
        "\n",
        "\n",
        "def sort_evals_descending(evals, evectors):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
        "  eigenvectors to be in first two quadrants (if 2D).\n",
        "\n",
        "  Args:\n",
        "    evals (numpy array of floats)    : Vector of eigenvalues\n",
        "    evectors (numpy array of floats) : Corresponding matrix of eigenvectors\n",
        "                                        each column corresponds to a different\n",
        "                                        eigenvalue\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
        "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
        "  \"\"\"\n",
        "\n",
        "  index = np.flip(np.argsort(evals))\n",
        "  evals = evals[index]\n",
        "  evectors = evectors[:, index]\n",
        "  if evals.shape[0] == 2:\n",
        "    if np.arccos(np.matmul(evectors[:, 0],\n",
        "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 0] = -evectors[:, 0]\n",
        "    if np.arccos(np.matmul(evectors[:, 1],\n",
        "                           1 / np.sqrt(2) * np.array([-1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 1] = -evectors[:, 1]\n",
        "  return evals, evectors\n",
        "\n",
        "\n",
        "def plot_data(X):\n",
        "  \"\"\"\n",
        "  Plots bivariate data. Includes a plot of each random variable, and a scatter\n",
        "  scatter plot of their joint activity. The title indicates the sample\n",
        "  correlation calculated from the data.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax1 = fig.add_subplot(gs[0, 0])\n",
        "  ax1.plot(X[:, 0], color='k')\n",
        "  plt.ylabel('Neuron 1')\n",
        "  ax2 = fig.add_subplot(gs[1, 0])\n",
        "  ax2.plot(X[:, 1], color='k')\n",
        "  plt.xlabel('Sample Number (sorted)')\n",
        "  plt.ylabel('Neuron 2')\n",
        "  ax3 = fig.add_subplot(gs[:, 1])\n",
        "  ax3.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
        "           markeredgewidth=0)\n",
        "  ax3.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(X[:, 0], X[:, 1])[0, 1]))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_data(cov_matrix):\n",
        "  \"\"\"\n",
        "  Returns a matrix of 1000 samples from a bivariate, zero-mean Gaussian\n",
        "\n",
        "  Note that samples are sorted in ascending order for the first random\n",
        "  variable.\n",
        "\n",
        "  Args:\n",
        "    var_1 (scalar)                     : variance of the first random variable\n",
        "    var_2 (scalar)                     : variance of the second random variable\n",
        "    cov_matrix (numpy array of floats) : desired covariance matrix\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)            : samples from the bivariate Gaussian,\n",
        "                                          with each column corresponding to a\n",
        "                                          different random variable\n",
        "  \"\"\"\n",
        "\n",
        "  mean = np.array([0, 0])\n",
        "  X = np.random.multivariate_normal(mean, cov_matrix, size=1000)\n",
        "  indices_for_sorting = np.argsort(X[:, 0])\n",
        "  X = X[indices_for_sorting, :]\n",
        "  return X\n",
        "\n",
        "\n",
        "def calculate_cov_matrix(var_1, var_2, corr_coef):\n",
        "  \"\"\"\n",
        "  Calculates the covariance matrix based on the variances and\n",
        "  correlation coefficient.\n",
        "\n",
        "  Args:\n",
        "    var_1 (scalar)         :  variance of the first random variable\n",
        "    var_2 (scalar)         :  variance of the second random variable\n",
        "    corr_coef (scalar)     :  correlation coefficient\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats) : covariance matrix\n",
        "  \"\"\"\n",
        "  cov = corr_coef * np.sqrt(var_1 * var_2)\n",
        "  cov_matrix = np.array([[var_1, cov], [cov, var_2]])\n",
        "  return cov_matrix\n",
        "\n",
        "\n",
        "def define_orthonormal_basis(u):\n",
        "  \"\"\"\n",
        "  Calculates an orthonormal basis given an arbitrary vector u.\n",
        "\n",
        "  Args:\n",
        "    u (numpy array of floats) : arbitrary 2D vector used for new basis\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : new orthonormal basis columns correspond to\n",
        "                                basis vectors\n",
        "  \"\"\"\n",
        "\n",
        "  u = u / np.sqrt(u[0] ** 2 + u[1] ** 2)\n",
        "  w = np.array([-u[1], u[0]])\n",
        "  W = np.column_stack((u, w))\n",
        "  return W\n",
        "\n",
        "\n",
        "def plot_data_new_basis(Y):\n",
        "  \"\"\"\n",
        "  Plots bivariate data after transformation to new bases. Similar to plot_data\n",
        "  but with colors corresponding to projections onto basis 1 (red) and\n",
        "  basis 2 (blue).\n",
        "  The title indicates the sample correlation calculated from the data.\n",
        "\n",
        "  Note that samples are re-sorted in ascending order for the first random\n",
        "  variable.\n",
        "\n",
        "  Args:\n",
        "    Y (numpy array of floats) : Data matrix in new basis each column\n",
        "                                corresponds to a different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax1 = fig.add_subplot(gs[0, 0])\n",
        "  ax1.plot(Y[:, 0], 'r')\n",
        "  plt.ylabel('Projection \\n basis vector 1')\n",
        "  ax2 = fig.add_subplot(gs[1, 0])\n",
        "  ax2.plot(Y[:, 1], 'b')\n",
        "  plt.xlabel('Sample number')\n",
        "  plt.ylabel('Projection \\n basis vector 2')\n",
        "  ax3 = fig.add_subplot(gs[:, 1])\n",
        "  ax3.plot(Y[:, 0], Y[:, 1], '.', color=[.5, .5, .5])\n",
        "  ax3.axis('equal')\n",
        "  plt.xlabel('Projection basis vector 1')\n",
        "  plt.ylabel('Projection basis vector 2')\n",
        "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(Y[:, 0], Y[:, 1])[0, 1]))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def change_of_basis(X, W):\n",
        "  \"\"\"\n",
        "  Projects data onto a new basis.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
        "                                basis vectors\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : Data matrix expressed in new basis\n",
        "  \"\"\"\n",
        "\n",
        "  Y = np.matmul(X, W)\n",
        "  return Y\n",
        "\n",
        "\n",
        "def plot_basis_vectors(X, W):\n",
        "  \"\"\"\n",
        "  Plots bivariate data as well as new basis vectors.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : Square matrix representing new orthonormal\n",
        "                                basis each column represents a basis vector\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure(figsize=[4, 4])\n",
        "  plt.plot(X[:, 0], X[:, 1], '.', color=[.5, .5, .5], label='Data')\n",
        "  plt.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  plt.plot([0, W[0, 0]], [0, W[1, 0]], color='r', linewidth=3,\n",
        "           label='Basis vector 1')\n",
        "  plt.plot([0, W[0, 1]], [0, W[1, 1]], color='b', linewidth=3,\n",
        "           label='Basis vector 2')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu-4TGqn2-uy"
      },
      "source": [
        "# Discussion of helper functions\n",
        "\n",
        "*plot_eigenvalues*:\n",
        "plots scree plot/ eigen values with component (xaxis) and eigenvalue (yaxis) from vector of eigenvalues.\n",
        "\n",
        "*sort_evals_descending*:\n",
        "Sorts vector of eigenvalues and eigenvectors matrix in decreasing order. Also aligns first two eigenvectors to be in first two quadrants (if 2D).\n",
        "\n",
        "*plot_data*:\n",
        "Plots bivariate data. Includes a plot of each random variable neuron1 (ylabel) and neuron2 (ylabel) with sample number (xaxis) : a scatter plot of their activity indicating the sample correlation calculated from the data.\n",
        "\n",
        "*get_data:* \n",
        "takes the covariance matrix and returns the data matrix (1000 samples from bivariate, zero mean gaussian - each column corresponding to different random variable) sorted in ascending order for the first random variable.\n",
        "\n",
        "*calculate_cov_matrix:*\n",
        "Calculates the covariance matrix based on the variances of first and second random variables and correlation coefficient.\n",
        "\n",
        "*define_orthonormal_basis:*\n",
        "Calculates an orthonormal basis given an arbitrary vector u and returns new orthonormal basis columns correspond to basis vectors.\n",
        "\n",
        "*plot_data_new_basis:*\n",
        "Plots bivariate data after transformation to new bases with colors corresponding to projections onto basis 1 and basis 2 indicating the sample correlation calculated from the data matrix in new basis with each column corresponding to a different random variable. Samples are re-sorted in ascending order for the first random variable.\n",
        "\n",
        "*change_of_basis*:\n",
        "Projects data onto a new basis through matrix multiplication of Data matrix where each column corresponds to a different random variable and Square matrix representing new orthonormal basis each column represents a basis vector. \n",
        "\n",
        "*plot_basis_vectors:*\n",
        "plots bivariable data with new basis vectors using orthonormal basis matrix (each column represents basis vector) and data matrix where each column corresponds to different random variable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMdGzNnkpniK"
      },
      "source": [
        "---\n",
        "# Section 1: Calculate the eigenvectors of the the sample covariance matrix\n",
        "\n",
        "As we saw in the lecture, PCA represents data in a new orthonormal basis defined by the eigenvectors of the covariance matrix. Remember that in the previous tutorial, we generated bivariate normal data with a specified covariance matrix $\\bf \\Sigma$, whose $(i,j)$th element is:\n",
        "\\begin{equation}\n",
        "\\Sigma_{ij} = E[ x_i x_j ] - E[ x_i] E[ x_j ] .\n",
        "\\end{equation}\n",
        "However, in real life we don't have access to this ground-truth covariance matrix. To get around this, we can use the sample covariance matrix, $\\bf\\hat\\Sigma$, which is calculated directly from the data. The $(i,j)$th element of the sample covariance matrix is:\n",
        "\\begin{equation}\n",
        " \\hat \\Sigma_{ij} =  \\frac{1}{N_\\text{samples}}{\\bf x}_i^T {\\bf x}_j - \\bar {\\bf x}_i \\bar{\\bf x}_j ,\n",
        "\\end{equation}\n",
        "where ${\\bf x}_i = [ x_i(1), x_i(2), \\dots,x_i(N_\\text{samples})]^T$ is a column vector representing all measurements of neuron $i$, and  $\\bar {\\bf x}_i$ is the mean of neuron $i$ across samples:\n",
        "\\begin{equation}\n",
        "\\bar {\\bf x}_i = \\frac{1}{N_\\text{samples}} \\sum_{k=1}^{N_\\text{samples}} x_i(k).\n",
        "\\end{equation}\n",
        "If we assume that the data has already been mean-subtracted, then we can write the sample covariance matrix in a much simpler matrix form:\n",
        "\\begin{align}\n",
        "{\\bf \\hat \\Sigma}\n",
        "&= \\frac{1}{N_\\text{samples}} {\\bf X}^T {\\bf X}.\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9x0E82XpniK"
      },
      "source": [
        "## Exercise 1: Calculation of the covariance matrix\n",
        "\n",
        "Before calculating the eigenvectors, you must first calculate the sample covariance matrix.\n",
        "\n",
        "**Steps**\n",
        "* Complete the function `get_sample_cov_matrix` by first subtracting the sample mean of the data, then calculate $\\bf \\hat \\Sigma$ using the equation above.\n",
        "* Use `get_data` to generate bivariate normal data, and calculate the sample covariance matrix with your finished `get_sample_cov_matrix`. Compare this estimate to the true covariate matrix using `calculate_cov_matrix`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKWV0v2npniK"
      },
      "source": [
        "help(get_data)\n",
        "help(calculate_cov_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BMCTFUApniK"
      },
      "source": [
        "def get_sample_cov_matrix(X):\n",
        "  \"\"\"\n",
        "  Returns the sample covariance matrix of data X\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : Covariance matrix\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: calculate the covariance matrix\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: calculate the covariance matrix!\")\n",
        "  #################################################\n",
        "\n",
        "  # Subtract the mean of X\n",
        "  X = ...\n",
        "  # Calculate the covariance matrix (hint: use np.matmul)\n",
        "  cov_matrix = ...\n",
        "\n",
        "  return cov_matrix\n",
        "\n",
        "\n",
        "##########################################################\n",
        "## TODO for students: generate bivariate Gaussian data\n",
        "# with variances of 1 and a correlation coefficient of 0.8\n",
        "# compare the true and sample covariance matrices\n",
        "##########################################################\n",
        "variance_1 = 1\n",
        "variance_2 = 1\n",
        "corr_coef = 0.8\n",
        "\n",
        "np.random.seed(2020)  # set random seed\n",
        "# Uncomment below code to test your function\n",
        "# cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
        "# print(cov_matrix)\n",
        "\n",
        "# X = get_data(cov_matrix)\n",
        "# sample_cov_matrix = get_sample_cov_matrix(X)\n",
        "# print(sample_cov_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jenwgaZcpniL"
      },
      "source": [
        "SAMPLE OUTPUT\n",
        "```\n",
        "[[1.  0.8]\n",
        " [0.8 1. ]]\n",
        "[[0.99315313 0.82347589]\n",
        " [0.82347589 1.01281397]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "1c78e264-5276-4b34-adcf-ad45083a3583",
        "id": "wVEvIwYwpniL"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial2_Solution_a98b14b7.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48g-5g5pniL"
      },
      "source": [
        "## Exercise 2: Eigenvectors of the Covariance matrix\n",
        "\n",
        "Next you will calculate the eigenvectors of the covariance matrix. Plot them along with the data to check that they align with the geometry of the data.\n",
        "\n",
        "**Steps:**\n",
        "* Calculate the eigenvalues and eigenvectors of the sample covariance matrix. (**Hint:** use `np.linalg.eigh`, which finds the eigenvalues of a symmetric matrix).\n",
        "* Use the provided code to sort the eigenvalues in descending order.\n",
        "* Plot the eigenvectors on a scatter plot of the data, using the function `plot_basis_vectors`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIx-51gcpniL"
      },
      "source": [
        "help(sort_evals_descending)\n",
        "help(plot_basis_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFq1M2EcpniL"
      },
      "source": [
        "#################################################\n",
        "## TO DO for students: Calculate and sort the eigenvalues in descending order\n",
        "#################################################\n",
        "# Calculate the eigenvalues and eigenvectors\n",
        "# evals, evectors = ...\n",
        "# Sort the eigenvalues in descending order\n",
        "# evals, evectors = ...\n",
        "\n",
        "# plot_basis_vectors(X, evectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "c3abaac9-4a8f-437c-df9a-6b42b50d0b7e",
        "id": "a3FqtNxbpniL"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial2_Solution_ca961dca.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=274 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial2_Solution_ca961dca_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9O1bdK_pniL"
      },
      "source": [
        "---\n",
        "# Section 2: Perform PCA by projecting data onto the eigenvectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88pwA8f8pniL"
      },
      "source": [
        "To perform PCA, we will project the data onto the eigenvectors of the covariance matrix, i.e.:\n",
        "\\begin{equation}\n",
        "\\bf S = X W\n",
        "\\end{equation}\n",
        "where $\\bf S$ is an $N_\\text{samples} \\times N$ matrix representing the projected data (also called *scores*), and $W$ is an $N\\times N$ orthogonal matrix, each of whose columns represents the eigenvectors of the covariance matrix (also called *weights* or *loadings*). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzdndFqYpniL"
      },
      "source": [
        "## Exercise 3: PCA implementation\n",
        "\n",
        "You will now perform PCA on the data using the intuition you have developed so far. Fill in the function below to carry out the steps to perform PCA by projecting the data onto the eigenvectors of its covariance matrix.\n",
        "\n",
        "**Steps:**\n",
        "* First subtract the mean.\n",
        "* Then calculate the sample covariance matrix.\n",
        "* Then find the eigenvalues and eigenvectors and sort them in descending order.\n",
        "* Finally project the mean-centered data onto the eigenvectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSgqdpiTpniL"
      },
      "source": [
        "help(change_of_basis)\n",
        "help(plot_data_new_basis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l37DtY_MpniL"
      },
      "source": [
        "def pca(X):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats): Data matrix each column corresponds to a\n",
        "                               different random variable\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)  : Data projected onto the new basis\n",
        "    (numpy array of floats)  : Vector of eigenvalues\n",
        "    (numpy array of floats)  : Corresponding matrix of eigenvectors\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: calculate the covariance matrix\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: sort eigenvalues/eigenvectors!\")\n",
        "  #################################################\n",
        "\n",
        "  # Subtract the mean of X\n",
        "  X = ...\n",
        "  # Calculate the sample covariance matrix\n",
        "  cov_matrix = ...\n",
        "  # Calculate the eigenvalues and eigenvectors\n",
        "  evals, evectors = ...\n",
        "  # Sort the eigenvalues in descending order\n",
        "  evals, evectors = ...\n",
        "  # Project the data onto the new eigenvector basis\n",
        "  score = ...\n",
        "\n",
        "  return score, evectors, evals\n",
        "\n",
        "\n",
        "#################################################\n",
        "## TODO for students: Call the function to calculate the eigenvectors/eigenvalues\n",
        "#################################################\n",
        "\n",
        "# Perform PCA on the data matrix X\n",
        "# score, evectors, evals = ...\n",
        "# Plot the data projected into the new basis\n",
        "# plot_data_new_basis(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "0f0ac644-46f3-4f5a-87ef-6f40fa54fdef",
        "id": "mc5StbsopniL"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial2_Solution_d65c4e04.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=560 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial2_Solution_d65c4e04_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONV3SOscpniL"
      },
      "source": [
        "## Plot and explore the eigenvalues\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uq1t-EIpniL"
      },
      "source": [
        "   \n",
        "Finally, we will examine the eigenvalues of the covariance matrix. Remember that each eigenvalue describes the variance of the data projected onto its corresponding eigenvector. This is an important concept because it allows us to rank the PCA basis vectors based on how much variance each one can capture. First run the code below to plot the eigenvalues (sometimes called the \"scree plot\"). Which eigenvalue is larger?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzCyhTigpniM"
      },
      "source": [
        "plot_eigenvalues(evals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgTh_wenpniM"
      },
      "source": [
        "## Interactive Demo: Exploration of the correlation coefficient\n",
        "\n",
        "Run the following cell and use the slider to change the correlation coefficient in the data. You should see the scree plot and the plot of basis vectors update.\n",
        "\n",
        "**Questions:**\n",
        "* What happens to the eigenvalues as you change the correlation coefficient?\n",
        "* Can you find a value for which both eigenvalues are equal?\n",
        "* Can you find a value for which only one eigenvalue is nonzero?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JHwZ8zMoI1U"
      },
      "source": [
        "*hint*:\n",
        "1. observe the screen plot as you play with the widget\n",
        "2. at what correlation coefficient value are the basis vectors orthogonal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FNlyUSPrpniM"
      },
      "source": [
        "# @title\n",
        "\n",
        "# @markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "\n",
        "def refresh(corr_coef=.8):\n",
        "  cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
        "  X = get_data(cov_matrix)\n",
        "  score, evectors, evals = pca(X)\n",
        "  plot_eigenvalues(evals)\n",
        "  plot_basis_vectors(X, evectors)\n",
        "\n",
        "\n",
        "_ = widgets.interact(refresh, corr_coef=(-1, 1, .1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSwVvtVzpniM"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "- In this tutorial, we learned that goal of PCA is to find an orthonormal basis capturing the directions of maximum variance of the data. More precisely, the $i$th basis vector is the direction that maximizes the projected variance, while being orthogonal to all previous basis vectors. Mathematically, these basis vectors are the eigenvectors of the covariance matrix (also called *loadings*). \n",
        "- PCA also has the useful property that the projected data (*scores*) are uncorrelated.\n",
        "- The projected variance along each basis vector is given by its corresponding eigenvalue. This is important because it allows us rank the \"importance\" of each basis vector in terms of how much of the data variability it explains. An eigenvalue of zero means there is no variation along that direction so it can be dropped without losing any information about the original data.\n",
        "- In the next tutorial, we will use this property to reduce the dimensionality of high-dimensional data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5PJcvY1pniM"
      },
      "source": [
        "---\n",
        "# Bonus: Mathematical basis of PCA properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pmXwr3k3pniM"
      },
      "source": [
        "# @title Video 2: Properties of PCA\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"p56UrMRt6-U\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMa2Pdv_mXxq"
      },
      "source": [
        "# Summary of Video 2:\n",
        "\n",
        "mathematical basis behind the key\n",
        "\n",
        "properties of PCA.\n",
        "- The first property: wi \n",
        "are orthogonal to each other. So we know\n",
        "that the wi are eigen vectors of the\n",
        "covariance matrix. And we also know\n",
        "that the covariance matrix is symmetric\n",
        "so it is equal to its own transpose(wi are orthogonal\n",
        "to each other therefore wi dot wj is equal\n",
        "to 0). If the eigenvalues are different, then the\n",
        "eigenvectors must be orthogonal to each\n",
        "other. \n",
        "- The second property: the scores are\n",
        "uncorrelated and the scores\n",
        "again the projected data are\n",
        "uncorrelated with each other.\n"
      ]
    }
  ]
}