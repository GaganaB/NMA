{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W2D5_Tutorial4",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSl5a3hX-SNh"
      },
      "source": [
        "# Neuromatch Academy: Week 2, Day 5, Tutorial 4\n",
        "# From Reinforcement Learning to Planning\n",
        "\n",
        "__Content creators:__ Marcelo Mattar and Eric DeWitt with help from Byron Galbraith\n",
        "\n",
        "__Content reviewers:__ Matthew Krause and Michael Waskom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTr7YxpM-SNn"
      },
      "source": [
        "---\n",
        "\n",
        "# Tutorial Objectives\n",
        "  \n",
        "In this tutorial you will implement one of the simplest model-based Reinforcement Learning algorithms, Dyna-Q. You will understand what a world model is, how it can improve the agent's policy, and the situations in which model-based algorithms are more advantageous than their model-free counterparts.\n",
        "    \n",
        "* You will implement a model-based RL agent, Dyna-Q, that can solve a simple task;\n",
        "* You will investigate the effect of planning on the agent's behavior;\n",
        "* You will compare the behaviors of a model-based and model-free agent in light of an environmental change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByiFnvDD-SNo"
      },
      "source": [
        "---\n",
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjU7NWjZ-SNo"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve as conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oCuPWQ0I-SNp"
      },
      "source": [
        "#@title Figure settings\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHHFgisD-SNq"
      },
      "source": [
        "#@title Helper functions\n",
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  be_greedy = np.random.random() > epsilon\n",
        "  if be_greedy:\n",
        "    action = np.argmax(q)\n",
        "  else:\n",
        "    action = np.random.choice(len(q))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def q_learning(state, action, reward, next_state, value, params):\n",
        "  \"\"\"Q-learning: updates the value function and returns it.\n",
        "\n",
        "  Args:\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing the default parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  # value of previous state-action pair\n",
        "  prev_value = value[int(state), int(action)]\n",
        "\n",
        "  # maximum Q-value at current state\n",
        "  if next_state is None or np.isnan(next_state):\n",
        "      max_value = 0\n",
        "  else:\n",
        "      max_value = np.max(value[int(next_state)])\n",
        "\n",
        "  # reward prediction error\n",
        "  delta = reward + params['gamma'] * max_value - prev_value\n",
        "\n",
        "  # update value of previous state-action pair\n",
        "  value[int(state), int(action)] = prev_value + params['alpha'] * delta\n",
        "\n",
        "  return value\n",
        "\n",
        "\n",
        "def learn_environment(env, model_updater, planner, params, max_steps,\n",
        "                      n_episodes, shortcut_episode=None):\n",
        "  # Start with a uniform value function\n",
        "  value = np.ones((env.n_states, env.n_actions))\n",
        "\n",
        "  # Run learning\n",
        "  reward_sums = np.zeros(n_episodes)\n",
        "  episode_steps = np.zeros(n_episodes)\n",
        "\n",
        "  # Dyna-Q state\n",
        "  model = np.nan*np.zeros((env.n_states, env.n_actions, 2))\n",
        "\n",
        "  # Loop over episodes\n",
        "  for episode in range(n_episodes):\n",
        "    if shortcut_episode is not None and episode == shortcut_episode:\n",
        "      env.toggle_shortcut()\n",
        "      state = 64\n",
        "      action = 1\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "      model[state, action] = reward, next_state\n",
        "      value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "\n",
        "    state = env.init_state  # initialize state\n",
        "    reward_sum = 0\n",
        "\n",
        "    for t in range(max_steps):\n",
        "      # choose next action\n",
        "      action = epsilon_greedy(value[state], params['epsilon'])\n",
        "\n",
        "      # observe outcome of action on environment\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "\n",
        "      # sum rewards obtained\n",
        "      reward_sum += reward\n",
        "\n",
        "      # update value function\n",
        "      value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "      # update model\n",
        "      model = model_updater(model, state, action, reward, next_state)\n",
        "      # execute planner\n",
        "      value = planner(model, value, params)\n",
        "\n",
        "      if next_state is None:\n",
        "        break  # episode ends\n",
        "      state = next_state\n",
        "\n",
        "    reward_sums[episode] = reward_sum\n",
        "    episode_steps[episode] = t+1\n",
        "\n",
        "  return value, reward_sums, episode_steps\n",
        "\n",
        "\n",
        "class world(object):\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def get_outcome(self):\n",
        "        print(\"Abstract method, not implemented\")\n",
        "        return\n",
        "\n",
        "    def get_all_outcomes(self):\n",
        "        outcomes = {}\n",
        "        for state in range(self.n_states):\n",
        "            for action in range(self.n_actions):\n",
        "                next_state, reward = self.get_outcome(state, action)\n",
        "                outcomes[state, action] = [(1, next_state, reward)]\n",
        "        return outcomes\n",
        "\n",
        "class QuentinsWorld(world):\n",
        "    \"\"\"\n",
        "    World: Quentin's world.\n",
        "    100 states (10-by-10 grid world).\n",
        "    The mapping from state to the grid is as follows:\n",
        "    90 ...       99\n",
        "    ...\n",
        "    40 ...       49\n",
        "    30 ...       39\n",
        "    20 21 22 ... 29\n",
        "    10 11 12 ... 19\n",
        "    0  1  2  ...  9\n",
        "    54 is the start state.\n",
        "    Actions 0, 1, 2, 3 correspond to right, up, left, down.\n",
        "    Moving anywhere from state 99 (goal state) will end the session.\n",
        "    Landing in red states incurs a reward of -1.\n",
        "    Landing in the goal state (99) gets a reward of 1.\n",
        "    Going towards the border when already at the border will stay in the same\n",
        "        place.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.name = \"QuentinsWorld\"\n",
        "        self.n_states = 100\n",
        "        self.n_actions = 4\n",
        "        self.dim_x = 10\n",
        "        self.dim_y = 10\n",
        "        self.init_state = 54\n",
        "        self.shortcut_state = 64\n",
        "\n",
        "    def toggle_shortcut(self):\n",
        "      if self.shortcut_state == 64:\n",
        "        self.shortcut_state = 2\n",
        "      else:\n",
        "        self.shortcut_state = 64\n",
        "\n",
        "    def get_outcome(self, state, action):\n",
        "        if state == 99:  # goal state\n",
        "            reward = 0\n",
        "            next_state = None\n",
        "            return next_state, reward\n",
        "        reward = 0  # default reward value\n",
        "        if action == 0:  # move right\n",
        "            next_state = state + 1\n",
        "            if state == 98:  # next state is goal state\n",
        "                reward = 1\n",
        "            elif state % 10 == 9:  # right border\n",
        "                next_state = state\n",
        "            elif state in [11, 21, 31, 41, 51, 61, 71,\n",
        "                           12, 72,\n",
        "                           73,\n",
        "                           14, 74,\n",
        "                           15, 25, 35, 45, 55, 65, 75]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 1:  # move up\n",
        "            next_state = state + 10\n",
        "            if state == 89:  # next state is goal state\n",
        "                reward = 1\n",
        "            if state >= 90:  # top border\n",
        "                next_state = state\n",
        "            elif state in [2, 12, 22, 32, 42, 52, 62,\n",
        "                           3, 63,\n",
        "                           self.shortcut_state,\n",
        "                           5, 65,\n",
        "                           6, 16, 26, 36, 46, 56, 66]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 2:  # move left\n",
        "            next_state = state - 1\n",
        "            if state % 10 == 0:  # left border\n",
        "                next_state = state\n",
        "            elif state in [17, 27, 37, 47, 57, 67, 77,\n",
        "                           16, 76,\n",
        "                           75,\n",
        "                           14, 74,\n",
        "                           13, 23, 33, 43, 53, 63, 73]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 3:  # move down\n",
        "            next_state = state - 10\n",
        "            if state <= 9:  # bottom border\n",
        "                next_state = state\n",
        "            elif state in [22, 32, 42, 52, 62, 72, 82,\n",
        "                           23, 83,\n",
        "                           84,\n",
        "                           25, 85,\n",
        "                           26, 36, 46, 56, 66, 76, 86]:  # next state is red\n",
        "                reward = -1\n",
        "        else:\n",
        "            print(\"Action must be between 0 and 3.\")\n",
        "            next_state = None\n",
        "            reward = None\n",
        "        return int(next_state) if next_state is not None else None, reward\n",
        "\n",
        "\n",
        "# HELPER FUNCTIONS FOR PLOTTING\n",
        "\n",
        "def plot_state_action_values(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing value of each action at each state.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  for a in range(env.n_actions):\n",
        "    ax.plot(range(env.n_states), value[:, a], marker='o', linestyle='--')\n",
        "  ax.set(xlabel='States', ylabel='Values')\n",
        "  ax.legend(['R','U','L','D'], loc='lower right')\n",
        "\n",
        "\n",
        "def plot_quiver_max_action(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing action of maximum value or maximum probability at\n",
        "    each state (not for n-armed bandit or cheese_world).\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  X = np.tile(np.arange(env.dim_x), [env.dim_y,1]) + 0.5\n",
        "  Y = np.tile(np.arange(env.dim_y)[::-1][:,np.newaxis], [1,env.dim_x]) + 0.5\n",
        "  which_max = np.reshape(value.argmax(axis=1), (env.dim_y,env.dim_x))\n",
        "  which_max = which_max[::-1,:]\n",
        "  U = np.zeros(X.shape)\n",
        "  V = np.zeros(X.shape)\n",
        "  U[which_max == 0] = 1\n",
        "  V[which_max == 1] = 1\n",
        "  U[which_max == 2] = -1\n",
        "  V[which_max == 3] = -1\n",
        "\n",
        "  ax.quiver(X, Y, U, V)\n",
        "  ax.set(\n",
        "      title='Maximum value/probability actions',\n",
        "      xlim=[-0.5, env.dim_x+0.5],\n",
        "      ylim=[-0.5, env.dim_y+0.5],\n",
        "  )\n",
        "  ax.set_xticks(np.linspace(0.5, env.dim_x-0.5, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_xticks(np.arange(env.dim_x+1), minor=True)\n",
        "  ax.set_yticks(np.linspace(0.5, env.dim_y-0.5, num=env.dim_y))\n",
        "  ax.set_yticklabels([\"%d\" % y for y in np.arange(0, env.dim_y*env.dim_x, env.dim_x)])\n",
        "  ax.set_yticks(np.arange(env.dim_y+1), minor=True)\n",
        "  ax.grid(which='minor',linestyle='-')\n",
        "\n",
        "\n",
        "def plot_heatmap_max_val(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate heatmap showing maximum value at each state\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  if value.ndim == 1:\n",
        "      value_max = np.reshape(value, (env.dim_y,env.dim_x))\n",
        "  else:\n",
        "      value_max = np.reshape(value.max(axis=1), (env.dim_y,env.dim_x))\n",
        "  value_max = value_max[::-1,:]\n",
        "\n",
        "  im = ax.imshow(value_max, aspect='auto', interpolation='none', cmap='afmhot')\n",
        "  ax.set(title='Maximum value per state')\n",
        "  ax.set_xticks(np.linspace(0, env.dim_x-1, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_yticks(np.linspace(0, env.dim_y-1, num=env.dim_y))\n",
        "  if env.name != 'windy_cliff_grid':\n",
        "      ax.set_yticklabels(\n",
        "          [\"%d\" % y for y in np.arange(\n",
        "              0, env.dim_y*env.dim_x, env.dim_x)][::-1])\n",
        "  return im\n",
        "\n",
        "\n",
        "def plot_rewards(n_episodes, rewards, average_range=10, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing total reward accumulated in each episode.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  smoothed_rewards = (conv(rewards, np.ones(average_range), mode='same')\n",
        "                      / average_range)\n",
        "\n",
        "  ax.plot(range(0, n_episodes, average_range),\n",
        "          smoothed_rewards[0:n_episodes:average_range],\n",
        "          marker='o', linestyle='--')\n",
        "  ax.set(xlabel='Episodes', ylabel='Total reward')\n",
        "\n",
        "\n",
        "def plot_performance(env, value, reward_sums):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "  plot_state_action_values(env, value, ax=axes[0,0])\n",
        "  plot_quiver_max_action(env, value, ax=axes[0,1])\n",
        "  plot_rewards(n_episodes, reward_sums, ax=axes[1,0])\n",
        "  im = plot_heatmap_max_val(env, value, ax=axes[1,1])\n",
        "  fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSUjfEP9E2JY"
      },
      "source": [
        "# Summary of Helper Functions:\n",
        "\n",
        "*epsilon_greedy*:\n",
        "Epsilon-greedy policy: selects the maximum value action with probability\n",
        " (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "*q_learning*:\n",
        "updates value function:\n",
        "calculate value of previous state-action pair; find maximum Q-value at current state; find reward prediction error and update value of previous state-action pair\n",
        " \n",
        "*learn_environment*:\n",
        "we start with a uniform value function and learn over episodes with Dyna-Q states; \n",
        "at every step over all episodes: \n",
        "1. choose next action;\n",
        "2. observe outcome of action on environment\n",
        "3. update value function; update model and execute planner.\n",
        "\n",
        "World:\n",
        "\t- *get_all_outcome*:\n",
        "find all outcomes\n",
        "\n",
        "- QuentinsWorld:\n",
        "World: Quentin's world.\n",
        "    100 states (10-by-10 grid world).\n",
        "    The mapping from state to the grid is as follows:\n",
        "    90 ...       99\n",
        "    ...\n",
        "    40 ...       49\n",
        "    30 ...       39\n",
        "    20 21 22 ... 29\n",
        "    10 11 12 ... 19\n",
        "    0  1  2  ...  9\n",
        "    54 is the start state.\n",
        "    Actions 0, 1, 2, 3 correspond to right, up, left, down.\n",
        "    Moving anywhere from state 99 (goal state) will end the session.\n",
        "    Landing in red states incurs a reward of -1.\n",
        "    Landing in the goal state (99) gets a reward of 1.\n",
        "    Going towards the border when already at the border will stay in the same place.\n",
        "\n",
        "\t- *toggle_shortcut*:\n",
        "if in shortcut_state, set to 2. else, stay in shortcut_state;\n",
        "\n",
        "\t- *get_outcome*:\n",
        "  dealing with edge cases:\n",
        "1. goal state => reward =0 and there is no next state.\n",
        "2. default reward = -1\n",
        "3. start state => reward =-100 and there's no next state (cliff: states between 11 and 18)\n",
        "4. move right => state+1\n",
        "5. move up => state+10\n",
        "6. move left => state -1\n",
        "7. move down => state-10\n",
        "8. if border => next_state = state\n",
        "\n",
        "*plot_state_action_value*:\n",
        "Generate plot showing value of each action at each state.\n",
        "\n",
        "*plot_quiver_max_action(env, value, ax=None)*:\n",
        " Generate plot showing action of maximum value or maximum probability at each state.\n",
        "\n",
        "*plot_heatmap_max_val*:\n",
        "Generate heatmap showing maximum value at each state\n",
        "\n",
        "*plot_rewards*:\n",
        "Generate plot showing total reward(yaxis) accumulated in each episode(xaxis).\n",
        "\n",
        "*plot_performance*:\n",
        "plots state action values, max action, and rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnapJm9D-SNs"
      },
      "source": [
        "---\n",
        "\n",
        "# Section 1: Model-based RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qTEzj-X_-SNs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "352a8691-8681-421f-8547-56edb2c4fa7d"
      },
      "source": [
        "#@title Video 1: Model-based RL\n",
        "# Insert the ID of the corresponding youtube video\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"zT_legTotF0\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/zT_legTotF0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/zT_legTotF0?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fb013a648d0>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjUC1K7nG6-z"
      },
      "source": [
        "# Summary of Video 1:\n",
        "\n",
        "- model-based\n",
        "reinforcement learning.\n",
        "- it's a great framework for understanding a variety of\n",
        "fascinating topics in neuroscience and cognitive science, such as planning, memory replay,\n",
        "memory consolidation & even dreaming. \n",
        "- we've seen how agents learn to act by acting in the world. So, it's like trial and error.\n",
        "They act. They experience the outcome of your actions, and then they learn from those outcomes.\n",
        "So, these models are called model-free, because they do not require a model. And in fact,\n",
        "that's the case for both Q-learning and SARSA algorithms; \n",
        "- learn about model-based methods, which compute actions by\n",
        "a process called planning. This means that instead of computing values from experience,\n",
        "planning will allow the engine to compute actions from a model.\n",
        "- A model is a representation of how the world might respond to the agent's actions. In some way,\n",
        "you can think of a model as a representation of the environment that lives inside the agent.\n",
        "So formally, you define a model, as a mathematical object such that, given a state in an action,\n",
        "it outputs a prediction of the resultant\n",
        "state and reward.\n",
        "Just like an environment works,\n",
        "except that it's inside the agent. \n",
        "- And the advantage of having a model is that\n",
        "you can use it to mimic or simulate real experience.\n",
        "But if you can simulate experience, you can then learn from those simulations; a model-free reinforcement learning, the agent uses real\n",
        "experience to compute values and ultimately, a policy, via a process called learning.\n",
        "- In model-based reinforcement learning, however,\n",
        "the agent instead uses simulated experience\n",
        "which is produced from a model, to compute those values. In the process of computing values, and\n",
        "ultimately a policy from simulated experiences, what we call planning.\n",
        "But, it turns out that planning and learning are not necessarily mutually\n",
        "exclusive. It's fairly easy to program an agent that can implement both\n",
        "things concurrently \n",
        "- the agent has a value function indicating the values of different actions, and\n",
        "using those values, the agent can then select and execute an action\n",
        "producing real experience. And\n",
        "then, this real experience can be used in two ways .\n",
        "1. First, it can be used in the usual way using model-free learning algorithms such as Q-learning & SARSA\n",
        "to teach the animal, or the agent about action values.\n",
        "2. But, the agent can also use real experience in a different way. For instance, the agent can learn about\n",
        "how the world works - how it evolves, how you respond to each action, and so on\n",
        "In other words, the agent can learn a model; with a model available,\n",
        "the agent can then use planning to produce its own simulated experiences and then to learn from them.\n",
        "- So, Dyna-Q is\n",
        "possibly the simplest\n",
        "reinforcement model-based reinforcement learning algorithm which integrates learning and planning.\n",
        "- analyze Dyna-Q in detail, implement Q-learning in steps; \n",
        "the agent observes, chooses an action, takes the action and observes the result, and then learns from the outcome; the agent learns the model, simply saves and remembers\n",
        "what was the result of the action that it executed. And then, implements planning.\n",
        "In loops and times,\n",
        "selecting a state and an action, to simulate, remembering what were the results by drawing the\n",
        "result from the model and then learning that in the very last line of this algorithm.\n",
        "- Now, what's the relevance of this for Neuroscience? Well, for one, I'm sure we would all agree that\n",
        "our brains are sometimes focused on things other than what's happening around us.\n",
        "For example, we may remember things that\n",
        "have happened in the past, or we may think about things that might happen in the future.\n",
        "And of course, this is not an accident of evolution.\n",
        "It's really not difficult for us to see why it might be useful for us to learn from\n",
        "imagined or remembered experiences in addition to real experiences.\n",
        "But, we don't have to rely only on our\n",
        "phenomenological experiences to study disability.\n",
        "So, it turns out that in the last couple decades, we've been able to record\n",
        "neural activity that relates precisely to this phenomena.\n",
        "This is called the hippocampal replay;  during sleep,\n",
        "observe that in both regions, both in the hippocampus and individual cortex,\n",
        "there were brief moments in time, when the neural activity resembled what had happened\n",
        "during awake behavior with the caveat that those sequences were a bit compressed in time.\n",
        "But, they really looked a lot like what had been observed during awake behavior; perhaps the hippocampus is engaged in model-based planning, simulating experiences from the past and\n",
        "then the cortex is learning from those replayed experiences that the hippocampus is\n",
        "producing. This in fact, is a long-standing idea about how\n",
        "the brain consolidates memories in the process called systems consolidation.\n",
        "But a similar phenomenon can actually happen also during awake behavior.\n",
        "Turns out that if you record from place cells in the hippocampus, as the animal navigates, while you know,\n",
        "most of the times you're seeing these cells firing according to the actual location of the animal.\n",
        "This is just how place cells work.\n",
        "Sometimes, if the animal stops, for instance,\n",
        "if it arrives at some bifurcation in a maze, you will find that place cells are not representing the\n",
        "actual location of the animal, but instead, they can represent future locations\n",
        "just like if the animal was remembering or imagining what might happen next.\n",
        "This is known as awake replay, and is thought to be the neural basis of planning.\n",
        "Sometimes they can also spend, in backwards\n",
        "behind the animal, and sometimes they can represent remote locations all together. So,\n",
        "it's difficult to imagine how all of this is serving the role of planning.\n",
        "reinforcement learning is a\n",
        "mathematical language can be used to explain all of these findings, and in fact, to link\n",
        "various ideas from memory retrieval to planning,\n",
        "consolidation, and even dreaming, \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRjzCN-SNt"
      },
      "source": [
        "The algorithms introduced in the previous tutorials are all *model-free*, as they do not require a model to use or control behavior. In this section, we will study a different class of algorithms called model-based. As we will see next, in contrast to model-free RL, model-based methods use a model to build a policy.\n",
        "\n",
        "But what is a model? A model (sometimes called a world model or internal model) is a representation of how the world will respond to the agent's actions. You can think of it as a representation of how the world *works*. With such a representation, the agent can simulate new experiences and learn from these simulations. This is advantageous for two reasons. First, acting in the real world can be costly and sometimes even dangerous: remember Cliff World from Tutorial 3? Learning from simulated experience can avoid some of these costs or risks. Second, simulations make fuller use of one's limited experience. To see why, imagine an agent interacting with the real world. The information acquired with each individual action can only be assimilated at the moment of the interaction. In contrast, the experiences simulated from a model can be simulated multiple times -- and whenever desired -- allowing for the information to be more fully assimilated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfEtu8Fy-SNt"
      },
      "source": [
        "## Section 1.1 Quentin's World Environment\n",
        "\n",
        "In this tutorial, our RL agent will act in the Quentin's world, a 10x10 grid world. \n",
        "\n",
        "<img alt=\"QuentinsWorld\" width=\"560\" height=\"560\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial4_QuentinsWorld.png?raw=true\">\n",
        "\n",
        "In this environment, there are 100 states and 4 possible actions: right, up, left, and down. The goal of the agent is to move, via a series of steps, from the start (green) location to the goal (yellow) region, while avoiding the red walls. More specifically:\n",
        "* The agent starts in the green state,\n",
        "* Moving into one of the red states incurs a reward of -1,\n",
        "* Moving into the world borders stays in the same place,\n",
        "* Moving into the goal state (yellow square in the upper right corner) gives you a reward of 1, and\n",
        "* Moving anywhere from the goal state ends the episode.\n",
        "\n",
        "Now that we have our environment and task defined, how can we solve this using a model-based RL agent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdZBStlb-SNu"
      },
      "source": [
        "---\n",
        "# Section 2: Dyna-Q\n",
        "\n",
        "In this section, we will implement Dyna-Q, one of the simplest model-based reinforcement learning algorithms. A Dyna-Q agent combines acting, learning, and planning. The first two components -- acting and learning -- are just like what we have studied previously. Q-learning, for example, learns by acting in the world, and therefore combines acting and learning. But a Dyna-Q agent also implements planning, or simulating experiences from a model--and learns from them. \n",
        "\n",
        "In theory, one can think of a Dyna-Q agent as implementing acting, learning, and planning simultaneously, at all times. But, in practice, one needs to specify the algorithm as a sequence of steps. The most common way in which the Dyna-Q agent is implemented is by adding a planning routine to a Q-learning agent: after the agent acts in the real world and learns from the observed experience, the agent is allowed a series of $k$ *planning steps*. At each one of those $k$ planning steps, the model generates a simulated experience by randomly sampling from the history of all previously experienced state-action pairs. The agent then learns from this simulated experience, again using the same Q-learning rule that you implemented for learning from real experience. This simulated experience is simply a one-step transition, i.e., a state, an action, and the resulting state and reward. So, in practice, a Dyna-Q agent learns (via Q-learning) from one step of **real** experience during acting, and then from k steps of **simulated** experience during planning.\n",
        "\n",
        "There's one final detail about this algorithm: where does the simulated experiences come from or, in other words, what is the \"model\"? In Dyna-Q, as the agent interacts with the environment, the agent also learns the model. For simplicity, Dyna-Q implements model-learning in an almost trivial way, as simply caching the results of each transition. Thus, after each one-step transition in the environment, the agent saves the results of this transition in a big matrix, and consults that matrix during each of the planning steps. Obviously, this model-learning strategy only makes sense if the world is deterministic (so that each state-action pair always leads to the same state and reward), and this is the setting of the exercise below. However, even this simple setting can already highlight one of Dyna-Q major strengths: the fact that the planning is done at the same time as the agent interacts with the environment, which means that new information gained from the interaction may change the model and thereby interact with planning in potentially interesting ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLlUjDNh-SNw"
      },
      "source": [
        "Since you already implemented Q-learning in the previous tutorial, we will focus here on the extensions new to Dyna-Q: the model update step and the planning step. For reference, here's the Dyna-Q algorithm that you will help implement:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL5BmM24-SNw"
      },
      "source": [
        "---\n",
        "**TABULAR DYNA-Q**\n",
        "\n",
        "Initialize $Q(s,a)$ and $Model(s,a)$ for all $s \\in S$ and $a \\in A$.\n",
        "\n",
        "Loop forever:\n",
        "\n",
        "> (a) $S$ &larr; current (nonterminal) state <br>\n",
        "> (b) $A$ &larr; $\\epsilon$-greedy$(S,Q)$ <br>\n",
        "> (c) Take action $A$; observe resultant reward, $R$, and state, $S'$ <br>\n",
        "> (d) $Q(S,A)$ &larr; $Q(S,A) + \\alpha \\left[R + \\gamma \\max_{a} Q(S',a) - Q(S,A)\\right]$ <br>\n",
        "> (e) $Model(S,A)$ &larr; $R,S'$ (assuming deterministic environment) <br>\n",
        "> (f) Loop repeat $k$ times: <br>\n",
        ">> $S$ &larr; random previously observed state <br>\n",
        ">> $A$ &larr; random action previously taken in $S$ <br>\n",
        ">> $R,S'$ &larr; $Model(S,A)$ <br>\n",
        ">> $Q(S,A)$ &larr; $Q(S,A) + \\alpha \\left[R + \\gamma \\max_{a} Q(S',a) - Q(S,A)\\right]$ <br>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQLgoiFi-SNw"
      },
      "source": [
        "## Exercise 1: Dyna-Q Model Update\n",
        "\n",
        "In this exercise you will implement the model update portion of the Dyna-Q algorithm. More specifically, after each action that the agent executes in the world, we need to update our model to remember what reward and next state we last experienced for the given state-action pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Yexbzr-SNw"
      },
      "source": [
        "def dyna_q_model_update(model, state, action, reward, next_state):\n",
        "  \"\"\" Dyna-Q model update\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated model\n",
        "  \"\"\"\n",
        "  ###############################################################\n",
        "  ## TODO for students: implement the model update step of Dyna-Q\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement the model update step of Dyna-Q\")\n",
        "  ###############################################################\n",
        "  # Update our model with the observed reward and next state\n",
        "  model[...] = ...\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYMV7t6K-SNw"
      },
      "source": [
        "# to_remove solution\n",
        "def dyna_q_model_update(model, state, action, reward, next_state):\n",
        "  \"\"\" Dyna-Q model update\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated model\n",
        "  \"\"\"\n",
        "  # Update our model with the observed reward and next state\n",
        "  model[state, action] = reward, next_state\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Hri7da-SNw"
      },
      "source": [
        "Now that we have a way to update our model, we can use it in the planning phase of Dyna-Q to simulate past experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O2G6UE3-SNw"
      },
      "source": [
        "## Exercise 2: Dyna-Q Planning\n",
        "\n",
        "In this exercise you will implement the other key part of Dyna-Q: planning. We will sample a random state-action pair from those we've experienced, use our model to simulate the experience of taking that action in that state, and update our value function using Q-learning with these simulated state, action, reward, and next state outcomes. Furthermore, we want to run this planning step $k$ times, which can be obtained from `params['k']`.\n",
        "\n",
        "For this exercise, you may use the `q_learning` function to handle the Q-learning value function update. Recall that the method signature is `q_learning(state, action, reward, next_state, value, params)` and it returns the updated `value` table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tAxyVHv-SNx"
      },
      "source": [
        "def dyna_q_planning(model, value, params):\n",
        "  \"\"\" Dyna-Q planning\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing learning parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  ############################################################\n",
        "  ## TODO for students: implement the planning step of Dyna-Q\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement the planning step of Dyna-Q\")\n",
        "  #############################################################\n",
        "  # Perform k additional updates at random (planning)\n",
        "  for _ in range(...):\n",
        "    # Find state-action combinations for which we've experienced a reward i.e.\n",
        "    # the reward value is not NaN. The outcome of this expression is an Nx2\n",
        "    # matrix, where each row is a state and action value, respectively.\n",
        "    candidates = np.array(np.where(~np.isnan(model[:,:,0]))).T\n",
        "\n",
        "    # Write an expression for selecting a random row index from our candidates\n",
        "    idx = ...\n",
        "\n",
        "    # Obtain the randomly selected state and action values from the candidates\n",
        "    state, action = ...\n",
        "\n",
        "    # Obtain the expected reward and next state from the model\n",
        "    reward, next_state = ...\n",
        "\n",
        "    # Update the value function using Q-learning\n",
        "    value = ...\n",
        "\n",
        "  return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUH5YRfr-SNx"
      },
      "source": [
        "# to_remove solution\n",
        "def dyna_q_planning(model, value, params):\n",
        "  \"\"\" Dyna-Q planning\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing learning parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  # Perform k additional updates at random (planning)\n",
        "  for _ in range(params['k']):\n",
        "    # Find state-action combinations for which we've experienced a reward i.e.\n",
        "    # the reward value is not NaN. The outcome of this expression is an Nx2\n",
        "    # matrix, where each row is a state and action value, respectively.\n",
        "    candidates = np.array(np.where(~np.isnan(model[:,:,0]))).T\n",
        "\n",
        "    # Write an expression for selecting a random row index from our candidates\n",
        "    idx = np.random.choice(len(candidates))\n",
        "\n",
        "    # Obtain the randomly selected state and action values from the candidates\n",
        "    state, action = candidates[idx]\n",
        "\n",
        "    # Obtain the expected reward and next state from the model\n",
        "    reward, next_state = model[state, action]\n",
        "\n",
        "    # Update the value function using Q-learning\n",
        "    value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "  return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg97_XKw-SNx"
      },
      "source": [
        "With a way to update our model and a means to use it in planning, it is time to see it in action. The following code sets up the our agent parameters and learning environment, then passes your model update and planning methods to the agent to try and solve Quentin's World. Notice that we set the number of planning steps $k=10$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSEeBSse-SNy"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "  'k': 10,  # number of Dyna-Q planning steps\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 500\n",
        "max_steps = 1000\n",
        "\n",
        "# environment initialization\n",
        "env = QuentinsWorld()\n",
        "\n",
        "# solve Quentin's World using Dyna-Q\n",
        "results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                            params, max_steps, n_episodes)\n",
        "value, reward_sums, episode_steps = results\n",
        "\n",
        "plot_performance(env, value, reward_sums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CoppIyY-SNy"
      },
      "source": [
        "Upon completion, we should see that our Dyna-Q agent is able to solve the task quite quickly, achieving a consistent positive reward after only a limited number of episodes (bottom left)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41r6aF4l-SNy"
      },
      "source": [
        "---\n",
        "# Section 3: How much to plan?\n",
        "\n",
        "Now that you implemented a Dyna-Q agent with $k=10$, we will try to understand the effect of planning on performance. How does changing the value of $k$ impact our agent's ability to learn?\n",
        "\n",
        "The following code is similar to what we just ran, only this time we run several experiments over several different values of $k$ to see how their average performance compares. In particular, we will choose $k \\in \\{0, 1, 10, 100\\}$. Pay special attention to the case where $k = 0$ which corresponds to no planning. This is, in effect, just regular Q-learning.\n",
        "\n",
        "The following code will take a bit of time to complete. To speed things up, try lowering the number of experiments or the number of $k$ values to compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtHCDPh-SNy"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_experiments = 10\n",
        "n_episodes = 100\n",
        "max_steps = 1000\n",
        "\n",
        "# number of planning steps\n",
        "planning_steps = np.array([0, 1, 10, 100])\n",
        "\n",
        "# environment initialization\n",
        "env = QuentinsWorld()\n",
        "\n",
        "steps_per_episode = np.zeros((len(planning_steps), n_experiments, n_episodes))\n",
        "\n",
        "for i, k in enumerate(planning_steps):\n",
        "  params['k'] = k\n",
        "  for experiment in range(n_experiments):\n",
        "    results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                                params, max_steps, n_episodes)\n",
        "    steps_per_episode[i, experiment] = results[2]\n",
        "\n",
        "# Average across experiments\n",
        "steps_per_episode = np.mean(steps_per_episode, axis=1)\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps_per_episode.T)\n",
        "ax.set(xlabel='Episodes', ylabel='Steps per episode',\n",
        "       xlim=[20, None], ylim=[0, 160])\n",
        "ax.legend(planning_steps, loc='upper right', title=\"Planning steps\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnpS8ulC-SNz"
      },
      "source": [
        "After an initial warm-up phase of the first 20 episodes, we should see that the number of planning steps has a noticeable impact on our agent's ability to rapidly solve the environment. We should also notice that after a certain value of $k$ our relative utility goes down, so it's important to balance a large enough value of $k$ that helps us learn quickly without wasting too much time in planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRws7ZQ-SNz"
      },
      "source": [
        "---\n",
        "# Section 4: When the world changes...\n",
        "\n",
        "In addition to speeding up learning about a new environment, planning can also help the agent to quickly incorporate new information about the environment into its policy. Thus, if the environment changes (e.g. the rules governing the transitions between states, or the rewards associated with each state/action), the agent doesn't need to experience that change *repeatedly* (as would be required in a Q-learning agent) in real experience. Instead, planning allows that change to be incorporated quickly into the agent's policy, without the need to experience the change more than once.\n",
        "\n",
        "In this final section, we will again have our agents attempt to solve Quentin's World. However, after 200 episodes, a shortcut will appear in the environment.  We will test how a model-free agent using Q-learning and a Dyna-Q agent adapt to this change in the environment.\n",
        "\n",
        "<img alt=\"QuentinsWorldShortcut\" width=\"560\" height=\"560\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial4_QuentinsWorldShortcut.png?raw=true\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h0F0oYQ-SNz"
      },
      "source": [
        "The following code again looks similar to what we've run previously. Just as above we will have multiple values for $k$, with $k=0$ representing our Q-learning agent and $k=10$ for our Dyna-Q agent with 10 planning steps. The main difference is we now add in an indicator as to when the shortcut appears. In particular, we will run the agents for 400 episodes, with the shortcut appearing in the middle after episode #200.\n",
        "\n",
        "When this shortcut appears we will also let each agent experience this change once i.e. we will evaluate the act of moving upwards when in the state that is below the now-open shortcut. After this single demonstration, the agents will continue on interacting in the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnK0DdYK-SN0"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 400\n",
        "max_steps = 1000\n",
        "shortcut_episode = 200  # when we introduce the shortcut\n",
        "\n",
        "# number of planning steps\n",
        "planning_steps = np.array([0, 10]) # Q-learning, Dyna-Q (k=10)\n",
        "\n",
        "# environment initialization\n",
        "steps_per_episode = np.zeros((len(planning_steps), n_episodes))\n",
        "\n",
        "# Solve Quentin's World using Q-learning and Dyna-Q\n",
        "for i, k in enumerate(planning_steps):\n",
        "  env = QuentinsWorld()\n",
        "  params['k'] = k\n",
        "  results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                              params, max_steps, n_episodes,\n",
        "                              shortcut_episode=shortcut_episode)\n",
        "  steps_per_episode[i] = results[2]\n",
        "\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps_per_episode.T)\n",
        "ax.set(xlabel='Episode', ylabel='Steps per Episode',\n",
        "       xlim=[20,None], ylim=[0, 160])\n",
        "ax.axvline(shortcut_episode, linestyle=\"--\", color='gray', label=\"Shortcut appears\")\n",
        "ax.legend(('Q-learning', 'Dyna-Q', 'Shortcut appears'),\n",
        "          loc='upper right');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWvCfnS-SN0"
      },
      "source": [
        "If all went well, we should see the Dyna-Q agent having already achieved near optimal performance before the appearance of the shortcut and then immediately incorporating this new information to further improve. In this case, the Q-learning agent takes much longer to fully incorporate the new shortcut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbAZeMUz-SN0"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "In this notebook, you have learned about model-based reinforcement learning and implemented one of the simplest architectures of this type, Dyna-Q. Dyna-Q is very much like Q-learning, but instead of learning only from real experience, you also learn from **simulated** experience. This small difference, however, can have huge benefits! Planning *frees* the agent from the limitation of its own environment, and this in turn allows the agent to speed-up learning -- for instance, effectively incorporating environmental changes into one's policy.\n",
        "\n",
        "Not surprisingly, model-based RL is an active area of research in machine learning. Some of the exciting topics in the frontier of the field involve (i) learning and representing a complex world model (i.e., beyond the tabular and deterministic case above), and (ii) what to simulate -- also known as search control -- (i.e., beyond the random selection of experiences implemented above).\n",
        "\n",
        "The framework above has also been used in neuroscience to explain various phenomena such as planning, memory sampling, memory consolidation, and even dreaming!"
      ]
    }
  ]
}