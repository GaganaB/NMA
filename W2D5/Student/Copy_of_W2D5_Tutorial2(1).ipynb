{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W2D5_Tutorial2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evSRB0BRSUYN"
      },
      "source": [
        "# Neuromatch Academy: Week 2, Day 5, Tutorial 2\n",
        "# Learning to Act: Multi-Armed Bandits\n",
        "\n",
        "__Content creators:__ Marcelo Mattar and Eric DeWitt with help from Byron Galbraith\n",
        "\n",
        "__Content reviewers:__ Matt Krause and Michael Waskom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9elpMxJ6SUYW"
      },
      "source": [
        "---\n",
        "\n",
        "# Tutorial Objectives\n",
        "  \n",
        "In this tutorial you will use 'bandits' to understand the fundamentals of how a policy interacts with the learning algorithm in reinforcement learning.\n",
        "    \n",
        "* You will understand the fundamental tradeoff between exploration and exploitation in a policy.\n",
        "* You will understand how the learning rate interacts with exploration to find the best available action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEsKWM6ASUYX"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "xiIbvQTKSUYY"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hw4_thqSSUYZ"
      },
      "source": [
        "#@title Figure settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0Lz4zjW5SUYZ"
      },
      "source": [
        "#@title Helper functions\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "def plot_choices(q, epsilon, choice_fn, n_steps=1000, rng_seed=1):\n",
        "  np.random.seed(rng_seed)\n",
        "  counts = np.zeros_like(q)\n",
        "  for t in range(n_steps):\n",
        "    action = choice_fn(q, epsilon)\n",
        "    counts[action] += 1\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(range(len(q)), counts/n_steps)\n",
        "  ax.set(ylabel='% chosen', xlabel='action', ylim=(0,1), xticks=range(len(q)))\n",
        "\n",
        "\n",
        "def plot_multi_armed_bandit_results(results):\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 4))\n",
        "  ax1.plot(results['rewards'])\n",
        "  ax1.set(title=f\"Total Reward: {np.sum(results['rewards']):.2f}\",\n",
        "          xlabel='step', ylabel='reward')\n",
        "  ax2.plot(results['qs'])\n",
        "  ax2.set(xlabel='step', ylabel='value')\n",
        "  ax2.legend(range(len(results['mu'])))\n",
        "  ax3.plot(results['mu'], label='latent')\n",
        "  ax3.plot(results['qs'][-1], label='learned')\n",
        "  ax3.set(xlabel='action', ylabel='value')\n",
        "  ax3.legend()\n",
        "\n",
        "\n",
        "def plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal):\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
        "\n",
        "  ax1.plot(np.mean(trial_rewards, axis=1).T)\n",
        "  ax1.set(title=f'Average Reward ({fixed})', xlabel='step', ylabel='reward')\n",
        "  ax1.legend(labels)\n",
        "\n",
        "  ax2.plot(np.mean(trial_optimal, axis=1).T)\n",
        "  ax2.set(title=f'Performance ({fixed})', xlabel='step', ylabel='% optimal')\n",
        "  ax2.legend(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4XeMrvUjKV"
      },
      "source": [
        "# Summary of Helper functions:\n",
        "\n",
        "*plot_choices*:\n",
        "plot % of chosen (yaxis) vs action (xaxis). \n",
        "\n",
        "*plot_multi_armed_bandit_results*:\n",
        "plot total reward with step(xaxis) vs reward/ value (yaxis) and action (xaxis) vs value (yaxis)\n",
        "\n",
        "*plot_parameter_performance*:\n",
        "plot average reward with step (xaxis) vs reward (yaxis)\n",
        "plot performance with step (xaxis) vs % optimal (yaxis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXJXxtJWSUYb"
      },
      "source": [
        "---\n",
        "# Section 1: Multi-Armed Bandits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "sSSReR1mSUYb"
      },
      "source": [
        "#@title Video 1: Multi-Armed Bandits\n",
        "# Insert the ID of the corresponding youtube video\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"kdiXr1zsfo0\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UL2s1sArNt7"
      },
      "source": [
        "# Summary of Video 1:\n",
        "\n",
        "- going to talk about how reinforcement learning agents learn to act.\n",
        "- build an intuition for how dopamine might implement\n",
        "a reward prediction error to learn future expected value.\n",
        "- But how does the brain use expected values to learn how to act?\n",
        "- a policy is the\n",
        "formal description of how an agent chooses an action based on\n",
        "the observed state of the world. And a fundamental problem with learning how to act\n",
        "is exploration versus exploitation. \n",
        "- Finally, when you have states and actions you need to use a representation of value that can inform your policy.\n",
        "Or an action value representation, if you want both the value function and the actions they produce,\n",
        "to produce the optimal behavior.\n",
        "- an agent will take an action.\n",
        "The environment will return the new state and the reward associated with it,\n",
        "and over time the agent can update its action values\n",
        "to estimate the future value of the action that was taken in the state.\n",
        "-  'n-armed bandit'.\n",
        "It's an analogy to a slot machine where you have a bunch of one-armed bandits and you get to pull\n",
        "each of the slot machines and sometimes the slot machines are better and sometimes they're worse; imagine a machine that has multiple arms. In this case : blue, red, and green.\n",
        "Here,\n",
        "if we choose the blue arm,\n",
        "we're going to get more value every time we take that action. If we choose the green arm,\n",
        "we're going to take less value every time we get that action and if we choose the red arm,\n",
        "we're going to get something in between. But let's imagine we\n",
        "just chose once and then decided whether we knew which was best. If we choose the red arm,\n",
        "we don't know whether it's the best.\n",
        "If we were to explore,\n",
        "we could try all of the arms and then figure out which one is best.\n",
        "If we imagine that these might be slightly noisy so that every time we make a selection,\n",
        "we don't always get exactly the average value. This question becomes even harder.\n",
        "- Fundamentally,\n",
        "the question of how to explore and exploit is very tricky.\n",
        "Imagine that sort of optimal scenario.\n",
        "If we knew how long to explore, until we knew the best set of values, we could explore for a\n",
        "period of time and then switch to exploiting, that is, choosing the best action forever.\n",
        "However, this is almost always computationally intractable in the real world.\n",
        "- Other extreme: We're normally going to exploit or be greedy.\n",
        "But some small fraction of the time, epsilon, we're going to choose just randomly.\n",
        "This would ensure that we continue to explore and over time, we would eventually learn what actions were best.\n",
        "We could be a little bit more sophisticated and try to combine these ideas\n",
        "by doing something where we take a large epsilon or more\n",
        "exploratory behavior at the beginning and then we allow it to decay over time becoming more exploitative.\n",
        "- If we look at the percentage of\n",
        "actions taken by an epsilon greedy policy\n",
        "over a series of episodes, we can see that when epsilon is zero,\n",
        "we're very likely not to find the best action. \n",
        "However, if we have an epsilon that allows us to explore 10 percent of the time - having a larger exploration bonus means you more quickly converge on finding the highest average reward.\n",
        "- Well,\n",
        "we already learned that dopamine which is in the VTA and SNC, probably represents reward prediction error.\n",
        "And it projects heavily to the striatum and also to cortex.\n",
        "We think the cortex is likely involved in state estimation and perhaps generating actions. But,\n",
        "it's the striatum or the caudate and putamen in primates, where we think action values or state values live.\n",
        "So if you estimated the\n",
        "preference that the animal had, in each of those blocks, for one side or the other you could estimate the\n",
        "expected action value that the animal had.\n",
        "-And if you look at the spike rate in ocular motor caudate neurons,\n",
        "what you find is that they very directly almost linearly reflect the action values.\n",
        "- Is there a neural signature that might tell us that we know when we're thinking about exploring?\n",
        "n-armed bandit task where subjects were pulling arms on virtual bandits on a screen.\n",
        "And the payoff of these bandits drifted over time. That meant that the\n",
        "subjects had to keep track of which arms they had not checked recently and if they did this correctly,\n",
        "they would be able to go and check to see if they had gotten better. When looking for\n",
        "neural signatures that correlated with exploratory behavior, they found activity in the frontal pole.\n",
        "So this suggests that we may actively control how much we are exploring or exploiting.\n",
        "\n",
        "Use an n-arm bandit to study action value representations and to build an intuition for the exploration exploitation trade-off; implement the epsilon greedy policy and an action value update function and explore how different values of the exploration parameter, epsilon, and the learning rate parameter, alpha,\n",
        "affect how well the agent learns. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyMMBjrSSUYc"
      },
      "source": [
        "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a reward signal in the form of a numerical value, where the larger value is the better. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
        "\n",
        "This is the original form of the k-armed bandit problem. This name derives from the colloquial name for a slot machine, the \"one-armed bandit\", because it has the one lever to pull, and it is often rigged to take more money than it pays out over time. The multi-armed bandit extension is to imagine, for instance, that you are faced with multiple slot machines that you can play, but only one at a time. Which machine should you play, i.e. which arm should you pull, which action should you take, at any given time to maximize your total payout.\n",
        "\n",
        "<img alt=\"MultiArmedBandit\" width=\"625\" height=\"269\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial2_MultiarmedBandit.png?raw=true\">\n",
        "\n",
        "\n",
        "While there are many different levels of sophistication and assumptions in how the rewards are determined, for simplicity's sake we will assume that each action results in a reward drawn from a fixed Gaussian distribution with unknown mean and unit variance. This problem setting is referred to as the *environment*, and goal is to find the arm with the highest mean value.\n",
        "\n",
        "We will solve this *optimization problem* with an *agent*, in this case an algorithm that takes in rewards and returns actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HofivlasSUYd"
      },
      "source": [
        "---\n",
        "# Section 2: Choosing an Action\n",
        "   \n",
        "The first thing our agent needs to be able to do is choose which arm to pull. The strategy for choosing actions based on our expectations is called a *policy* (often denoted $\\pi$). We could have a random policy -- just pick an arm at random each time -- though this doesn't seem likely to be capable of optimizing our reward. We want some intentionality, and to do that we need a way of describing our beliefs about the arms' reward potential. We do this with an action-value function\n",
        "\n",
        "\\begin{align}\n",
        "q(a) = \\mathbb{E} [r_{t} | a_{t} = a]\n",
        "\\end{align}\n",
        "\n",
        "where the value $q$ for taking action $a \\in A$ at time $t$ is equal to the expected value of the reward $r_t$ given that we took action $a$ at that time. In practice, this is often represented as an array of values, where each action's value is a different element in the array.\n",
        "\n",
        "Great, now that we have a way to describe our beliefs about the values each action should return, let's come up with a policy.\n",
        "\n",
        "An obvious choice would be to take the action with the highest expected value. This is referred to as the *greedy* policy\n",
        "\n",
        "\\begin{align}\n",
        "a_{t} = \\text{argmax}_{a} \\; q_{t} (a)\n",
        "\\end{align}\n",
        "\n",
        "where our choice action is the one that maximizes the current value function.\n",
        "\n",
        "So far so good, but it can't be this easy. And, in fact, the greedy policy does have a fatal flaw: it easily gets trapped in local maxima. It never explores to see what it hasn't seen before if one option is already better than the others. This leads us to a fundamental challenge in coming up with effective policies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg-ATzXJSUYd"
      },
      "source": [
        "## Section 2.1: The Exploitation-Exploration Dilemma\n",
        "\n",
        "If we never try anything new, if we always stick to the safe bet, we don't know what we are missing. Sometimes we aren't missing much of anything, and regret not sticking with our preferred choice, yet other times we stumble upon something new that was way better than we thought.\n",
        "\n",
        "This is the exploitation-exploration dilemma: do you go with your best choice now, or risk the less certain option with the hope of finding something better. Too much exploration, however, means you may end up with a sub-optimal reward once it's time to stop.\n",
        "\n",
        "In order to avoid getting stuck in local minima while also maximizing reward, effective policies need some way to balance between these two aims.\n",
        "\n",
        "A simple extension to our greedy policy is to add some randomness. For instance, a coin flip -- heads we take the best choice now, tails we pick one at random. This is referred to as the $\\epsilon$-greedy policy:\n",
        "\n",
        "\\begin{align}\n",
        "P (a_{t} = a) = \n",
        "        \\begin{cases}\n",
        "        1 - \\epsilon + \\epsilon/N    & \\quad \\text{if } a_{t} = \\text{argmax}_{a} \\; q_{t} (a) \\\\\n",
        "        \\epsilon/N        & \\quad \\text{else} \n",
        "        \\end{cases} \n",
        "\\end{align}\n",
        "\n",
        "which is to say that with probability 1 - $\\epsilon$ for $\\epsilon \\in [0,1]$ we select the greedy choice, and otherwise we select an action at random (including the greedy option).\n",
        "\n",
        "Despite its relative simplicity, the epsilon-greedy policy is quite effective, which leads to its general popularity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXTauTXvSUYe"
      },
      "source": [
        "### Exercise 1: Implement Epsilon-Greedy\n",
        "\n",
        "In this exercise you will implement the epsilon-greedy algorithm for deciding which action to take from a set of possible actions given their value function and a probability $\\epsilon$ of simply choosing one at random. \n",
        "\n",
        "TIP: You may find [`np.random.random`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html), [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) useful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDkVQV4FSUYf"
      },
      "source": [
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  #####################################################################\n",
        "  ## TODO for students: implement the epsilon greedy decision algorithm\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: implement the epsilon greedy decision algorithm\")\n",
        "  #####################################################################\n",
        "  # write a boolean expression that determines if we should take the best action\n",
        "  be_greedy = ...\n",
        "  if be_greedy:\n",
        "    # write an expression for selecting the best action from the action values\n",
        "    action = ...\n",
        "  else:\n",
        "    # write an expression for selecting a random action\n",
        "    action = ...\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "# Uncomment once the epsilon_greedy function is complete\n",
        "# q = [-2, 5, 0, 1]\n",
        "# epsilon = 0.1\n",
        "# plot_choices(q, epsilon, epsilon_greedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mvn43OaSUYf"
      },
      "source": [
        "# to_remove solution\n",
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  # write a boolean expression that determines if we should take the best action\n",
        "  be_greedy = np.random.random() > epsilon\n",
        "  if be_greedy:\n",
        "    # write an expression for selecting the best action from the action values\n",
        "    action = np.argmax(q)\n",
        "  else:\n",
        "    # write an expression for selecting a random action\n",
        "    action = np.random.choice(len(q))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "q = [-2, 5, 0, 1]\n",
        "epsilon = 0.1\n",
        "with plt.xkcd():\n",
        "  plot_choices(q, epsilon, epsilon_greedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9LfWR3CSUYf"
      },
      "source": [
        "This is what we should expect, that the action with the largest value (action 1) is selected about (1-$\\epsilon$) of the time, or 90% for $\\epsilon = 0.1$, and the remaining 10% is split evenly amongst the other options. Use the demo below to explore how changing $\\epsilon$ affects the distribution of selected actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bHkQ1kwSUYf"
      },
      "source": [
        "### Interactive Demo: Changing Epsilon\n",
        "\n",
        "Epsilon is our one parameter for balancing exploitation and exploration.  Given a set of values $q = [-2, 5, 0, 1]$, use the widget below to see how changing $\\epsilon$ influences our selection of the max value 5 (action = 1) vs the others. \n",
        "\n",
        "At the extremes of its range (0 and 1), the $\\epsilon$-greedy policy reproduces two other policies. What are they?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-XfMUD5MSUYf"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact(epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0))\n",
        "def explore_epilson_values(epsilon=0.1):\n",
        "  q = [-2, 5, 0, 1]\n",
        "  plot_choices(q, epsilon, epsilon_greedy, rng_seed=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ5W5Gh7SUYf"
      },
      "source": [
        "#to_remove explanation\n",
        "\"\"\"\n",
        "When epsilon is zero, the agent always chooses the currently best option; it\n",
        "becomes greedy. When epsilon is 1, the agent chooses randomly.\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVzXTEjbSUYg"
      },
      "source": [
        "---\n",
        "# Section 3: Learning from Rewards\n",
        "\n",
        "Now that we have a policy for deciding what to do, how do we learn from our actions?\n",
        "\n",
        "One way to do this is just keep a record of every result we ever got and use the averages for each action. If we have a potentially very long running episode, the computational cost of keeping all these values and recomputing the mean over and over again isn't ideal. Instead we can use a streaming mean calculation, which looks like this:\n",
        "\n",
        "\\begin{align}\n",
        "q_{t+1}(a) \\leftarrow q_{t}(a) + \\frac{1}{n_t} (r_{t} - q_{t}(a))\n",
        "\\end{align}\n",
        "\n",
        "where our action-value function $q_t(a)$ is the mean of the rewards seen so far, $n_t$ is the number of actions taken by time $t$, and $r_t$ is the reward just received for taking action $a$.\n",
        "\n",
        "This still requires us to remember how many actions we've taken, so let's generalize this a bit further and replace the action total with a general parameter $\\alpha$, which we will call the learning rate\n",
        "\n",
        "\\begin{align}\n",
        "q_{t+1}(a) \\leftarrow q_{t}(a) + \\alpha (r_{t} - q_{t}(a)).\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomgqH0aSUYg"
      },
      "source": [
        "## Exercise 2: Updating Action Values\n",
        "\n",
        "In this exercise you will implement the action-value update rule above. The function will take in the action-value function represented as an array `q`, the action taken, the reward received, and the learning rate, `alpha`. The function will return the updated value for the selection action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIza_KYwSUYg"
      },
      "source": [
        "def update_action_value(q, action, reward, alpha):\n",
        "  \"\"\" Compute the updated action value given the learning rate and observed\n",
        "  reward.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received for taking the action\n",
        "    alpha (float): the learning rate\n",
        "\n",
        "  Returns:\n",
        "    float: the updated value for the selected action\n",
        "  \"\"\"\n",
        "  #####################################################\n",
        "  ## TODO for students: compute the action value update\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: compute the action value update\")\n",
        "  #####################################################\n",
        "  # write an expression for the updated action value\n",
        "  value = ...\n",
        "  return value\n",
        "\n",
        "\n",
        "# Uncomment once the update_action_value function is complete\n",
        "# q = [-2, 5, 0, 1]\n",
        "# action = 2\n",
        "# print(f\"Original q({action}) value = {q[action]}\")\n",
        "# q[action] = update_action_value(q, 2, 10, 0.01)\n",
        "# print(f\"Updated q({action}) value = {q[action]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQEIHDBaSUYh"
      },
      "source": [
        "# to_remove solution\n",
        "def update_action_value(q, action, reward, alpha):\n",
        "  \"\"\" Compute the updated action value given the learning rate and observed\n",
        "  reward.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received for taking the action\n",
        "    alpha (float): the learning rate\n",
        "\n",
        "  Returns:\n",
        "    float: the updated value for the selected action\n",
        "  \"\"\"\n",
        "  # write an expression for the updated action value\n",
        "  value = q[action] + alpha * (reward - q[action])\n",
        "  return value\n",
        "\n",
        "\n",
        "q = [-2, 5, 0, 1]\n",
        "action = 2\n",
        "print(f\"Original q({action}) value = {q[action]}\")\n",
        "q[action] = update_action_value(q, 2, 10, 0.01)\n",
        "print(f\"Updated q({action}) value = {q[action]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nedeUsdySUYh"
      },
      "source": [
        "---\n",
        "# Section 4: Solving Multi-Armed Bandits\n",
        "\n",
        "Now that we have both a policy and a learning rule, we can combine these to solve our original multi-armed bandit task. Recall that we have some number of arms that give rewards drawn from Gaussian distributions with unknown mean and unit variance, and our goal is to find the arm with the highest mean.\n",
        "\n",
        "First, let's see how we will simulate this environment by reading through the annotated code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V2M9wauSUYh"
      },
      "source": [
        "def multi_armed_bandit(n_arms, epsilon, alpha, n_steps):\n",
        "  \"\"\" A Gaussian multi-armed bandit using an epsilon-greedy policy. For each\n",
        "  action, rewards are randomly sampled from normal distribution, with a mean\n",
        "  associated with that arm and unit variance.\n",
        "\n",
        "  Args:\n",
        "    n_arms (int): number of arms or actions\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "    alpha (float): the learning rate\n",
        "    n_steps (int): number of steps to evaluate\n",
        "\n",
        "  Returns:\n",
        "    dict: a dictionary containing the action values, actions, and rewards from\n",
        "    the evaluation along with the true arm parameters mu and the optimality of\n",
        "    the chosen actions.\n",
        "  \"\"\"\n",
        "  # Gaussian bandit parameters\n",
        "  mu = np.random.normal(size=n_arms)\n",
        "\n",
        "  # evaluation and reporting state\n",
        "  q = np.zeros(n_arms)\n",
        "  qs = np.zeros((n_steps, n_arms))\n",
        "  rewards = np.zeros(n_steps)\n",
        "  actions = np.zeros(n_steps)\n",
        "  optimal = np.zeros(n_steps)\n",
        "\n",
        "  # run the bandit\n",
        "  for t in range(n_steps):\n",
        "    # choose an action\n",
        "    action = epsilon_greedy(q, epsilon)\n",
        "    actions[t] = action\n",
        "\n",
        "    # copmute rewards for all actions\n",
        "    all_rewards = np.random.normal(mu)\n",
        "    # observe the reward for the chosen action\n",
        "    reward = all_rewards[action]\n",
        "    rewards[t] = reward\n",
        "    # was it the best possible choice?\n",
        "    optimal_action = np.argmax(all_rewards)\n",
        "    optimal[t] = action == optimal_action\n",
        "\n",
        "    # update the action value\n",
        "    q[action] = update_action_value(q, action, reward, alpha)\n",
        "    qs[t] = q\n",
        "\n",
        "  results = {\n",
        "      'qs': qs,\n",
        "      'actions': actions,\n",
        "      'rewards': rewards,\n",
        "      'mu': mu,\n",
        "      'optimal': optimal\n",
        "  }\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6fxaAEFSUYi"
      },
      "source": [
        "We can use our multi-armed bandit method to evaluate how our epsilon-greedy policy and learning rule perform at solving the task. First we will set our environment to have 10 arms and our agent parameters to $\\epsilon=0.1$ and $\\alpha=0.01$. In order to get a good sense of the agent's performance, we will run the episode for 1000 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R3TToblSUYi"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "n_arms = 10\n",
        "epsilon = 0.1\n",
        "alpha = 0.01\n",
        "n_steps = 1000\n",
        "\n",
        "results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
        "ax1.plot(results['rewards'])\n",
        "ax1.set(title=f'Observed Reward ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
        "        xlabel='step', ylabel='reward')\n",
        "ax2.plot(results['qs'])\n",
        "ax2.set(title=f'Action Values ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
        "        xlabel='step', ylabel='value')\n",
        "ax2.legend(range(n_arms));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-ARz7RdSUYi"
      },
      "source": [
        "Alright, we got some rewards that are kind of all over the place, but the agent seemed to settle in on the first arm as the preferred choice of action relatively quickly. Let's see how well we did at recovering the true means of the Gaussian random variables behind the arms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cxocYBESUYi"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(results['mu'], label='latent')\n",
        "ax.plot(results['qs'][-1], label='learned')\n",
        "ax.set(title=f'$\\epsilon$={epsilon}, $\\\\alpha$={alpha}',\n",
        "       xlabel='action', ylabel='value')\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tfhBG29SUYi"
      },
      "source": [
        "Well, we seem to have found a very good estimate for action 0, but most of the others are not great. In fact, we can see the effect of the local maxima trap at work -- the greedy part of our algorithm locked onto action 0, which is actually the 2nd best choice to action 6. Since these are the means of Gaussian random variables, we can see that the overlap between the two would be quite high, so even if we did explore action 6, we may draw a sample that is still lower than our estimate for action 0.\n",
        "\n",
        "However, this was just one choice of parameters. Perhaps there is a better combination?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbClhqgzSUYj"
      },
      "source": [
        "## Interactive Demo: Changing Epsilon and Alpha\n",
        "\n",
        "Use the widget below to explore how varying the values of $\\epsilon$ (exploitation-exploration tradeoff), $\\alpha$ (learning rate), and even the number of actions $k$, changes the behavior of our agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NIKoe7M3SUYj"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact_manual(k=widgets.IntSlider(10, min=2, max=15),\n",
        "                         epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0),\n",
        "                         alpha=widgets.FloatLogSlider(0.01, min=-3, max=0))\n",
        "def explore_bandit_parameters(k=10, epsilon=0.1, alpha=0.001):\n",
        "  results = multi_armed_bandit(k, epsilon, alpha, 1000)\n",
        "  plot_multi_armed_bandit_results(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmABK1pqSUYk"
      },
      "source": [
        "While we can see how changing the epsilon and alpha values impact the agent's behavior, this doesn't give a great sense of which combination is optimal. Due to the stochastic nature of both our rewards and our policy, a single trial run isn't sufficient to give us this information. Let's run multiple trials and compare the average performance.\n",
        "\n",
        "First we will look at different values for $\\epsilon \\in [0.0, 0.1, 0.2]$ to a fixed $\\alpha=0.1$. We will run 200 trials as a nice balance between speed and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE0YvZ2SSUYk"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "epsilons = [0.0, 0.1, 0.2]\n",
        "alpha = 0.1\n",
        "n_trials = 200\n",
        "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "for i, epsilon in enumerate(epsilons):\n",
        "  for n in range(n_trials):\n",
        "    results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "    trial_rewards[i, n] = results['rewards']\n",
        "    trial_optimal[i, n] = results['optimal']\n",
        "\n",
        "labels = [f'$\\epsilon$={e}' for e in epsilons]\n",
        "fixed = f'$\\\\alpha$={alpha}'\n",
        "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_NiWrUHSUYk"
      },
      "source": [
        "On the left we have plotted the average reward over time, and we see that while $\\epsilon=0$ (the greedy policy) does well initially, $\\epsilon=0.1$ starts to do slightly better in the long run, while $\\epsilon=0.2$ does the worst. Looking on the right, we see the percentage of times the optimal action (the best possible choice at time $t$) was taken, and here again we see a similar pattern of $\\epsilon=0.1$ starting out a bit slower but eventually having a slight edge in the longer run.\n",
        "\n",
        "We can also do the same for the learning rates. We will evaluate $\\alpha \\in [0.01, 0.1, 1.0]$ to a fixed $\\epsilon=0.1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6W0zLRwSUYk"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "epsilon = 0.1\n",
        "alphas = [0.01, 0.1, 1.0]\n",
        "n_trials = 200\n",
        "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "for i, alpha in enumerate(alphas):\n",
        "  for n in range(n_trials):\n",
        "    results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "    trial_rewards[i, n] = results['rewards']\n",
        "    trial_optimal[i, n] = results['optimal']\n",
        "\n",
        "labels = [f'$\\\\alpha$={a}' for a in alphas]\n",
        "fixed = f'$\\epsilon$={epsilon}'\n",
        "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvEWQ1BiSUYk"
      },
      "source": [
        "Again we see a balance between an effective learning rate. $\\alpha=0.01$ is too weak to quickly incorporate good values, while $\\alpha=1$ is too strong likely resulting in high variance in values due to the Gaussian nature of the rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guSyPDfBSUYk"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "In this tutorial you implemented both the epsilon-greedy descision algorithm and a learning rule for solving a multi-armed bandit scenario. You saw how balancing exploitation and exploration in action selection is crtical in finding optimal solutions. You also saw how choosing an appropriate learning rate determines how well an agent can generalize the information they receive from rewards.\n"
      ]
    }
  ]
}