{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W2D4_Tutorial1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": [
          "# Tutorial 1- Optimal Control for Discrete State\n",
          "\n",
          "Please execute the cell below to initialize the notebook environment.\n",
          "\n",
          "import numpy as np                 # import numpy\n",
          "import scipy               # import scipy\n",
          "import random                      # import basic random number generator functions\n",
          "from scipy.linalg import inv\n",
          "\n",
          "import matplotlib.pyplot as plt    # import matplotlib\n",
          "\n",
          "---\n",
          "\n",
          "## Tutorial objectives\n",
          "\n",
          "In this tutorial, we will implement a binary HMM task.\n",
          "\n",
          "---\n",
          "\n",
          "## Task Description\n",
          "\n",
          "There are two boxes. The box can be in a high-rewarding state ($s=1$), which means that a reward will be delivered with high probabilty $q_{high}$; or the box can be in low-rewarding state ($s=0$), then the reward will be delivered with low probabilty $q_{low}$.\n",
          "\n",
          "The states of the two boxes are latent. At a certain time, only one of the sites can be in high-rewarding state, and the other box will be the opposite. The states of the two boxes switches with a certain probability $p_{sw}$. \n",
          "\n",
          "![alt text](switching.png \"Title\")\n",
          "\n",
          "\n",
          "The agent may stay at one site for sometime. As the agent accumulates evidence about the state of the box on that site, it may choose to stay or switch to the other side with a switching cost $c$. The agent keeps beliefs on the states of the boxes, which is the posterior probability of the state being high-rewarding given all the past observations. Consider the belief on the state of the left box, we have \n",
          "\n",
          "$$b(s_t) = p(s_t = 1 | o_{0:t}, l_{0:t}, a_{0:t-1})$$\n",
          "\n",
          "where $o$ is the observation that whether a reward is obtained, $l$ is the location of the agent, $a$ is the action of staying ($a=0$) or switching($a=1$). \n",
          "\n",
          "Since the two boxes are completely anti-correlated, i.e. only one of the boxes is high-rewarded at a certain time, the the other one is low-rewarded, the belief on the two boxes should sum up to be 1. As a result, we only need to track the belief on one of the boxes. \n",
          "\n",
          "The policy of the agent depends on a threshold on beliefs. When the belief on the box on the other side gets higher than the threshold $\\theta$, the agent will switch to the other side. In other words, the agent will choose to switch when it is confident enough that the other side is high rewarding. \n",
          "\n",
          "The value function can be defined as the reward rate during a single trial.\n",
          "\n",
          "$$v(\\theta) = \\sum_t r_t - c\\cdot 1_{a_t = 1}$$ \n",
          "\n",
          "we would like to see the relation between the threshold and the value function. \n",
          "\n",
          "### Exercise 1: Control for binary HMM\n",
          "In this excercise, we generate the dynamics for the binary HMM task as described above. \n",
          "\n",
          "# This function is the policy based on threshold\n",
          "\n",
          "def policy(threshold, bel, loc):\n",
          "    if loc == 0:\n",
          "        if bel[1]  >= threshold:\n",
          "            act = 1\n",
          "        else:\n",
          "            act = 0\n",
          "    else:  # loc = 1\n",
          "        if bel[0] >= threshold:\n",
          "            act = 1\n",
          "        else:\n",
          "            act = 0\n",
          "\n",
          "    return act\n",
          "\n",
          "# This function generates the dynamics\n",
          "\n",
          "def generateProcess(params):\n",
          "\n",
          "    T, p_sw, q_high, q_low, cost_sw, threshold = params\n",
          "    world_state = np.zeros((2, T), int)  # value :1: good box; 0: bad box\n",
          "    loc = np.zeros(T, int)  # 0: left box               1: right box\n",
          "    obs = np.zeros(T, int)  # 0: did not get food        1: get food\n",
          "    act = np.zeros(T, int)  # 0 : stay                   1: switch and get food from the other side\n",
          "    bel = np.zeros((2, T), float)  # the probability that the left box has food,\n",
          "    # then the probability that the second box has food is 1-b\n",
          "\n",
          "\n",
          "    p = np.array([1 - p_sw, p_sw])  # transition probability to good state\n",
          "    q = np.array([q_low, q_high])\n",
          "    q_mat = np.array([[1 - q_high, q_high], [1 - q_low, q_low]])\n",
          "\n",
          "    for t in range(T):\n",
          "        if t == 0:\n",
          "            world_state[0, t] = 1    # good box\n",
          "            world_state[1, t] = 1 - world_state[0, t]\n",
          "            loc[t] = 0\n",
          "            obs[t] = 0\n",
          "            bel_0 = np.random.random(1)[0]\n",
          "            bel[:, t] = np.array([bel_0, 1-bel_0])\n",
          "\n",
          "            act[t] = policy(threshold, bel[:, t], loc[t])\n",
          "\n",
          "        else:\n",
          "            world_state[0, t] = np.random.binomial(1, p[world_state[0, t - 1]])\n",
          "            world_state[1, t] = 1 - world_state[0, t]\n",
          "\n",
          "            if act[t - 1] == 0:\n",
          "                loc[t] = loc[t - 1]\n",
          "            else:  # after weitching, open the new box, deplete if any; then wait a usualy time\n",
          "                loc[t] = 1 - loc[t - 1]\n",
          "\n",
          "            # new observation\n",
          "            obs[t] = np.random.binomial(1, q[world_state[loc[t], t-1]])\n",
          "\n",
          "            # update belief posterior, p(s[t] | obs(0-t), act(0-t-1))\n",
          "            bel_0 = (bel[0, t-1] * p_sw  + bel[1, t-1] * (1 - p_sw)) * q_mat[loc[t], obs[t]]\n",
          "            bel_1 = (bel[1, t - 1] * p_sw + bel[0, t - 1] * (1 - p_sw)) * q_mat[1-loc[t], obs[t]]\n",
          "\n",
          "            bel[0, t] = bel_0 / (bel_0 + bel_1)\n",
          "            bel[1, t] = bel_1 / (bel_0 + bel_1)\n",
          "\n",
          "            act[t] = policy(threshold, bel[:, t], loc[t])\n",
          "\n",
          "    return bel, obs, act, world_state, loc\n",
          "\n",
          "# value function \n",
          "def value_function(obs, act, cost_sw, discount):\n",
          "    T = len(obs)\n",
          "    discount_time = np.array([discount ** t for t in range(T)])\n",
          "\n",
          "    #value = (np.sum(obs) - np.sum(act) * cost_sw) / T\n",
          "    value = (np.sum(np.multiply(obs, discount_time)) - np.sum(np.multiply(act, discount_time)) * cost_sw) / T\n",
          "\n",
          "    return value\n",
          "\n",
          "def switch_int(obs, act):\n",
          "    sw_t = np.where(act == 1)[0]\n",
          "    sw_int = sw_t[1:] - sw_t[:-1]\n",
          "\n",
          "    return sw_int\n",
          "\n",
          "#Plotting \n",
          "def plot_dynamics(bel, obs, act, world_state, loc):\n",
          "    T = len(obs)\n",
          "\n",
          "    showlen = min(T, 100)\n",
          "    startT = 0\n",
          "\n",
          "    endT = startT + showlen\n",
          "    showT = range(startT, endT)\n",
          "    time_range = np.linspace(0, showlen - 1)\n",
          "\n",
          "    fig_posterior, [ax0, ax1, ax_loc, ax2, ax3] = plt.subplots(5, 1, figsize=(15, 10))\n",
          "\n",
          "    ax0.plot(world_state[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax0.set_ylabel('Left box', rotation=360, fontsize=22)\n",
          "    ax0.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax0.set_xticks(np.arange(0, showlen, 10))\n",
          "    ax0.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax0.set_xlim([0, showlen])\n",
          "\n",
          "\n",
          "    ax3.plot(world_state[1, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax3.set_ylabel('Right box', rotation=360, fontsize=22)\n",
          "    ax3.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax3.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax3.set_xlim([0, showlen])\n",
          "    ax3.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "    ax1.plot(bel[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax1.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
          "    ax1.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax1.set_ylabel('Belief on \\n left box', rotation=360, fontsize=22)\n",
          "    ax1.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax1.set_xlim([0, showlen])\n",
          "    ax1.set_ylim([0, 1])\n",
          "    ax1.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "\n",
          "    ax_loc.plot(1 - loc[showT], 'g.-', markersize=12, linewidth=5, label = 'location')\n",
          "    ax_loc.plot((act[showT] - .1) * .8, 'v', markersize=10, label = 'action')\n",
          "    ax_loc.plot(obs[showT] * .5, '*', markersize=5, label = 'reward')\n",
          "    ax_loc.legend(loc=\"upper right\")\n",
          "    ax_loc.set_xlim([0, showlen])\n",
          "    ax_loc.set_ylim([0, 1])\n",
          "    #ax_loc.set_yticks([])\n",
          "    ax_loc.set_xticks([0, showlen])\n",
          "    ax_loc.tick_params(axis='both', which='major', labelsize=18)\n",
          "    labels = [item.get_text() for item in ax_loc.get_yticklabels()]\n",
          "    labels[0] = 'Right'\n",
          "    labels[-1] = 'Left'\n",
          "    ax_loc.set_yticklabels(labels)\n",
          "\n",
          "    ax2.plot(bel[1, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax2.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
          "    ax2.set_xlabel('time', fontsize=18)\n",
          "    ax2.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax2.set_ylabel('Belief on  \\n  right box', rotation=360, fontsize=22)\n",
          "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax2.set_xlim([0, showlen])\n",
          "    ax2.set_ylim([0, 1])\n",
          "    ax2.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "    plt.show()\n",
          "\n",
          "def plot_val_thre(threshold_array, value_array):\n",
          "    fig_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
          "    ax.plot(threshold_array, value_array)\n",
          "    ax.set_ylim([np.min(value_array), np.max(value_array)])\n",
          "    ax.set_title('threshold vs value')\n",
          "    ax.set_xlabel('threshold')\n",
          "    ax.set_ylabel('value')\n",
          "    plt.show()\n",
          "\n",
          "T = 5000\n",
          "p_sw = .95          # state transiton probability\n",
          "q_high = .7\n",
          "q_low = 0 #.2\n",
          "cost_sw = 1 #int(1/(1-p_sw)) - 5\n",
          "threshold = .8    # threshold of belief for switching\n",
          "discount = 1\n",
          "\n",
          "step = 0.1\n",
          "threshold_array = np.arange(0, 1 + step, step)\n",
          "value_array = np.zeros(threshold_array.shape)\n",
          "\n",
          "for i in range(len(threshold_array)):\n",
          "    threshold = threshold_array[i]\n",
          "    params = [T, p_sw, q_high, q_low, cost_sw, threshold]\n",
          "    bel, obs, act, world_state, loc = generateProcess(params)\n",
          "    value_array[i] = value_function(obs, act, cost_sw, discount)\n",
          "    sw_int = switch_int(obs, act)\n",
          "    #print(np.mean(sw_int))\n",
          "\n",
          "    if threshold == 0.8:\n",
          "        plot_dynamics(bel, obs, act, world_state, loc)\n",
          "\n",
          "plot_val_thre(threshold_array, value_array)\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n"
        ]
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Sc8fnrYYL6"
      },
      "source": [
        "# Neuromatch Academy: Week 2, Day 4, Tutorial 1\n",
        "# Optimal Control for Discrete States and Actions\n",
        "\n",
        "__Content creators:__ Zhengwei Wu, Shreya Saxena, Xaq Pitkow\n",
        "\n",
        "__Content reviewers:__ Karolina Stosio, Roozbeh Farhoodi, Saeed Salehi, Spiros Chavlis, Matt Krause and Michael Waskom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LviBRjTTYYMD"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "In this tutorial, we will implement a binary control task: a Partially Observable Markov Decision Process (POMDP) that describes fishing. The agent (you) seeks reward from two fishing sites without directly observing where the school of fish is (a group of fish is called a school!). This makes the world a Hidden Markov Model. Based on when and where you catch fish, you keep updating your belief about the fish location, _i.e._ the posterior of the fish given past observations. You should control your position to get the most fish while minimizing the cost of switching sides.\n",
        "\n",
        "You've already learned about stochastic dynamics, latent states, and measurements. Now we introduce you to the new concepts of **control, utility, and policy**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeLX4CdEYYME"
      },
      "source": [
        "---\n",
        "## Setup  \n",
        "Please execute the cells below to initialize the notebook environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wae8T11UYYMF"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.linalg import inv\n",
        "from math import isclose\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0AMwOxBrYYMH"
      },
      "source": [
        "#@title Figure Settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "RiAA2VBsYYMH"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "binomial = np.random.binomial\n",
        "\n",
        "\n",
        "class ExcerciseError(AssertionError):\n",
        "  pass\n",
        "\n",
        "\n",
        "def test_policy_threshold():\n",
        "  well_done = True\n",
        "  for loc in [-1, 1]:\n",
        "    threshold = 0.4\n",
        "    belief = np.array([.2, .3])\n",
        "    if policy_threshold(threshold, belief, loc) != \"switch\":\n",
        "      raise ExcerciseError(\"'policy_threshold' function is not correctly implemented!\")\n",
        "  for loc in [1, -1]:\n",
        "    threshold = 0.6\n",
        "    belief = np.array([.7, .8])\n",
        "    if policy_threshold(threshold, belief, loc) != \"stay\":\n",
        "      raise ExcerciseError(\"'policy_threshold' function is not correctly implemented!\")\n",
        "  print(\"Well Done!\")\n",
        "\n",
        "\n",
        "def test_value_function():\n",
        "  measurement = np.array([0, 0, 0, 1, 0, 0, 0, 0, 1, 1])\n",
        "  act = np.array([\"switch\", \"stay\", \"switch\", \"stay\", \"stay\",\n",
        "                  \"stay\", \"switch\", \"switch\", \"stay\", \"stay\"])\n",
        "  cost_sw = .5\n",
        "  if not isclose(value_function(measurement, act, cost_sw), .1):\n",
        "    raise ExcerciseError(\"'value_function' function is not correctly implemented!\")\n",
        "  print(\"Well Done!\")\n",
        "\n",
        "\n",
        "def plot_fish(fish_state, ax=None):\n",
        "  \"\"\"\n",
        "  Plot the fish dynamics\n",
        "  \"\"\"\n",
        "  T = len(fish_state)\n",
        "\n",
        "  showlen = min(T, 200)\n",
        "  startT = 0\n",
        "\n",
        "  endT = startT + showlen\n",
        "  showT = range(startT, endT)\n",
        "  time_range = np.linspace(0, showlen - 1)\n",
        "\n",
        "  if not ax:\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 2.5))\n",
        "  ax.plot(- fish_state[showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
        "  ax.set_xlabel('time', fontsize=18)\n",
        "  ax.set_ylabel('Fish state', rotation=360, fontsize=18)\n",
        "  ax.yaxis.set_label_coords(-0.1, 0.25)\n",
        "  ax.set_xticks([0, showlen, showlen])\n",
        "  ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "  ax.set_xlim([0, showlen])\n",
        "  ax.set_ylim([-1.1, 1.1])\n",
        "  ax.set_yticks([-1, 1])\n",
        "  ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "  labels = [item.get_text() for item in ax.get_yticklabels()]\n",
        "  labels[0] = 'Right'\n",
        "  labels[1] = 'Left'\n",
        "  ax.set_yticklabels(labels)\n",
        "\n",
        "\n",
        "def plot_measurement(measurement, ax=None):\n",
        "  \"\"\"\n",
        "  Plot the measurements\n",
        "  \"\"\"\n",
        "  T = len(measurement)\n",
        "\n",
        "  showlen = min(T, 200)\n",
        "  startT = 0\n",
        "  endT = startT + showlen\n",
        "  showT = range(startT, endT)\n",
        "  time_range = np.linspace(0, showlen - 1)\n",
        "\n",
        "  if not ax:\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 2.5))\n",
        "  ax.plot(measurement[showT], 'r*', markersize=5)\n",
        "  ax.set_xlabel('time', fontsize=18)\n",
        "  ax.set_ylabel('Measurement', rotation=360, fontsize=18)\n",
        "  ax.yaxis.set_label_coords(-0.2, 0.4)\n",
        "  ax.set_xticks([0, showlen, showlen])\n",
        "  ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "  ax.set_xlim([0, showlen])\n",
        "  ax.set_ylim([-.1, 1.1])\n",
        "  ax.set_yticks([0, 1])\n",
        "  ax.set_yticklabels(['no fish', 'caught fish'])\n",
        "\n",
        "\n",
        "def plot_act_loc(loc, act, ax_loc=None):\n",
        "  \"\"\"\n",
        "  Plot the action and location of 200 time points\n",
        "  \"\"\"\n",
        "  T = len(act)\n",
        "\n",
        "  showlen = min(T, 200)\n",
        "  startT = 0\n",
        "\n",
        "  endT = startT + showlen\n",
        "  showT = range(startT, endT)\n",
        "  time_range = np.linspace(0, showlen - 1)\n",
        "\n",
        "  if not ax_loc:\n",
        "    fig, ax_loc = plt.subplots(1, 1, figsize=(12, 2.5))\n",
        "\n",
        "  act_int = (act == \"switch\").astype(int)\n",
        "  ax_loc.plot(-loc[showT], 'g.-', markersize=8, linewidth=5)\n",
        "  ax_loc.plot((act_int[showT] * 4 - 3) * .5, 'rv', markersize=12,\n",
        "              label='switch')\n",
        "  ax_loc.set_xlabel('time', fontsize=18)\n",
        "  ax_loc.set_ylabel('Your state', rotation=360, fontsize=18)\n",
        "\n",
        "  ax_loc.legend(loc=\"upper right\", fontsize=12)\n",
        "  ax_loc.set_xlim([0, showlen])\n",
        "  ax_loc.set_ylim([-1.1, 1.1])\n",
        "  ax_loc.set_yticks([-1, 1])\n",
        "  ax_loc.set_xticks([0, showlen, showlen])\n",
        "  ax_loc.tick_params(axis='both', which='major', labelsize=18)\n",
        "  labels = [item.get_text() for item in ax_loc.get_yticklabels()]\n",
        "  labels[1] = 'Left'\n",
        "  labels[0] = 'Right'\n",
        "  ax_loc.set_yticklabels(labels)\n",
        "\n",
        "\n",
        "def plot_belief(belief, ax1=None, choose_policy=None):\n",
        "  \"\"\"\n",
        "  Plot the belief dynamics of 200 time points\n",
        "  \"\"\"\n",
        "\n",
        "  T = belief.shape[1]\n",
        "\n",
        "  showlen = min(T, 200)\n",
        "  startT = 0\n",
        "\n",
        "  endT = startT + showlen\n",
        "  showT = range(startT, endT)\n",
        "  time_range = np.linspace(0, showlen - 1)\n",
        "\n",
        "  if not ax1:\n",
        "      fig, ax1 = plt.subplots(1, 1, figsize=(12, 2.5))\n",
        "\n",
        "  ax1.plot(belief[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
        "  ax1.yaxis.set_label_coords(-0.1, 0.25)\n",
        "  ax1.set_xlabel('time', rotation=360, fontsize=18)\n",
        "  ax1.set_ylabel('Belief on \\n left', rotation=360, fontsize=18)\n",
        "  ax1.tick_params(axis='both', which='major', labelsize=18)\n",
        "  ax1.set_xlim([0, showlen])\n",
        "  ax1.set_yticks([0, 1])\n",
        "  ax1.set_ylim([0, 1.1])\n",
        "  ax1.set_xticks([0, showlen, showlen])\n",
        "\n",
        "  if choose_policy == \"threshold\":\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
        "    ax2.plot(time_range, (1 - threshold) * np.ones(time_range.shape), 'c--')\n",
        "    ax2.set_yticks([threshold, 1 - threshold])\n",
        "    ax2.set_ylim([0, 1.1])\n",
        "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
        "    labels = [item.get_text() for item in ax2.get_yticklabels()]\n",
        "    labels[0] = 'threshold to switch \\n from left to right'\n",
        "    labels[-1] = 'threshold to switch \\n from right to left'\n",
        "    ax2.set_yticklabels(labels)\n",
        "\n",
        "\n",
        "def plot_dynamics(belief, loc, act, meas, fish_state, choose_policy):\n",
        "  \"\"\"\n",
        "  Plot the dynamics of 200 time points\n",
        "  \"\"\"\n",
        "  if choose_policy == 'threshold':\n",
        "    fig, [ax0, ax_loc, ax1, ax_bel] = plt.subplots(4, 1, figsize=(12, 9))\n",
        "    plot_fish(fish_state, ax=ax0)\n",
        "    plot_belief(belief, ax1=ax_bel)\n",
        "    plot_measurement(meas, ax=ax1)\n",
        "    plot_act_loc(loc, act, ax_loc=ax_loc)\n",
        "  else:\n",
        "    fig, [ax0, ax1, ax_bel] = plt.subplots(3, 1, figsize=(12, 7))\n",
        "    plot_fish(fish_state, ax=ax0)\n",
        "    plot_belief(belief, ax1=ax_bel)\n",
        "    plot_measurement(meas, ax=ax1)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def belief_histogram(belief, bins=100):\n",
        "  \"\"\"\n",
        "  Plot the histogram of belief states\n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "  ax.hist(belief, bins)\n",
        "  ax.set_xlabel('belief', fontsize=18)\n",
        "  ax.set_ylabel('count', fontsize=18)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_value_threshold(cost_sw=0.5, T=10000, p_stay=.95,\n",
        "                         high_rew_p=.4, low_rew_p=.1, step=.05):\n",
        "  \"\"\"\n",
        "  Helper function to plot the value function and threshold\n",
        "  \"\"\"\n",
        "  params = [T, p_stay, high_rew_p, low_rew_p, _]\n",
        "\n",
        "  threshold_array, value_array = value_threshold(params, cost_sw, step)\n",
        "  yrange = np.max(value_array) - np.min(value_array)\n",
        "\n",
        "  fig_, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "  ax.plot(threshold_array, value_array, 'b')\n",
        "  ax.set_ylim([np.min(value_array) - yrange * .1, np.max(value_array) + yrange * .1])\n",
        "  ax.set_title(f'threshold vs value with switching cost c = {cost_sw:.2f}',\n",
        "               fontsize=20)\n",
        "  ax.set_xlabel('threshold', fontsize=16)\n",
        "  ax.set_ylabel('value', fontsize=16)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-3Sm1kfHbpc"
      },
      "source": [
        "# Summary of helper functions:\n",
        "\n",
        "*test_policy_threshold*:\n",
        "based on belief, threshold, policy, this function tests if policy_threshold function has been correctly implemented.\n",
        "\n",
        "*test_value_function*:\n",
        "based on measurement, cost and action, this function tests if value_function has been correctly implemented.\n",
        "\n",
        "*plot_fish*:\n",
        "plots fish's dynamics across left and right lakes. Fish state (yaxis) over time(xaxis)\n",
        "\n",
        "*plot_measurement*:\n",
        "plots measurements(yaxis) over time(xaxis) when fish are caught and no fish are caught\n",
        "\n",
        "*plot_act_loc*:\n",
        "plot the action and location over switching between left and right; state (yaxis) of 200 time points (xaxis)\n",
        "\n",
        "*plot_belief*:\n",
        "plot belief dynamics (yaxis) over 200 time points (xaxis). Consider cases from there's switching from left to right and right to left. \n",
        "\n",
        "*plot_dynamics*:\n",
        "plot dynamics over 200 time points based on policy. If policy is threshold based then plot fish, belief, measurements, along with action location, \n",
        "\n",
        "*belief histogram*:\n",
        "plots histogram (yaxis - count) of belief states (xaxis)\n",
        "\n",
        "*plot_value_threshold*:\n",
        "helper function to plot value function and threshold where threshold vs value with switching cost c. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAN_gLiNYYMJ"
      },
      "source": [
        "---\n",
        "# Section 1: Dynamics of Fishing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "irvzafyNYYMJ"
      },
      "source": [
        "#@title Video 1: Gone fishing\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='3oIwUFpolVA', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw2l08Y6K0gb"
      },
      "source": [
        "# Summary of video 1:\n",
        "\n",
        "- how should the brain act?\n",
        "perception\n",
        "action loop where an observer gets to\n",
        "make measurements of the world\n",
        "and take actions accordingly in such a\n",
        "way as to\n",
        "maximize their expected value\n",
        "- this involves having an internal model\n",
        "of how the dynamics of the world work\n",
        "and what is valuable for the agent\n",
        "\n",
        "bayesian modeling\n",
        "allows us to\n",
        "have some key tools that formalize and\n",
        "understand this perception action loop\n",
        "\n",
        "use those tools to go beyond perception\n",
        "and include action\n",
        "we'll begin with a markov decision\n",
        "process\n",
        "this is a sequential decision-making\n",
        "process where again you're trying to\n",
        "maximize your rewards or your utility\n",
        "\n",
        "here the current world state is\n",
        "observable\n",
        "you get to see exactly what there is in\n",
        "the world\n",
        "right now but you don't know the future\n",
        "so we have a state and we\n",
        "have actions\n",
        "and we know the transitions\n",
        "what the future state probabilities are\n",
        "given\n",
        "the current state and the different\n",
        "actions that you can take\n",
        "this is why we call this a markovian\n",
        "process\n",
        "because the future distribution only\n",
        "depends on the present state;\n",
        "there's only a one time step history now\n",
        "\n",
        "we also have rewards or utilities\n",
        "which depend on the state and action\n",
        "that you chose\n",
        "and we have a policy which defines what\n",
        "what actions you select\n",
        "from each state that you observe\n",
        "and all of those together give us a\n",
        "value\n",
        "trying to optimize\n",
        "(value for each state)\n",
        "\n",
        "the world\n",
        "state evolves over time\n",
        "but now the updates depend on what you\n",
        "do so you can take actions that change\n",
        "the evolution of the state\n",
        "compared to if it was evolving\n",
        "independently and you can see that over\n",
        "here\n",
        "that the transition probability used to\n",
        "be in just a hidden markov; \n",
        "there's this additional\n",
        "action dependence\n",
        "one from the previous state and the\n",
        "other from your action\n",
        "and your action is determined by the\n",
        "previous state as well\n",
        "through the policy here \n",
        "and the action are giving you rewards\n",
        "and those rewards are what you're trying\n",
        "to optimize the long term\n",
        "expected reward\n",
        "\n",
        "Consider instead a partially\n",
        "observable\n",
        "markov decision process \n",
        "now you do not get to observe the world\n",
        "state\n",
        "it's uncertain because you have only\n",
        "partial observations about it\n",
        "now on the other hand you do know what\n",
        "uncertainty you have\n",
        "that uncertainty is known it's your\n",
        "posterior\n",
        "\n",
        "however you don't know the\n",
        "future\n",
        "of the world state but you also don't\n",
        "even know what your future uncertainty\n",
        "will be\n",
        "okay so this is a more uncertain\n",
        "situation and yet you still have to make\n",
        "decisions\n",
        "change them to\n",
        "incorporate partial observability\n",
        "so the state becomes a belief state\n",
        "which is a posterior \n",
        "\n",
        "lean towards the hidden markov model \n",
        "along with probability of that state\n",
        "given the whole history of measurements\n",
        "those are sensory\n",
        "observations and the whole history of\n",
        "the actions that you've taken\n",
        "that posterior we will call a belief\n",
        "and designated by b\n",
        "now you in order to compute those\n",
        "beliefs you need a new ingredient which\n",
        "is your measurement and\n",
        "those measurements are the whole history\n",
        "they're generated according to a hidden\n",
        "markov model; \n",
        "now we still have the actions but now\n",
        "the transitions that we have to consider\n",
        "are not just world transitions but in\n",
        "fact\n",
        "belief transitions this is given that\n",
        "you were in some belief state before\n",
        "that you had obtained some measurements\n",
        "previously\n",
        "\n",
        "what is the probability that you will\n",
        "enter a new belief state\n",
        "in the future at time t plus one\n",
        "\n",
        "similarly\n",
        "whereas before we had a reward or\n",
        "utility that depended on the world state\n",
        "in action\n",
        "\n",
        "now we have an expected utility which\n",
        "depends\n",
        "on the belief and the action\n",
        "and finally our policy that we will\n",
        "choose depends not on the world state\n",
        "which again we don't know but only on\n",
        "our belief about that state or the\n",
        "posterior about that state\n",
        "and finally the value that we assess\n",
        "for this policy and for all of these\n",
        "ingredients is now a function of the\n",
        "belief\n",
        "\n",
        "as well so certain belief states are\n",
        "valuable\n",
        "and others are less valuable we would\n",
        "like to choose\n",
        "actions that maximize our total\n",
        "expected future utility that's going to\n",
        "give us the highest value\n",
        "\n",
        "the world state is emitting measurements\n",
        "which we get to incorporate to generate\n",
        "our beliefs\n",
        "or our posteriors about the world and\n",
        "these beliefs\n",
        "are updated according to the dynamics\n",
        "that we know about the world\n",
        "that is the same assumed to be parallel\n",
        "to the world dynamics here\n",
        "and the actions that we take as well as\n",
        "the measurements that we observe\n",
        "so we have this bouncing back and\n",
        "forth of information from the world\n",
        "which gives measurements\n",
        "updates our belief then\n",
        "determines our actions\n",
        "which then go back and affect the world\n",
        "state\n",
        "\n",
        "- learn how to take\n",
        "actions\n",
        "based on those hidden markov model\n",
        "inferences\n",
        "- contrast\n",
        "optimal control in open and closed loop\n",
        "states; open loop control is going to\n",
        "solve one of these\n",
        "decision processes when you don't have\n",
        "any measurements\n",
        "- make the best control\n",
        "possible\n",
        "you can do better if you get to\n",
        "continuously measure the world\n",
        "and then that gives you closed loop\n",
        "control which will solve these\n",
        "decision processes with measurements\n",
        "based on a\n",
        "binary and continuous variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In_rORgeYYMK"
      },
      "source": [
        "\n",
        "## Introduction\n",
        "\n",
        "There are two locations for the fish and you (Left and Right). If you're on the same side as the fish, you'll catch more, with probabilty $q_{\\rm high}$ per discrete time step. Otherwise you may still catch fish with probability $q_{\\rm low}$. One fish is worth 1 \"point\".\n",
        "\n",
        "The fish location $s^{\\rm fish}$ is latent. The only information you get about the fish location is when you catch one. Secretly at each time step, the fish may switch sides with a certain probability $p_{\\rm sw} = 1 - p_{\\rm stay}$.\n",
        "\n",
        "\n",
        "You are in control of your own location. You may stay on your current side with no cost, or switch to the other side and incur an action cost $C$ (again, in units of fish).\n",
        "\n",
        "You select controls or actions by following a **policy**. This defines what to do in any situation. Here the situation is specified by your location and your belief $b_t$ about the fish location. For optimal control we assume that this belief is the posterior probability over the current fish location, given all the past measurements. We only need one number for this, since the fish are either on the left or the right. So we write \n",
        "\n",
        "$$b_t = p(s^{\\rm fish}_t = {\\rm Right}\\  |\\  m_{0:t}, a_{0:t-1})$$\n",
        "\n",
        "where $m$ are the measurements, and $a$ are the controls or actions (stay or switch).\n",
        "\n",
        "Ultimately we will parameterize the policy by a simple threshold on beliefs. (This happens to be optimal if you pick the right threshold!) When your belief that fish are on your current side falls below a threshold $\\theta$, you switch to the other side.\n",
        "\n",
        "Your **overall goals** in this tutorial are:\n",
        "1. Measure when fish are caught, first if the school of fish doesn't move.\n",
        "2. For moving fish, plot their dynamics and your belief about it based on your measurements.\n",
        "3. Compute the value for a given control policy.\n",
        "4. Find the optimal policy for controlling your position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM-ojoCNYYML"
      },
      "source": [
        "## Exercise 1.1\n",
        "Evaluate the following methods implemented in `binaryHMM` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "pdm5WaAtYYMM"
      },
      "source": [
        "# class for the binary HMM task\n",
        "\n",
        "class binaryHMM():\n",
        "\n",
        "  def __init__(self, params, fish_initial=-1, loc_initial=-1):\n",
        "    self.params = params\n",
        "    self.fish_initial = fish_initial\n",
        "    self.loc_initial = loc_initial\n",
        "\n",
        "  def fish_state_telegraph(self, fish_past, p_stay):\n",
        "    \"\"\"\n",
        "    fish state update according to telegraph process\n",
        "\n",
        "    Args:\n",
        "      fish_past (int): the fish location (-1 for left side, 1 for right side)\n",
        "      p_stay : the probability that the state of a certain site stays the same\n",
        "\n",
        "    Returns:\n",
        "      fish_new (int): updated fish location\n",
        "    \"\"\"\n",
        "    # we use logical operation XOR (denoted by ^ in python)\n",
        "    fish_new = (1 - binomial(1, p_stay)) ^ ((fish_past + 1) // 2)\n",
        "    fish_new = fish_new * 2 - 1\n",
        "\n",
        "    return fish_new\n",
        "\n",
        "\n",
        "  def fish_dynamics(self):\n",
        "    \"\"\"\n",
        "    fish state dynamics according to telegraph process\n",
        "\n",
        "    Returns:\n",
        "      fish_state (numpy array of int)\n",
        "    \"\"\"\n",
        "    T, p_stay, _, _, _ = self.params\n",
        "    fish_state = np.zeros(T, int)  # -1: left side ; 1: right side\n",
        "\n",
        "    # initialization\n",
        "    fish_state[0] = self.fish_initial\n",
        "\n",
        "    for t in range(1, T):\n",
        "        fish_state[t] = self.fish_state_telegraph(fish_state[t - 1], p_stay)\n",
        "\n",
        "    return fish_state\n",
        "\n",
        "  def generate_process_lazy(self):\n",
        "    \"\"\"\n",
        "    fish dynamics and measurements if you always stay in the intial location\n",
        "    without changing sides\n",
        "\n",
        "    Returns:\n",
        "      fish_state (numpy array of int): locations of the fish\n",
        "      loc (numpy array of int): left or right site, -1 for left, and 1 for right\n",
        "      measurement (numpy array of binary): whether a reward is obtained\n",
        "    \"\"\"\n",
        "\n",
        "    T, _, high_rew_p, low_rew_p, _ = self.params\n",
        "    rew_p_vector = np.array([low_rew_p, high_rew_p])\n",
        "\n",
        "    fish_state = self.fish_dynamics()\n",
        "    loc  = np.zeros(T, int)            # -1: left side, 1: right side\n",
        "    measurement = np.zeros(T, int)     # 0: no food, 1: get food\n",
        "\n",
        "    for t in range(0, T):\n",
        "      loc[t] = self.loc_initial\n",
        "      # new measurement\n",
        "      measurement[t] = binomial(1, rew_p_vector[(fish_state[t] == loc[t]) * 1])\n",
        "\n",
        "    return fish_state, loc, measurement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_lSZTOxYYMN"
      },
      "source": [
        "## Exercise 1.2\n",
        "\n",
        "Run the following code to see the dynamics of the fish as they follow the telegraph process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qelelR2bYYMO"
      },
      "source": [
        "def update_ex_1(p_stay=.95, high_rew_p=.4, low_rew_p=.1, T=200):\n",
        "  \"\"\"\n",
        "    p_stay: probability fish stay\n",
        "    high_rew_p: p(catch fish) when you're on their side\n",
        "    low_rew_p : p(catch fish) when you're on other side\n",
        "  \"\"\"\n",
        "\n",
        "  params = [T, p_stay, high_rew_p, low_rew_p, _]\n",
        "\n",
        "  #### initial condition for fish [fish_initial] and you [loc_initial] ####\n",
        "  binaryHMM_test = binaryHMM(params, fish_initial=-1, loc_initial=-1)\n",
        "\n",
        "  fish_state = binaryHMM_test.fish_dynamics()\n",
        "  plot_fish(fish_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "JDhH-EcPYYMO"
      },
      "source": [
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "widget=interactive(update_ex_1, {'manual': True},\n",
        "                high_rew_p=fixed(.4),\n",
        "                low_rew_p=fixed(.1),\n",
        "                p_stay=(.5, 1., .001),\n",
        "                T=fixed(200))\n",
        "\n",
        "widget.children[-2].description='Run Simulation'\n",
        "widget.children[-2].style.button_color='lightgreen'\n",
        "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
        "output = widget.children[-1]\n",
        "display(VBox([controls, output]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q52aHgJKYYMP"
      },
      "source": [
        "---\n",
        "# Section 2: Catch some fish on each side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oC_J2Kn0YYMP"
      },
      "source": [
        "#@title Video 2: Catch some fish\n",
        "video = YouTubeVideo(id='1-Wionllt9U', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmcKBd4M12s"
      },
      "source": [
        "# Summary of Video 2:\n",
        "\n",
        "- seen fish swimming back and forth between the left and the right side,\n",
        "with some typical duration, although it's a random process.\n",
        "That random process typically stays on one side for a number of time steps,\n",
        "that depends on the probability of staying for one time step.\n",
        "- The general rule is that if your probability of staying is p,\n",
        "then the time scale on which you stay on one side is of the order 1/(1-p).\n",
        "That's a good order of magnitude estimate for this process.\n",
        "- understand how we can assess where the fish are, using our measurements.\n",
        "We'll have two different probabilities -\n",
        "One, of catching fish if you're on the same side as the fish.\n",
        "And another if you're on the other side from the fish.\n",
        "Of course, you'll catch more fish when you're on the side with the school.\n",
        "- enforce the fish to stay on one side by picking p_stay = 1\n",
        "Then you have two reward probabilities, p_high, p_low\n",
        "You'll catch fish on one of those sides or the other, depending on your initial condition.\n",
        "\\- You'll just stay on that side,\n",
        "and the fish will stay on that side because p_stay = 1.\n",
        "See, when you catch the fish.\n",
        "Then, you can switch so the fish start on the other side.\n",
        "You can compare how often you catch fish in those cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqzW7Py8YYMP"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Now set $p_{\\rm stay} = 1$ so that the state of the two sites are fixed, and we can directly see the chances of catching fish on each side. The variable `fish_initial` indicates the initial side of the fish, and `loc_initial` indicates your initial location. They each take value $-1$ for left and $1$ for right.\n",
        "\n",
        "**Instructions:**\n",
        "1. set the two locations (`fish_initial` and `loc_initial`) to be the _same_, and measure when you catch fish.\n",
        "2. set the two locations (`fish_initial` and `loc_initial`) to be the _different_, and measure when you catch fish.\n",
        "3. visually compare the measurements from 1 and 2.\n",
        "4. Finally, you can also play around with `high_rew_p` (high reward probability) and `low_rew_p` (low reward probability) sliders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEk23IemYYMQ"
      },
      "source": [
        "def update_ex_2(p_stay=1., high_rew_p=.6, low_rew_p=.05, T=100):\n",
        "  \"\"\"\n",
        "    p_stay: probability fish stay\n",
        "    high_rew_p: p(catch fish) when you're on their side\n",
        "    low_rew_p : p(catch fish) when you're on other side\n",
        "  \"\"\"\n",
        "  params = [T, p_stay, high_rew_p, low_rew_p, _]\n",
        "\n",
        "  #### initial condition for fish [fish_initial] and you [loc_initial] ####\n",
        "  binaryHMM_test = binaryHMM(params, fish_initial=-1, loc_initial=-1)\n",
        "\n",
        "  fish_state, loc, measurement = binaryHMM_test.generate_process_lazy()\n",
        "  plot_measurement(measurement)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lbz3MXxkYYMQ"
      },
      "source": [
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "widget=interactive(update_ex_2, {'manual': True},\n",
        "                high_rew_p=(.0, 1., .001),\n",
        "                low_rew_p=(.0, 1., .001),\n",
        "                p_stay=fixed(1.),\n",
        "                T=fixed(100))\n",
        "\n",
        "widget.children[-2].description='Run Simulation'\n",
        "widget.children[-2].style.button_color='lightgreen'\n",
        "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
        "output = widget.children[-1]\n",
        "display(VBox([controls, output]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsb1fmSYYMQ"
      },
      "source": [
        "---\n",
        "# Section 3: Belief dynamics and belief distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "01w_qMB6YYMR"
      },
      "source": [
        "#@title Video 3: Where are the fish?\n",
        "video = YouTubeVideo(id='wCzVnnd4bmg', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9YYHoqANpvK"
      },
      "source": [
        "# Summary of Video 3:\n",
        "\n",
        "- allow the fish to move, according to a telegraph process.\n",
        "That makes this a Hidden Markov Model, again with a binary latent state.\n",
        "Unlike yesterday with the Drift Diffusion Model (DMM), we will have binary observations\n",
        "i.e. Did you catch a fish in this time step or not?\n",
        "Also, unlike before, you're only observing on one side,\n",
        "which means that the quality of your evidence is time-dependent.\n",
        "Nonetheless, you can still use the same recurrence equations\n",
        "that we saw for the Hidden Markov Model,\n",
        "which expresses the probability that the fish are in a given state,\n",
        "given all of the measurements up till that time,\n",
        "as a product of the measurement probability for the fish being in a state,\n",
        "times the prediction of the fish being in that state, given the past evidence.\n",
        "\n",
        "- update that equation\n",
        "according to the fact that we are making actions that affect the world state,\n",
        "i.e. the state of the fish, and us, especially us.\n",
        "Now we have the same probabilities, but they are also conditioned on what actions we've taken.\n",
        "But it's still recurrence.\n",
        "Now this posterior will quantify our belief about where the fish are.\n",
        "\n",
        "- plot those beliefs, follow the Telegraph Process,\n",
        "update the beliefs according to the Hidden Markov Model recurrence equations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASc36h4JYYMR"
      },
      "source": [
        "## Exercise 3.1: Plot belief dynamics and belief distributions\n",
        "\n",
        "We have provided a class for the binary task, with the agent always staying at one side following a lazy policy function `def policy_lazy(belief, loc)` that we provided. Now in this exercise, you will extend the module to generate the real dynamics, including beliefs and a moving agent. With the generated data, we will see how beliefs change over time, and how often different beliefs happen.\n",
        "\n",
        "For convenience, your belief at time *t* is actually a 2-dimensional vector. The first element is the belief that the fish are on the left, and the second element is the belief the fish are on the right. At every time, these elements sum to $1$.\n",
        "\n",
        "We will first check the dynamics with lazy policy, and then explore the case with a threshold-based policy.\n",
        "\n",
        "**Instructions:**\n",
        "Evaluate the cells below to setup the lazy policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "H3BiOQwBYYMR"
      },
      "source": [
        "class binaryHMM_belief(binaryHMM):\n",
        "\n",
        "  def __init__(self, params, fish_initial = -1, loc_initial = -1,\n",
        "                choose_policy = 'threshold'):\n",
        "    binaryHMM.__init__(self, params, fish_initial, loc_initial)\n",
        "    self.choose_policy = choose_policy\n",
        "\n",
        "  def generate_process(self):\n",
        "    \"\"\"\n",
        "    fish dynamics and measurements based on the choosen policy\n",
        "\n",
        "    Returns:\n",
        "      belief (numpy array of float): belief on the states of the two sites\n",
        "      act (numpy array of string): actions over time\n",
        "      loc (numpy array of int): left or right site\n",
        "      measurement (numpy array of binary): whether a reward is obtained\n",
        "      fish_state (numpy array of int): fish locations\n",
        "    \"\"\"\n",
        "\n",
        "    T, p_stay, high_rew_p, low_rew_p, threshold = self.params\n",
        "    fish_state = self.fish_dynamics()     # -1: left side; 1: right side\n",
        "    loc = np.zeros(T, int)                # -1: left side, 1: right side\n",
        "    measurement = np.zeros(T, int)        # 0: no food, 1: get food\n",
        "    act = np.empty(T, dtype='object')     # \"stay\", or \"switch\"\n",
        "    belief = np.zeros((2, T), float)      # the probability that the fish is on the left (1st element)\n",
        "                                          # or on the right (2nd element),\n",
        "                                          # the beliefs on the two boxes sum up to be 1\n",
        "\n",
        "    rew_prob = np.array([low_rew_p, high_rew_p])\n",
        "\n",
        "    # initialization\n",
        "    loc[0] = -1\n",
        "    measurement[0] = 0\n",
        "    belief_0 = np.random.random(1)[0]\n",
        "    belief[:, 0] = np.array([belief_0, 1 - belief_0])\n",
        "    act[0] = self.policy(threshold, belief[:, 0], loc[0])\n",
        "\n",
        "    for t in range(1, T):\n",
        "      if act[t - 1] == \"stay\":\n",
        "        loc[t] = loc[t - 1]\n",
        "      else:\n",
        "        loc[t] = - loc[t - 1]\n",
        "\n",
        "      # new measurement\n",
        "      measurement[t] = binomial(1, rew_prob[(fish_state[t] == loc[t]) * 1])\n",
        "      belief[0, t] = self.belief_update(belief[0, t - 1] , loc[t],\n",
        "                                        measurement[t], p_stay,\n",
        "                                        high_rew_p, low_rew_p)\n",
        "      belief[1, t] = 1 - belief[0, t]\n",
        "\n",
        "      act[t] = self.policy(threshold, belief[:, t], loc[t])\n",
        "\n",
        "    return belief, loc, act, measurement, fish_state\n",
        "\n",
        "  def policy(self, threshold, belief, loc):\n",
        "    \"\"\"\n",
        "    chooses policy based on whether it is lazy policy\n",
        "        or a threshold-based policy\n",
        "\n",
        "    Args:\n",
        "      threshold (float): the threshold of belief on the current site,\n",
        "          when the belief is lower than the threshold, switch side\n",
        "      belief (numpy array of float): the belief on the two sites\n",
        "      loc (int) : the location of the agent\n",
        "\n",
        "    Returns:\n",
        "      act (string): \"stay\" or \"switch\"\n",
        "    \"\"\"\n",
        "    if self.choose_policy == \"threshold\":\n",
        "      act = policy_threshold(threshold, belief, loc)\n",
        "    if self.choose_policy == \"lazy\":\n",
        "      act = policy_lazy(belief, loc)\n",
        "\n",
        "    return act\n",
        "\n",
        "  def belief_update(self, belief_past, loc, measurement, p_stay,\n",
        "                    high_rew_p, low_rew_p):\n",
        "    \"\"\"\n",
        "    using PAST belief on the LEFT box, CURRENT location and\n",
        "        and measurement to update belief\n",
        "    \"\"\"\n",
        "    rew_prob_matrix = np.array([[1 - high_rew_p, high_rew_p],\n",
        "                                [1 - low_rew_p, low_rew_p]])\n",
        "\n",
        "    # update belief posterior, p(s[t] | measurement(0-t), act(0-t-1))\n",
        "    belief_0 = (belief_past * p_stay  + (1 - belief_past) * (1 - p_stay)) *\\\n",
        "                            rew_prob_matrix[(loc + 1) // 2, measurement]\n",
        "    belief_1 = ((1 - belief_past) * p_stay + belief_past * (1 - p_stay)) *\\\n",
        "                            rew_prob_matrix[1-(loc + 1) // 2, measurement]\n",
        "\n",
        "    belief_0 = belief_0 / (belief_0 + belief_1)\n",
        "\n",
        "    return belief_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeM8_FFsYYMS"
      },
      "source": [
        "def policy_lazy(belief, loc):\n",
        "  \"\"\"\n",
        "  This function is a lazy policy where stay is also taken\n",
        "  \"\"\"\n",
        "  act = \"stay\"\n",
        "\n",
        "  return act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3xTKkepYYMS"
      },
      "source": [
        "### Exercise 3.1.1 Belief Update (Optional) \n",
        "The belief update function is provided in the `binaryHMM_belief` class in the above exercise. One optional exercise is to write your own code for the belief update based on the past belief `belief_past`, your location `loc`, the observation `measurement`, and the probability that fish stays on one side `p_stay`.\n",
        "\n",
        "You should only do this if you still have time after Exercise 3.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Crnz1MsYYMS"
      },
      "source": [
        "## Exercise 3.2: Task dynamics following a **lazy** policy\n",
        "\n",
        "The parameter for policy `choose_policy` can be either \"*lazy*\" or \"*threshold*\". In the following example, use the lazy policy.\n",
        "\n",
        "**Instructions:**\n",
        "* With the class defined above, we have created an object of `binaryHMM_belief` given parameters of the dynamics, *params*, and a parameter for policy.\n",
        "* Run the dynamics and explain the time series of the beliefs you see.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKwa_WiYYMT"
      },
      "source": [
        "def update_ex_3(p_stay=.98, threshold=.2, high_rew_p=.4, low_rew_p=.1, T=200):\n",
        "  \"\"\"\n",
        "    p_stay: probability fish stay\n",
        "    high_rew_p: p(catch fish) when you're on their side\n",
        "    low_rew_p : p(catch fish) when you're on other side\n",
        "    threshold: threshold of belief below which switching is taken\n",
        "  \"\"\"\n",
        "\n",
        "  params = [T, p_stay, high_rew_p, low_rew_p, threshold]\n",
        "\n",
        "  #### initial condition for fish [fish_initial] and you [loc_initial] ####\n",
        "  binaryHMM_test = binaryHMM_belief(params, choose_policy=\"lazy\",\n",
        "                                    fish_initial=-1, loc_initial=-1)\n",
        "\n",
        "  belief, loc, act, measurement, fish_state = binaryHMM_test.generate_process()\n",
        "  plot_dynamics(belief, loc, act, measurement, fish_state,\n",
        "                binaryHMM_test.choose_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DGN20VhGYYMT"
      },
      "source": [
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "widget=interactive(update_ex_3, {'manual': True},\n",
        "                high_rew_p=(.0, 1., .001),\n",
        "                low_rew_p=(.0, 1., .001),\n",
        "                p_stay=(.5, 1., .001),\n",
        "                T=fixed(200),\n",
        "                threshold=fixed(.2))\n",
        "\n",
        "widget.children[-2].description='Run Simulation'\n",
        "widget.children[-2].style.button_color='lightgreen'\n",
        "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
        "output = widget.children[-1]\n",
        "display(VBox([controls, output]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlvsMd-IYYMT"
      },
      "source": [
        "---\n",
        "# Section 4: Implementing threshold policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "HAit79iOYYMU"
      },
      "source": [
        "#@title Video 4: How should you act?\n",
        "video = YouTubeVideo(id='G3fNz23IDUg', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJmALlkOvFK"
      },
      "source": [
        "# Summary of Video 4:\n",
        "\n",
        "- the fish are bouncing back and forth again, and\n",
        "you're catching different fish at different times.\n",
        "But now you also see the belief that the fish are on the left evolving over time.\n",
        "Over here when the fish are in one place and you're catching a lot of fish\n",
        "then your belief that you're in the correct place is quite high.\n",
        "But then the fish leave, you don't catch very many and your belief goes down.\n",
        "- There are times later when the fish are on the other side,\n",
        "but still every once in a while you catch the rare fish, and\n",
        "suddenly you have a glimmer of hope that maybe the fish are back on your side.\n",
        "But then no, it decays back down towards the low probability.\n",
        "\n",
        "-  Before, we were just taking a lazy policy of staying on one side.\n",
        "And now we have to choose how to act in order to maximize our utility. So what is that utility?\n",
        "Well, the world state utility. We define the utility of catching a fish as being 1.\n",
        "Now the utility of the state, which is you being on the same side as the fish, let's say, or the other side,\n",
        "it's the utility of the fish times the probability of catching the fish in that state.\n",
        "Okay, so if you have a 40% chance, the utility of that state is 0.4.\n",
        "Also, you have a utility of acting, and those actions could be stay or switch.\n",
        "Staying = no cost, and switching, you incur a travel cost - so minus that cost.\n",
        "Now we have to consider what happens as a function of your belief about the state,\n",
        "which is the same utility but now multiplied by your confidence that you're in that state.\n",
        "And there's always the probability that you're on the other side too, which has its own utility.\n",
        "So you take a weighted sum of those two utilities given your probability or your belief about that.\n",
        "Now we have to choose our policy.\n",
        "\n",
        "- What is a policy? It's what to do in any given situation and here the situation is the current state.\n",
        "That means that you have your own position, which you know, and you have a belief about the fish position,\n",
        "which is the posterior that the fish are on the right or on the left given your measurements and past actions.\n",
        "There are really only two actions here, stay and switch. So which one are you going to pick?\n",
        "\n",
        "Well,\n",
        "the policy that we're going to impose here is to switch when the chance that you're on the same side as the fish\n",
        "is too low.\n",
        "How low is too low? Well, that's a parameter that we have control over.\n",
        "So here, if we plot the the actions, either stay or switch, as a function of your belief of whether you're on the right side,\n",
        "we're going to have a step function.\n",
        "If you believe you're on the wrong side, then you'll switch and if you believe you're on the right side,\n",
        "you'll stay, and there's some threshold that we will define which determines when your belief is low enough\n",
        "and it's time to switch.\n",
        "\n",
        "- So we will change the lazy policy to a threshold policy first by commenting out the lazy line and\n",
        "uncommenting the threshold line.\n",
        "And then you have to define what that policy is.\n",
        "This is an action, again, stay or switch, which is based on the threshold and the belief and your current location.\n",
        "Run and plot those dynamics again and see how things\n",
        "compare to when you are following the lazy policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIt1fAewYYMU"
      },
      "source": [
        "## Exercise 4: dynamics following a **threshold-based** policy.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "* You need to code a new policy `def policy_threshold(threshold, belief, loc)`. The policy takes three inputs: your belief about the fish state, your location (\"Left\" or \"Right\"), and a belief _threshold_: when your belief that you are on the same side as the fish drops below this threshold, you choose to switch; otherwise you stay.\n",
        "\n",
        "* You should return an action for each time *t*, which takes the value of \"stay\" or \"switch\".\n",
        "\n",
        "* After you complete the code for the policy based on threshold, create an object of `binaryHMM_belief` and set the policy parameter to be `choose_policy = threshold`.\n",
        "* We have provided an example of the parameters. You should play with the parameters to see the various dynamics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "vYntol8RYYMU"
      },
      "source": [
        "def policy_threshold(threshold, belief, loc):\n",
        "  \"\"\"\n",
        "  chooses whether to switch side based on whether the belief\n",
        "      on the current site drops below the threshold\n",
        "\n",
        "  Args:\n",
        "    threshold (float): the threshold of belief on the current site,\n",
        "                        when the belief is lower than the threshold, switch side\n",
        "    belief (numpy array of float, 2-dimensional): the belief on the\n",
        "                                                  two sites at a certain time\n",
        "    loc (int) : the location of the agent at a certain time\n",
        "                -1 for left side, 1 for right side\n",
        "\n",
        "  Returns:\n",
        "    act (string): \"stay\" or \"switch\"\n",
        "  \"\"\"\n",
        "  ############################################################################\n",
        "  ## Insert code to:\n",
        "  ## generate actions (Stay or Switch) for current belief and location\n",
        "  ##\n",
        "  ## Belief is a 2d vector: first element = Prob(fish on Left | measurements)\n",
        "  ##                       second element = Prob(fish on Right  | measurements)\n",
        "  ## Returns \"switch\" if Belief that fish are in your current location < threshold\n",
        "  ##         \"stay\" otherwise\n",
        "  ##\n",
        "  ## Hint: use loc value to determine which row of belief you need to use\n",
        "  ##       see the docstring for more information about loc\n",
        "  ##\n",
        "  ## complete the function and remove\n",
        "  raise NotImplementedError(\"Student exercise: Please complete <act>\")\n",
        "  ############################################################################\n",
        "  # Write the if statement\n",
        "  if ...:\n",
        "    # action below threshold\n",
        "    act = ...\n",
        "  else:\n",
        "    # action above threshold\n",
        "    act = ...\n",
        "\n",
        "  return act\n",
        "\n",
        "\n",
        "# uncomment the line below to test your function\n",
        "# test_policy_threshold()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "e336f814-79ab-4e9b-9def-08d9f637d5c5",
        "id": "DkmgC5qVYYMU"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D4_OptimalControl/solutions/W2D4_Tutorial1_Solution_d61b596a.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1zt-LBqYYMV"
      },
      "source": [
        "def update_ex_4(p_stay=.98, threshold=.2, high_rew_p=.4, low_rew_p=.1, T=200):\n",
        "  \"\"\"\n",
        "    p_stay: probability fish stay\n",
        "    high_rew_p: p(catch fish) when you're on their side\n",
        "    low_rew_p : p(catch fish) when you're on other side\n",
        "    threshold: threshold of belief below which switching is taken\n",
        "  \"\"\"\n",
        "  params = [T, p_stay, high_rew_p, low_rew_p, threshold]\n",
        "\n",
        "  #### initial condition for fish [fish_initial] and you [loc_initial] ####\n",
        "  binaryHMM_test = binaryHMM_belief(params, fish_initial=-1, loc_initial=-1,\n",
        "                                    choose_policy=\"threshold\")\n",
        "\n",
        "  belief, loc, act, measurement, fish_state = binaryHMM_test.generate_process()\n",
        "  plot_dynamics(belief, loc, act, measurement,\n",
        "                fish_state, binaryHMM_test.choose_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jSY8RHYWYYMV"
      },
      "source": [
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "widget=interactive(update_ex_4, {'manual': True},\n",
        "                high_rew_p=fixed(.4),\n",
        "                low_rew_p=fixed(.1),\n",
        "                p_stay=fixed(.95),\n",
        "                T=fixed(200),\n",
        "                threshold=(.0, 1., .001))\n",
        "\n",
        "widget.children[-2].description='Run Simulation'\n",
        "widget.children[-2].style.button_color='lightgreen'\n",
        "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
        "output = widget.children[-1]\n",
        "display(VBox([controls, output]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06B5YFxqYYMV"
      },
      "source": [
        "---\n",
        "# Section 5: Implementing Value function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OZn27kEAYYMV"
      },
      "source": [
        "#@title Video 5: Evaluate policy\n",
        "video = YouTubeVideo(id='aJhffROC74w', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2cAEte5Pi8-"
      },
      "source": [
        "# Summary of Video 5:\n",
        "\n",
        "- you've seen the dynamics with the policy we have to evaluate those policies.\n",
        "What's the value that's the total expected utility?\n",
        "There are two ways to calculate this.\n",
        "\t- One is by using the fraction of time that you're on each side and the fraction of time you're taking actions.\n",
        "\t- But here the simpler approach is just simply to sum up the rewards that you get over time and\n",
        "subtract off the costs that you incur over time. \n",
        "\n",
        "You'll define a value function which just adds up all the fish you catch minus the costs of switching.\n",
        "Then plot the value as a function of the threshold, plot and then by eye you can pick the optimal threshold and you'll see a curve\n",
        "and the optimal threshold will be the peak of that curve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXTbjo19YYMW"
      },
      "source": [
        "## Exercise 5\n",
        "Now we have generated behavior for a policy parameterized by a threshold. While it seems clear that this is at least better than being lazy, we want to know how good it is. For that, we will calculate a _value function_. We will use this value to compare different policies, and maximize the amount of fish we catch while minimizing our effort. \n",
        "\n",
        "Specifically, here the value is total expected utility per unit time.\n",
        "\n",
        "$$V(\\theta) = \\frac{1}{T}\\left(\\sum_t U_s(s_t) + U_a(a_t)\\right)$$ \n",
        "\n",
        "where $U_s(s_t)$ is the instantaneous utility (reward) from the site, and $U_a(a_t)$ is the utility (negative cost) for the chosen action. Here, the action cost is 0 if you stay, and `cost_sw` if you switch. \n",
        "\n",
        "We could take this average mathematically over the probabilities of rewards and actions. More simply, we get the same answer by simply averaging the _actual_ rewards and costs over a long time, so that's what you should do.\n",
        "\n",
        "\n",
        "**Instructions**\n",
        "* Fill in the function `value_function(measurement, act, cost_sw)` given a sequence of measurements, actions, and the cost of switching.\n",
        "* Visually find the threshold that yields the highest total value. We have provided code for plotting value versus threshold. The threshold $\\theta^*$ with the highest value gives the optimal policy for controlling where you should fish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "P1Ii4aO2YYMW"
      },
      "source": [
        "def value_function(measurement, act, cost_sw):\n",
        "  \"\"\"\n",
        "  value function\n",
        "\n",
        "  Args:\n",
        "    act (numpy array of string): length T with each element\n",
        "                                  taking value \"stay\" or \"switch\"\n",
        "    cost_sw (float): the cost of switching side\n",
        "    measurement (numpy array of binary): whether a reward is obtained\n",
        "\n",
        "  Returns:\n",
        "    value (float): expected utility per unit time\n",
        "  \"\"\"\n",
        "  act_int = (act == \"switch\").astype(int)\n",
        "  T = len(measurement)\n",
        "  ############################################################################\n",
        "  ## Insert your code here to:\n",
        "  ##        compute the value function = rate of catching fish - costs\n",
        "  ##\n",
        "  ## complete the function and remove\n",
        "  raise NotImplementedError(\"Student exercise: Please complete <value>\")\n",
        "  ############################################################################\n",
        "  # Calculate the value function\n",
        "  value = ...\n",
        "\n",
        "  return value\n",
        "\n",
        "# uncomment the line below to test your function\n",
        "# test_value_function()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "94f5c068-5642-4122-d138-2d220ab871a4",
        "id": "43HxrfIUYYMX"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D4_OptimalControl/solutions/W2D4_Tutorial1_Solution_205c2a7d.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8-1oHGiYYMX"
      },
      "source": [
        "# Brute force search for optimal policy: loop over thresholds and compute value for each.\n",
        "# This function is needed for the second exercise.\n",
        "\n",
        "\n",
        "def value_threshold(params, cost_sw, step):\n",
        "  threshold_array = np.arange(0, .5 + step, step)\n",
        "  value_array = np.zeros(threshold_array.shape)\n",
        "\n",
        "  T, p_stay, high_rew_p, low_rew_p, _ = params\n",
        "\n",
        "  for i in range(len(threshold_array)):\n",
        "    threshold = threshold_array[i]\n",
        "\n",
        "    params = [T, p_stay, high_rew_p, low_rew_p, threshold]\n",
        "    binaryHMM_test = binaryHMM_belief(params, choose_policy=\"threshold\")\n",
        "    belief, loc, act, measurement, fish_state = binaryHMM_test.generate_process()\n",
        "\n",
        "    value_array[i] = value_function(measurement, act, cost_sw)\n",
        "\n",
        "  return threshold_array, value_array\n",
        "\n",
        "\n",
        "plot_value_threshold(cost_sw=0.5, p_stay=0.95, high_rew_p=0.4, low_rew_p=0.1, T=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxITjCSoYYMX"
      },
      "source": [
        "---\n",
        "# Section 6 (Optional): Different task, different optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OPsCL3LYYYMX"
      },
      "source": [
        "#@title Video 6: Sensitivity of optimal policy\n",
        "video = YouTubeVideo(id='wd8IVsKoEfA', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb0mAPh2P7A7"
      },
      "source": [
        "# Summary of Video 6:\n",
        "\n",
        "It turns out that the threshold policy is in fact optimal control for this problem\n",
        "if you choose the threshold well.\n",
        "- now see how that optimal threshold varies as a function of the parameters of the task,\n",
        "including the travel cost, the fish dynamics, and the odds of catching fish.\n",
        "- look at the value function and understand \n",
        "why it changes the way it does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigWz52vYYMY"
      },
      "source": [
        "## Exercise 6\n",
        "This is a thinking exercise.\n",
        "\n",
        "**Instructions:**\n",
        "After plotting value versus threshold, adjust various task parameters using the sliders below, and observe how the optimal threshold moves with\n",
        "* switching cost (`cost_sw`)\n",
        "* fish dynamics (`p_switch`)\n",
        "* probability of catching fish on each side, `low_rew_p` and `high_rew_p`\n",
        "\n",
        "Can you explain why the optimal threshold changes with these parameters?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4QLLnrgOYYMY"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "widget=interactive(plot_value_threshold, {'manual': True},\n",
        "                   T=fixed(10000),\n",
        "                   p_stay=(0.5, 1., 0.001),\n",
        "                   high_rew_p=(0., 1., 0.001),\n",
        "                   low_rew_p=(0., 1., 0.001),\n",
        "                   cost_sw=(0., 2., .1),\n",
        "                   step=fixed(0.1))\n",
        "\n",
        "widget.children[-2].description='Run Simulation'\n",
        "widget.children[-2].style.button_color='lightgreen'\n",
        "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
        "output = widget.children[-1]\n",
        "display(VBox([controls, output]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UYi8-SMYYMY"
      },
      "source": [
        "**Questions:**\n",
        "\n",
        "EXPLAIN why the optimal threshold changes for:\n",
        "* lower switching cost?\n",
        "* faster fish dynamics?\n",
        "* rarer fish?\n",
        "\n",
        "Note that it may require long simulations to see subtle changes in values of different policies, so look for coarse trends first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4SijydSRUHA"
      },
      "source": [
        "*Hints*:\n",
        "\n",
        "1. Think about the relationship between threshold and switching. What happens in the limit of zero switching cost?\n",
        "2. Think about how dynamics affect switching. \n",
        "3. How does fish quantity affect confidence of prediction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPgCUQtSYYMY"
      },
      "source": [
        "**Answers:**\n",
        "* High switching cost means that you should be more certain that the other side is better before committing to change sides. This means that beliefs must fall below a threshold before acting. Conversely, a lower switching cost allows you more flexibility to switch at less stringent thresholds. In the limit of _zero_ switching cost, you should always switch whenever you think the other side is better, even if it's just 51%, and even if you switch every time step.\n",
        "* Faster fish dynamics (lower `p_stay`) also promotes faster switching, because you cannot plan as far into the future. In that case you must base your decisions on more immediate evidence, but since you still pay the same switching cost that cost is a higher fraction of your predictable rewards. And thus you should be more conservative, and switch only when you are more confident.\n",
        "* When `high_rew_p` and/or `low_rew_p` decreases, your predictions become less reliable, again encouraging you to require more confidence before committing to a switch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "t7258_IkYYMZ"
      },
      "source": [
        "#@title Video 7: From discrete to continuous control\n",
        "video = YouTubeVideo(id='ndCMgdjv9Gg', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6NEL9XZWC8M"
      },
      "source": [
        "# Summary of Video 7:\n",
        "- moving from discrete to continuous control\n",
        "DISCRETE: you had discrete states actions and measurements\n",
        "and CONTINUOUS: you'll again control a POMDP, but now with continuous Gaussian states,\n",
        "continuous actions and continuous measurements.\n",
        "This will allow you to combine the control principles \n",
        "with the Kalman filtering \n",
        "and gradually build up to full linear quadratic\n",
        "Gaussian control which has all of the ingredients for control starting with open-loop control where you don't make measurements,\n",
        "and then include closed-loop control with measurements, but full measurements.\n",
        "And then finally, we'll allow those measurements to be unreliable and partial.\n"
      ]
    }
  ]
}