{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W2D1_Tutorial4",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A9KZ_MM_nnL"
      },
      "source": [
        "# NMA 2020 W2D1 -- (Bonus) Tutorial 4: Bayesian Decision Theory & Cost functions\n",
        "__Content creators:__ Vincent Valton, Konrad Kording, with help from Matthew Krause\n",
        "\n",
        "__Content reviewers:__ Matthew Krause, Jesse Livezey, Karolina Stosio, Saeed Salehi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9XI_A0L3K95"
      },
      "source": [
        "# Tutorial Objectives\n",
        "\n",
        "*This tutorial is optional! Please do not feel pressured to finish it!*\n",
        "\n",
        "In the previous tutorials, we investigated the posterior, which describes  beliefs based on a combination of current evidence and prior experience. This tutorial focuses on Bayesian Decision Theory, which combines the posterior with **cost functions** that allow us to quantify the potential impact of making a decision or choosing an action based on that posterior. Cost functions are therefore critical for turning probabilities into actions!\n",
        "\n",
        "In Tutorial 3, we used the mean of the posterior $p(x | \\tilde x)$ as a proxy for the response $\\hat x$ for the participants. What prompted us to use the mean of the posterior as a **decision rule**? In this tutorial we will see how different common decision rules such as the choosing the mean, median or mode of the posterior distribution correspond to minimizing different cost functions.\n",
        "\n",
        "In this tutorial, you will\n",
        "  1. Implement three commonly-used cost functions: mean-squared error, absolute error, and zero-one loss\n",
        "  2. Discover the concept of expected loss, and\n",
        "  3. Choose optimal locations on the posterior that minimize these cost functions. You will verify that these locations can be found analytically as well as empirically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "HEvryENG_nnY"
      },
      "source": [
        "#@title Video 1: Introduction\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='z2DF4H_sa-k', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnFVH2vS41L4"
      },
      "source": [
        "# Summary of Video 1:\n",
        "\n",
        "Bayesian Decision Theory.\n",
        "So, optimal decisions are not defined without objectives.\n",
        "- it's impossible to argue that some behavior is optimal, unless we can\n",
        "have reasons for why there is a cost function that we can understand.\n",
        "\n",
        "We call the opposite of the cost the utility function. We call them loss functions (expected rewards.)\n",
        "Cost functions are also otherwise all over in neuroscience.\n",
        "All our experiments, or a good part of our experiments, are\n",
        "explicitly or implicitly designed around cost functions, and, arguably,\n",
        "evolution gives us cost functions as well.\n",
        "\n",
        "So, in the case of causal inference, \n",
        "We assumed that people estimate the mean of the posterior.\n",
        "\n",
        "So, let's recapitulate what the situation is.\n",
        "So, we have an actual stimulus. We know about that because we designed the experiment,\n",
        "and we write down what the stimulus is at a given point of time.\n",
        "This is x.\n",
        "What arrives in the brain is x̃,\n",
        "which is a variable that we cannot know if we're on the outside.\n",
        "And then, we have Subject's estimate, x̂.\n",
        "\n",
        "Now, the loss for the subject is -\n",
        "- the behavior is generally we ask people to reveal their estimates -\n",
        "So, the loss (L) depends on the difference between the actual stimulus and the Subject's estimate.\n",
        "\n",
        "So, what are the cost functions that people could care about?\n",
        "They could care about the so called 0-1 loss function,\n",
        "which is, if you're perfect, then great! You get a point, you win!\n",
        "Otherwise, you lose a point.\n",
        "It could be the absolute error. Twice the size of the error, twice as bad.\n",
        "It could be the square error. Twice as big the error, four times as bad!\n",
        "Or, it could be some arbitrary other function. The important thing is,\n",
        "the lost function for such estimation problems is\n",
        "defined in terms of the difference between what people estimate, and what the stimulus really is.\n",
        "\n",
        "Is that the best estimate? Well, certainly, the estimate that makes it the most probable that we are right.\n",
        "But, alternatively, maybe we want to go for the median of the data,\n",
        "or the mean of the data. Now, which one should we take?\n",
        "Which one we should take will ultimately depend on what our cost function is.\n",
        "\n",
        "The loss is defined as the following :\n",
        "it's a function, L, that depends on x minus x̂, the difference between my estimate and what reality is.\n",
        "Now, can I simply choose x̂ as x? Now that would be perfect! It would produce zero loss.\n",
        "No, because I only ever have x̃.\n",
        "x̃ is what goes into my nervous system\n",
        "\n",
        "So what I have to do is estimate\n",
        "what I should believe about x, and how my loss looks like and then, I can make an estimate.\n",
        "\n",
        "So how may the loss look like?\n",
        "So, as before we now have a marginalization parameter.\n",
        "\n",
        "We want to calculate the expected loss\n",
        "\n",
        "But, we don't know what the right x is! So what do we have to do? Well, we have to consider each possible x,\n",
        "\n",
        "proportional to what its probability is! So that's what we have here -\n",
        "\n",
        "We have the posterior - the probability of x, given x̃, which is the posterior that we believe\n",
        "\n",
        "subjects calculate. Now, we need to multiply that with the loss that would be expected if x was true.\n",
        "\n",
        "Therefore, the expected loss is the integral over x of the loss that would be associated\n",
        "\n",
        "with x, if we chose x̂, times the probability of x given x̃ and that's\n",
        "\n",
        "The probability here is the posterior and then we need to integrate dx. That's why we have this.\n",
        "\n",
        "So in the 0-1 loss,\n",
        "\n",
        "I think it's relatively easy. We get a reward only if we get it right.\n",
        "\n",
        "So the integral takes on a trivial form, that only one point matters in the entire integral.\n",
        "\n",
        "So, therefore, we want to go for the highest posterior probability,\n",
        "\n",
        "which is the mode.\n",
        "\n",
        "There's other losses- absolute error, squared error.\n",
        "\n",
        "So let's see what the predictions are in those different cases.\n",
        "\n",
        "So what you will now do is, you will implement\n",
        "\n",
        "the various loss functions - mean square error loss, absolute error loss, 0-1 loss, and so on, and so forth,\n",
        "and then we will \n",
        "explore what's the best estimate, for all of them.\n",
        "\n",
        "Please write the cost functions and visualize them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8P6PhY0_nna"
      },
      "source": [
        "---\n",
        "Please execute the cell below to initialize the notebook environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEkGeq9U_nnc"
      },
      "source": [
        "--- \n",
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "A8UDXIdb_nnc"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5_J847qL_nne"
      },
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eQ3qXq2g_nne"
      },
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "def my_gaussian(x_points, mu, sigma):\n",
        "  \"\"\"Returns un-normalized Gaussian estimated at points `x_points`\n",
        "\n",
        "  DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "  Args :\n",
        "    x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
        "    mu (scalar) - mean of the Gaussian\n",
        "    sigma (scalar) - std of the gaussian\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats): un-normalized Gaussian (i.e. without constant) evaluated at `x`\n",
        "  \"\"\"\n",
        "  return np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
        "\n",
        "def visualize_loss_functions(mse=None, abse=None, zero_one=None):\n",
        "  \"\"\"Visualize loss functions\n",
        "    Args:\n",
        "      - mse (func) that returns mean-squared error\n",
        "      - abse: (func) that returns absolute_error\n",
        "      - zero_one: (func) that returns zero-one loss\n",
        "    All functions should be of the form f(x, x_hats). See Exercise #1.\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "\n",
        "  x = np.arange(-3, 3.25, 0.25)\n",
        "\n",
        "  fig, ax = plt.subplots(1)\n",
        "\n",
        "  if mse is not None:\n",
        "    ax.plot(x, mse(0, x), linewidth=2, label=\"Mean Squared Error\")\n",
        "  if abse is not None:\n",
        "    ax.plot(x, abse(0, x), linewidth=2, label=\"Absolute Error\")\n",
        "  if zero_one_loss is not None:\n",
        "    ax.plot(x, zero_one_loss(0, x), linewidth=2, label=\"Zero-One Loss\")\n",
        "\n",
        "  ax.set_ylabel('Cost')\n",
        "  ax.set_xlabel('Predicted Value ($\\hat{x}$)')\n",
        "  ax.set_title(\"Loss when the true value $x$=0\")\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "def moments_myfunc(x_points, function):\n",
        "    \"\"\"Returns the mean, median and mode of an arbitrary function\n",
        "\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Args :\n",
        "      x_points (numpy array of floats) - x-axis values\n",
        "      function (numpy array of floats) - y-axis values of the function evaluated at `x_points`\n",
        "\n",
        "    Returns:\n",
        "       (tuple of 3 scalars): mean, median, mode\n",
        "    \"\"\"\n",
        "\n",
        "    # Calc mode of an arbitrary function\n",
        "    mode = x_points[np.argmax(function)]\n",
        "\n",
        "    # Calc mean of an arbitrary function\n",
        "    mean = np.sum(x_points * function)\n",
        "\n",
        "    # Calc median of an arbitrary function\n",
        "    cdf_function = np.zeros_like(x_points)\n",
        "    accumulator = 0\n",
        "    for i in np.arange(x.shape[0]):\n",
        "        accumulator = accumulator + posterior[i]\n",
        "        cdf_function[i] = accumulator\n",
        "    idx = np.argmin(np.abs(cdf_function - 0.5))\n",
        "    median = x_points[idx]\n",
        "\n",
        "    return mean, median, mode\n",
        "\n",
        "def loss_plot(x, loss, min_loss, loss_label, show=False, ax=None):\n",
        "  if not ax:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  ax.plot(x, loss, '-C1', linewidth=2, label=loss_label)\n",
        "  ax.axvline(min_loss, ls='dashed', color='C1', label='Minimum')\n",
        "  ax.set_ylabel('Expected Loss')\n",
        "  ax.set_xlabel('Orientation (Degrees)')\n",
        "  ax.legend()\n",
        "\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "def loss_plot_subfigures(x,\n",
        "              MSEloss, min_MSEloss, loss_MSElabel,\n",
        "              ABSEloss, min_ABSEloss, loss_ABSElabel,\n",
        "              ZeroOneloss, min_01loss, loss_01label):\n",
        "\n",
        "  fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
        "  fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(fig_w*2, fig_h*2), sharex=True)\n",
        "\n",
        "  ax[0, 0].plot(x, MSEloss, '-C1', linewidth=2, label=loss_MSElabel)\n",
        "  ax[0, 0].axvline(min_MSEloss, ls='dashed', color='C1', label='Minimum')\n",
        "  ax[0, 0].set_ylabel('Expected Loss')\n",
        "  ax[0, 0].set_xlabel('Orientation (Degrees)')\n",
        "  ax[0, 0].set_title(\"Mean Squared Error\")\n",
        "  ax[0, 0].legend()\n",
        "\n",
        "  pmoments_plot(x, posterior, ax=ax[1,0])\n",
        "\n",
        "  ax[0, 1].plot(x, ABSEloss, '-C0', linewidth=2, label=loss_ABSElabel)\n",
        "  ax[0, 1].axvline(min_ABSEloss, ls='dashdot', color='C0', label='Minimum')\n",
        "  ax[0, 1].set_ylabel('Expected Loss')\n",
        "  ax[0, 1].set_xlabel('Orientation (Degrees)')\n",
        "  ax[0, 1].set_title(\"Absolute Error\")\n",
        "  ax[0, 1].legend()\n",
        "\n",
        "  pmoments_plot(x, posterior, ax=ax[1,1])\n",
        "\n",
        "\n",
        "  ax[0, 2].plot(x, ZeroOneloss, '-C2', linewidth=2, label=loss_01label)\n",
        "  ax[0, 2].axvline(min_01loss, ls='dotted', color='C2', label='Minimum')\n",
        "  ax[0, 2].set_ylabel('Expected Loss')\n",
        "  ax[0, 2].set_xlabel('Orientation (Degrees)')\n",
        "  ax[0, 2].set_title(\"0-1 Loss\")\n",
        "  ax[0, 2].legend()\n",
        "\n",
        "  pmoments_plot(x, posterior, ax=ax[1,2])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def pmoments_plot(x, posterior,\n",
        "                  prior=None, likelihood=None, show=False, ax=None):\n",
        "\n",
        "  if not ax:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "  if prior:\n",
        "    ax.plot(x, prior, '-C1', linewidth=2, label='Prior')\n",
        "  if likelihood:\n",
        "    ax.plot(x, likelihood, '-C0', linewidth=2, label='Likelihood')\n",
        "  ax.plot(x, posterior, '-C2', linewidth=4, label='Posterior')\n",
        "\n",
        "  mean, median, mode = moments_myfunc(x, posterior)\n",
        "\n",
        "  ax.axvline(mean, ls='dashed', color='C1', label='Mean')\n",
        "  ax.axvline(median, ls='dashdot', color='C0', label='Median')\n",
        "  ax.axvline(mode, ls='dotted', color='C2', label='Mode')\n",
        "  ax.set_ylabel('Probability')\n",
        "  ax.set_xlabel('Orientation (Degrees)')\n",
        "  ax.legend()\n",
        "\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def generate_example_pdfs():\n",
        "  \"\"\"Generate example probability distributions as in T2\"\"\"\n",
        "  x=np.arange(-5, 5, 0.01)\n",
        "\n",
        "  prior_mean = 0\n",
        "  prior_sigma1 = .5\n",
        "  prior_sigma2 = 3\n",
        "  prior1 = my_gaussian(x, prior_mean, prior_sigma1)\n",
        "  prior2 = my_gaussian(x, prior_mean, prior_sigma2)\n",
        "\n",
        "  alpha = 0.05\n",
        "  prior_combined = (1-alpha) * prior1 + (alpha * prior2)\n",
        "  prior_combined = prior_combined / np.sum(prior_combined)\n",
        "\n",
        "  likelihood_mean = -2.7\n",
        "  likelihood_sigma = 1\n",
        "  likelihood = my_gaussian(x, likelihood_mean, likelihood_sigma)\n",
        "  likelihood = likelihood / np.sum(likelihood)\n",
        "\n",
        "  posterior = prior_combined * likelihood\n",
        "  posterior = posterior / np.sum(posterior)\n",
        "\n",
        "  return x, prior_combined, likelihood, posterior\n",
        "\n",
        "def plot_posterior_components(x, prior, likelihood, posterior):\n",
        "  with plt.xkcd():\n",
        "    fig = plt.figure()\n",
        "    plt.plot(x, prior, '-C1', linewidth=2, label='Prior')\n",
        "    plt.plot(x, likelihood, '-C0', linewidth=2, label='Likelihood')\n",
        "    plt.plot(x, posterior, '-C2', linewidth=4, label='Posterior')\n",
        "    plt.legend()\n",
        "    plt.title('Sample Output')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtHlaPsz8K-k"
      },
      "source": [
        "# Summary of Helper functions:\n",
        "\n",
        "*my_gaussian*:\n",
        "eturns un-normalized Gaussian estimated at points `x_points` using arguments: mu and sigma\n",
        "\n",
        "*visualise*:\n",
        "visualise the loss function (mse/ absolute error/ 0-1 error) by plotting cost (yaxis) vs predicted value (xaxis)\n",
        "\n",
        "*moments_myfunc*:\n",
        "Returns the mean, median and mode of an arbitrary function\n",
        "\n",
        "*loss_plot*:\n",
        "plots expected loss (yaxis) with orientation (xaxis)\n",
        "\n",
        "*loss_plot_subfigures*:\n",
        "plots expected loss - mse/ absolute error/ 0-1 error (yaxis) with orientation (xaxis)\n",
        "\n",
        "*pmoments_plot*:\n",
        "plots probability (yaxis) vs orientation (xaxis)\n",
        "\n",
        "*generate_example_pdfs*:\n",
        "compute prior, prior for mixture of gaussians, likelihood, and posterior\n",
        "\n",
        "*plot_posterior_components*:\n",
        "plot the sample output of x vs prior/ likelihood/ posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRS_uGiR3K-A"
      },
      "source": [
        "### The Posterior Distribution\n",
        "\n",
        "This notebook will use a model similar to the puppet & puppeteer sound experiment developed in Tutorial 2, but with different probabilities for $p_{common}$, $p_{independent}$, $\\sigma_{common}$ and $\\sigma_{independent}$. Specifically, our model will consist of these components, combined according to Bayes' rule:\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\textrm{Prior} &=& \\begin{cases} \\mathcal{N_{common}}(0, 0.5) & 95\\% \\textrm{ weight}\\\\\n",
        "                                 \\mathcal{N_{independent}}(0, 3.0) &  5\\% \\textrm{ weight} \\\\\n",
        "                    \\end{cases}\\\\\\\\\n",
        "\\textrm{Likelihood} &= &\\mathcal{N}(-2.7, 1.0)\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "We will use this posterior as an example through this notebook. Please run the cell below to import and plot the model. You do not need to edit anything. These parameter values were deliberately chosen for illustration purposes: there is nothing intrinsically special about them, but they make several of the exercises easier. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "921pzgHh_nnh"
      },
      "source": [
        "x, prior, likelihood, posterior = generate_example_pdfs()\n",
        "plot_posterior_components(x, prior, likelihood, posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Cc73Nm3K-A"
      },
      "source": [
        "# Section 1: The Cost Functions\n",
        "\n",
        "Next, we will implement the cost functions. \n",
        "A cost function determines the \"cost\" (or penalty) of estimating $\\hat{x}$ when the true or correct quantity is really $x$ (this is essentially the cost of the error between the true stimulus value: $x$ and our estimate: $\\hat x$ -- Note that the error can be defined in different ways):\n",
        "\n",
        "$$\\begin{eqnarray}\n",
        "\\textrm{Mean Squared Error} &=& (x - \\hat{x})^2 \\\\ \n",
        "\\textrm{Absolute Error} &=& \\big|x - \\hat{x}\\big| \\\\ \n",
        "\\textrm{Zero-One Loss} &=& \\begin{cases}\n",
        "                            0,& \\text{if } x = \\hat{x} \\\\\n",
        "                            1,              & \\text{otherwise}\n",
        "                            \\end{cases}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "In the cell below, fill in the body of these cost function. Each function should take one single value for $x$ (the true stimulus value : $x$) and one or more possible value estimates: $\\hat{x}$. \n",
        "\n",
        "Return an array containing the costs associated with predicting $\\hat{x}$ when the true value is $x$. Once you have written all three functions, uncomment the final line to visualize your results.\n",
        "\n",
        " _Hint:_ These functions are easy to write (1 line each!) but be sure *all* three functions return arrays of `np.float` rather than another data type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjB-stWI_nnl"
      },
      "source": [
        "## Exercise 1: Implement the cost functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "vqjKEpY5_nnl"
      },
      "source": [
        "def mse(x, x_hats):\n",
        "  \"\"\"Mean-squared error cost function\n",
        "    Args:\n",
        "      x (scalar): One true value of $x$\n",
        "      x_hats (scalar or ndarray): Estimate of x\n",
        "    Returns:\n",
        "      same shape/type as x_hats): MSE costs associated with\n",
        "      predicting x_hats instead of x$\n",
        "  \"\"\"\n",
        "\n",
        "  ##############################################################################\n",
        "  # Complete the MSE cost function\n",
        "  #\n",
        "  ### Comment out the line below to test your function\n",
        "  raise NotImplementedError(\"You need to complete the MSE cost function!\")\n",
        "  ##############################################################################\n",
        "\n",
        "  my_mse = ...\n",
        "  return my_mse\n",
        "\n",
        "\n",
        "def abs_err(x, x_hats):\n",
        "  \"\"\"Absolute error cost function\n",
        "    Args:\n",
        "      x (scalar): One true value of $x$\n",
        "      x_hats (scalar or ndarray): Estimate of x\n",
        "    Returns:\n",
        "      (same shape/type as x_hats): absolute error costs associated with\n",
        "      predicting x_hats instead of x$\n",
        "  \"\"\"\n",
        "\n",
        "  ##############################################################################\n",
        "  # Complete the absolute error cost function\n",
        "  #\n",
        "  ### Comment out the line below to test your function\n",
        "  raise NotImplementedError(\"You need to complete the absolute error function!\")\n",
        "  ##############################################################################\n",
        "\n",
        "  my_abs_err = ...\n",
        "  return my_abs_err\n",
        "\n",
        "\n",
        "def zero_one_loss(x, x_hats):\n",
        "  \"\"\"Zero-One loss cost function\n",
        "    Args:\n",
        "      x (scalar): One true value of $x$\n",
        "      x_hats (scalar or ndarray): Estimate of x\n",
        "    Returns:\n",
        "      (same shape/type as x_hats) of the 0-1 Loss costs associated with predicting x_hat instead of x\n",
        "  \"\"\"\n",
        "\n",
        "  ##############################################################################\n",
        "  # Complete the zero-one loss cost function\n",
        "  #\n",
        "  ### Comment out the line below to test your function\n",
        "  raise NotImplementedError(\"You need to complete the 0-1 loss cost function!\")\n",
        "  ##############################################################################\n",
        "\n",
        "  my_zero_one_loss = ...\n",
        "  return my_zero_one_loss\n",
        "\n",
        "\n",
        "## When you are done with the functions above, uncomment the line below to\n",
        "## visualize them\n",
        "# visualize_loss_functions(mse, abs_err, zero_one_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "e5e05e54-c2ce-4c81-84d3-c7a866c15209",
        "id": "iuHn7xQo_nnm"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial4_Solution_8da2f3c2.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=416 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_8da2f3c2_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1WYAQfA_nnm"
      },
      "source": [
        "# Section 2: Expected Loss\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7aG7QxJh_nnn"
      },
      "source": [
        "#@title Video 2: Expected Loss\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='FTBpCfylV_Y', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ9nqZIo9MBe"
      },
      "source": [
        "# Summary of Video 2:\n",
        "\n",
        "optimal estimates:\n",
        "the expected loss given a potential choice x̂: Well, it's the integral over x\n",
        "of the posterior, p of x given x̃, of the loss that depends on x minus x̂, integrated dx.\n",
        "Try all possible estimates, see which one is the best. \n",
        "We start with estimates,  potential estimates that we pull out an array of potential estimates, x.\n",
        "Then, we have the loss which is a function of the estimate minus x; \n",
        "Now calculate the expected loss\n",
        "for a potential choice,\n",
        "which is going to be, well, the sum of all possible values for the loss, times the posterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-4UohfG_nnn"
      },
      "source": [
        "A posterior distribution tells us about the confidence or credibility we assign to different choices. A cost function describes the penalty we incur when choosing an incorrect option. These concepts can be combined into an *expected loss* function. Expected loss is defined as:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "    \\mathbb{E}[\\text{Loss} | \\hat{x}] = \\int L[\\hat{x},x] \\odot  p(x|\\tilde{x}) dx\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "where $L[ \\hat{x}, x]$ is the loss function, $p(x|\\tilde{x})$ is the posterior, and $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e., elementwise multiplication), and $\\mathbb{E}[\\text{Loss} | \\hat{x}]$ is the expected loss. \n",
        "\n",
        "In this exercise, we will calculate the expected loss for the: means-squared error, the absolute error, and the zero-one loss over our bimodal posterior $p(x | \\tilde x)$. \n",
        "\n",
        "**Suggestions:**\n",
        "* We already pre-completed the code (commented-out) to calculate the mean-squared error, absolute error, and zero-one loss between $x$ and an estimate $\\hat x$ using the functions you created in exercise 1\n",
        "* Calculate the expected loss ($\\mathbb{E}[MSE Loss]$) using your posterior (imported above as `posterior`) & each of the loss functions described above (MSELoss, ABSELoss, and Zero-oneLoss).\n",
        "* Find the x position that minimizes the expected loss for each cost function and plot them using the `loss_plot` function provided (commented-out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHqQoBaH_nnn"
      },
      "source": [
        "## Exercise 2: Finding the expected loss empirically via integration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "I2xuVc_i_nno"
      },
      "source": [
        "def expected_loss_calculation(x, posterior):\n",
        "\n",
        "  ExpectedLoss_MSE = np.zeros_like(x)\n",
        "  ExpectedLoss_ABSE = np.zeros_like(x)\n",
        "  ExpectedLoss_01 = np.zeros_like(x)\n",
        "\n",
        "  for idx in np.arange(x.shape[0]):\n",
        "    estimate = x[idx]\n",
        "\n",
        "    ###################################################################\n",
        "    ## Insert code below to find the expected loss under each loss function\n",
        "    ##\n",
        "    ## remove the raise when the function is complete\n",
        "    raise NotImplementedError(\"Calculate the expected loss over all x values!\")\n",
        "    ###################################################################\n",
        "\n",
        "    MSELoss = mse(estimate, x)\n",
        "    ExpectedLoss_MSE[idx] = ...\n",
        "\n",
        "    ABSELoss = abs_err(estimate, x)\n",
        "    ExpectedLoss_ABSE[idx] = ...\n",
        "\n",
        "    ZeroOneLoss = zero_one_loss(estimate, x)\n",
        "    ExpectedLoss_01[idx] = ...\n",
        "\n",
        "  ###################################################################\n",
        "  ## Now, find the `x` location that minimizes expected loss\n",
        "  ##\n",
        "  ## remove the raise when the function is complete\n",
        "  raise NotImplementedError(\"Finish the Expected Loss calculation\")\n",
        "  ###################################################################\n",
        "\n",
        "  min_MSE = ...\n",
        "  min_ABSE = ...\n",
        "  min_01 = ...\n",
        "\n",
        "  return (ExpectedLoss_MSE, ExpectedLoss_ABSE, ExpectedLoss_01,\n",
        "          min_MSE, min_ABSE, min_01)\n",
        "\n",
        "## Uncomment the lines below to plot the expected loss as a function of the estimates\n",
        "#ExpectedLoss_MSE, ExpectedLoss_ABSE, ExpectedLoss_01,  min_MSE, min_ABSE, min_01 = expected_loss_calculation(x, posterior)\n",
        "#loss_plot(x, ExpectedLoss_MSE, min_MSE, f\"Mean Squared Error = {min_MSE:.2f}\")\n",
        "#loss_plot(x, ExpectedLoss_ABSE, min_ABSE, f\"Absolute Error = {min_ABSE:.2f}\")\n",
        "#loss_plot(x, ExpectedLoss_01, min_01, f\"Zero-One Error = {min_01:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "cfca1ee7-2e20-42a6-cce3-1f2861f1016e",
        "id": "1t1-d4rC_nnp"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial4_Solution_3a9250ef.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_3a9250ef_0.png>\n",
        "\n",
        "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_3a9250ef_1.png>\n",
        "\n",
        "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_3a9250ef_2.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kft-D4au_nnp"
      },
      "source": [
        "# Section 3: Analytical Solutions\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OrXvURz4_nnp"
      },
      "source": [
        "#@title Video 3: Analytical Solutions\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='wmDD51N9rs0', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVl9Hr8P989S"
      },
      "source": [
        "# Summary of Video 3:\n",
        "So there are many cases where we can analytically calculate the optimal estimates.\n",
        "- the expected value of the loss, given an estimate x̂ is the integral of the posterior times the loss.\n",
        "- At the maximum, at the best estimate that we will have,\n",
        "- the partial derivative must vanish. So we must have zero as the derivative of the expected loss\n",
        "after my estimate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxM8tENN_nnq"
      },
      "source": [
        "In the previous exercise, we found the minimum expected loss via brute-force: we searched over all possible values of $x$ and found the one that minimized each of our loss functions. This is feasible for our small toy example, but can quickly become intractable. \n",
        "\n",
        "Fortunately, the three loss functions examined in this tutorial have are minimized at specific points on the posterior, corresponding to the itss mean, median, and mode. To verify this property, we have replotted the loss functions from Exercise 2 below, with the posterior on the same scale beneath. The mean, median, and mode are marked on the posterior. \n",
        "\n",
        "Which loss form corresponds to each summary statistics? \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzcU4vpP_nnq"
      },
      "source": [
        "loss_plot_subfigures(x,\n",
        "                    ExpectedLoss_MSE, min_MSE, f\"Mean Squared Error = {min_MSE:.2f}\",\n",
        "                    ExpectedLoss_ABSE, min_ABSE, f\"Absolute Error = {min_ABSE:.2f}\",\n",
        "                    ExpectedLoss_01, min_01, f\"Zero-One Error = {min_01:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "704d3a6a-ea3a-4b1a-f83c-579585e9a4bf",
        "id": "_LvpkQcv_nnq"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial4_Solution_8cdbd46a.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0whwNH9f_nnr"
      },
      "source": [
        "# Section 4: Conclusion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "p0K5xZEq_nns"
      },
      "source": [
        "#@title Video 4: Outro\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='3nTvamDVx2s', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV5jRP-S-lu7"
      },
      "source": [
        "# Summary of Outro:\n",
        "\n",
        "typical experimental strategy in estimation tasks?\n",
        "- We generally assume that mean squared error is what matters.\n",
        "In most studies with reward, we assume that the sum of the reward matters.\n",
        "We rarely test these approaches, and there's a good amount of evidence now\n",
        "that these simple assumptions are not quite right (but convenient).\n",
        "\n",
        "- Now, what have we just learned? Cost functions define optimal behaviors.\n",
        "For normative models, we need cost functions. The shape of the posterior matters,\n",
        "because mean, median, and mode give different optimal estimates.\n",
        "They correspond to the minimization of squared error, absolute error, and 0-1 error respectively.\n",
        "Now, there's a biological principle. If we think about optimal behavior,\n",
        "it's always linked to relevant objectives, and, as we couldn't deal with here,\n",
        "it also actually matters that those cost functions should somehow be ethologically relevant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_CpZGDc_nnt"
      },
      "source": [
        "In this tutorial, we learned about three kinds of cost functions: mean-squared error, absolute error, and zero-one loss. We used expected loss to quantify the results of making a decision, and showed that optimizing under different cost functions led us to choose different locations on the posterior. Finally, we found that these optimal locations can be identified analytically, sparing us from a brute-force search. \n",
        "\n",
        "Here are some additional questions to ponder:\n",
        "*   Suppose your professor offered to grade your work with a zero-one loss or mean square error. \n",
        "    * When might you choose each?\n",
        "    * Which would be easier to learn from?\n",
        "* All of the loss functions we considered are symmetrical. Are there situations where an asymmetrical loss function might make sense? How about a negative one?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvs2RSFB_Opy"
      },
      "source": [
        "*hints*:\n",
        "1. ponder about/ discuss the pros and cons of each of the loss functions\n",
        "2. plot different loss functions/ think about their parameters and trends to understand their symmetry. "
      ]
    }
  ]
}