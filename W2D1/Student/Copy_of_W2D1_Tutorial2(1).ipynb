{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W2D1_Tutorial2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWxYSQ1Sz-DL"
      },
      "source": [
        "# Neuromatch Academy: Week 2, Day 1, Tutorial 2\n",
        "# Causal inference with mixture of Gaussians\n",
        "\n",
        "__Content creators:__ Vincent Valton, Konrad Kording, with help from Matt Krause\n",
        "\n",
        "__Content reviewers:__ Matt Krause, Jesse Livezey, Karolina Stosio, Saeed Salehi, Michael Waskom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCgW9DaHz-DR"
      },
      "source": [
        "# Tutorial Objectives\n",
        "\n",
        "The previous notebook introduced Gaussians and Bayes' rule, allowing us to model very simple combinations of auditory and visual input. In this and the following notebook, we will use those building blocks to explore more complicated sensory integration and ventriloquism! \n",
        "\n",
        "In this notebook, you will:\n",
        "1. Learn more about the problem setting, which we wil also use in Tutorial 3,\n",
        "2. Implement a mixture-of-Gaussian prior, and\n",
        "3. Explore how that prior produces more complex posteriors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yHkVbBbgz-DT"
      },
      "source": [
        "# @title Video 1: Introduction\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luaw19K7s5GJ"
      },
      "source": [
        "# Summary of Video 1:\n",
        "\n",
        "in\n",
        "standing experiments in that field we\n",
        "look at two things that are very\n",
        "compatible - we have a visual stimulus\n",
        "somewhere here in a sound that somewhere\n",
        "close but if we make the difference very\n",
        "large we get into different domain \n",
        "\n",
        "now we will build a Bayesian model for\n",
        "this case and if we make two things\n",
        "very different from one another that\n",
        "they break apart that they're processed\n",
        "independently so we will both be another\n",
        "Bayesian why model. \n",
        "So we again assume\n",
        "that vision is near perfect in sets of a\n",
        "prior belief of where auditory is\n",
        "but we now assume that it's\n",
        "not the case that in all cases we expect\n",
        "the auditor system to be close to the\n",
        "visual one but we assume that there's\n",
        "two implied cases \n",
        "- there's the common\n",
        "case where the assumption is that the\n",
        "visual stimulus must be very close to\n",
        "the auditor one and \n",
        "- then there's an\n",
        "alternative case the independent case\n",
        "where we don't believe\n",
        "two of them to be very close to one\n",
        "another if anything \n",
        "\n",
        "we assume again\n",
        "that the brain has the objective of\n",
        "minimizing estimation errors but what\n",
        "you see is like it's almost the same\n",
        "model it's just we take one part where\n",
        "we did a strong simplifying assumption\n",
        "that the two things always belong\n",
        "together and now we relax that\n",
        "assumption \n",
        "\n",
        "assume that the vision induced prior\n",
        "isn't just a Gaussian but it contains\n",
        "two parts the common cost part which is\n",
        "the gaussian as we had it before which is\n",
        "that believe that the auditory is\n",
        "often should be coming from close to\n",
        "\n",
        "- want to have the mixture of gaussians\n",
        "model where we have one - that describes\n",
        "what happens when they're close to one\n",
        "another (the standard deviation will be small) t\n",
        "- not common case or the independent\n",
        "case where the standard deviation will\n",
        "be very large; model doesn't implicitly\n",
        "assume that there's - that the prior\n",
        "than what that is set up by relation can\n",
        "have two very different cases either\n",
        "belongs together and there's a common\n",
        "cause in there for the priors now\n",
        "otherwise it's white and both of them\n",
        "are possible \n",
        "\n",
        "- we have a\n",
        "parameter here P common that tells us\n",
        "how important one of them is relative to\n",
        "the other \n",
        "\n",
        "We now \n",
        "implement to get the combined\n",
        "prior that has both components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAvjJTcz-DU"
      },
      "source": [
        "---\n",
        "##Setup  \n",
        "Please execute the cells below to initialize the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvzGrB6ez-DV"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7A8VEt94z-DW"
      },
      "source": [
        "#@title Figure Settings\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Pyea-QOaz-DY"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "def my_gaussian(x_points, mu, sigma):\n",
        "    \"\"\"\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Returns normalized Gaussian estimated at points `x_points`, with parameters `mu` and `sigma`\n",
        "\n",
        "    Args:\n",
        "      x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
        "      mu (scalar) - mean of the Gaussian\n",
        "      sigma (scalar) - standard deviation of the gaussian\n",
        "    Returns:\n",
        "      (numpy array of floats): normalized Gaussian (i.e. without constant) evaluated at `x`\n",
        "    \"\"\"\n",
        "    px = np.exp(- 1/2/sigma**2 * (mu - x_points) ** 2)\n",
        "\n",
        "    px = px / px.sum() # this is the normalization part with a very strong assumption, that\n",
        "                       # x_points cover the big portion of probability mass around the mean.\n",
        "                       # Please think/discuss when this would be a dangerous assumption.\n",
        "\n",
        "    return px\n",
        "\n",
        "def plot_mixture_prior(x, gaussian1, gaussian2, combined):\n",
        "    \"\"\"\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Plots a prior made of a mixture of gaussians\n",
        "\n",
        "    Args:\n",
        "      x (numpy array of floats):         points at which the likelihood has been evaluated\n",
        "      gaussian1 (numpy array of floats): normalized probabilities for Gaussian 1 evaluated at each `x`\n",
        "      gaussian2 (numpy array of floats): normalized probabilities for Gaussian 2 evaluated at each `x`\n",
        "      posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, gaussian1, '--b', LineWidth=2, label='Gaussian 1')\n",
        "    ax.plot(x, gaussian2, '-.b', LineWidth=2, label='Gaussian 2')\n",
        "    ax.plot(x, combined, '-r', LineWidth=2, label='Gaussian Mixture')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-pB0e1ixR8x"
      },
      "source": [
        "# Discussion of helper functions:\n",
        "\n",
        "*my_gaussian*:\n",
        "it returns normalized Gaussian estimated at points `x_points`, with parameters `mu` and `sigma`\n",
        "we make very strong assumption, that x_points cover the big portion of probability mass around the mean.\n",
        "When is this a dangerous assumption to make?\n",
        "In the independent case here or when the x_points are too scattered and not concentrated around the mean!\n",
        "\n",
        "*plot_mixture_prior*:\n",
        "Plots a prior made of a mixture of gaussians using:\n",
        "points at which the likelihood has been evaluated, normalized probabilities for Gaussian 1 evaluated at each `x`, normalized probabilities for Gaussian 2 evaluated at each `x`. normalized probabilities for the posterior evaluated at each `x`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhWwmVT8z-DY"
      },
      "source": [
        "# Section 1: Motivating example\n",
        "\n",
        "Ventriloquists produce the illusion that their puppets are talking because:\n",
        "1. We observe the visual input of the puppet moving its mouth, as if speaking.\n",
        "2. The speech that the puppeteer generates originates near the puppet's mouth. \n",
        "\n",
        "Since we are accustomed to voices coming from moving mouths, we tend to interpret the voice as coming directly from the puppet itself rather than from the puppeteer (who is also hiding his/her own mouth movements). In the remaining tutorials, we will study how this illusion breaks down as the distance between the visual stimulus (the puppet's mouth) and the auditory stimulus (the puppeteer's concealed speech) changes. \n",
        "\n",
        "Imagine an experiment where participants are shown a puppet moving its mouth at a location directly in front of them (at position 0˚). The subjects are told that 75% of the time, the voice they hear originates from the puppet. On the remaining 25% of trials, sounds come from elsewhere. Participants learn this over multiple trials, after which a curtain is dropped in front of the puppeteer and the puppet. \n",
        "\n",
        "Next, we present only the auditory stimulus at varying locations and we ask participants to report where the source of the sound is located. The participants have access to two pieces of information:\n",
        "\n",
        "*   The prior information about sound localization, learned during the trials before the curtain fell.\n",
        "*   Their noisy sensory estimates about where a particular sound originates. \n",
        "\n",
        "Our eventual goal, which we achieve in Tutorial 3, is to predict the subjects' responses: when do subjects ascribe a sound to the puppet, and when do they believe it originated elsewhere? Doing so requires building a prior that captures the participant's knowledge and expectations, which we wil do in the exercises that follow here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i-eDhUoyyI5"
      },
      "source": [
        "# Section 2: Mixture-of-Gaussians Prior\n",
        "\n",
        "In the previous tutorial, you learned how to create a single Gaussian prior that could represent one of these possibilities. A broad Gaussian with a large $\\sigma$ could represent sounds originating from nearly anywhere, while a narrow Gaussian with $\\mu$ near zero could represent sounds originating from the puppet. \n",
        "\n",
        "Here, we will combine those into a mixture-of-Gaussians probability density function (PDF) that captures both possibilities. We will control how the Gaussians are mixed by summing them together with a 'mixing' or weight parameter $p_{common}$, set to a value between 0 and 1, like so:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    \\text{Mixture} = \\bigl[\\; p_{common} \\times \\mathcal{N}(\\mu_{common},\\sigma_{common}) \\; \\bigr] + \\bigl[ \\;\\underbrace{(1-p_{common})}_{p_{independent}} \\times \\mathcal{N}(\\mu_{independent},\\sigma_{independent}) \\; \\bigr]\n",
        "\\end{eqnarray}\n",
        "\n",
        "$p_{common}$ denotes the probability that auditory stimulus shares a \"common\" source with the learnt visual input; in other words, the probability that the \"puppet\" is speaking. You might think that we need to include a separate weight for the possibility that sound is \"independent\" from the puppet. However, since there are only two, mutually-exclusive possibilities, we can replace $p_{independent}$ with $(1 - p_{common})$ since, by the law of total probability, $p_{common} + p_{independent}$ must equal one. \n",
        "\n",
        "Using the formula above, complete the code to build this mixture-of-Gaussians PDF: \n",
        "* Generate a Gaussian with mean 0 and standard deviation 0.5 to be the 'common' part of the Gaussian mixture prior. (This is already done for you below).\n",
        "* Generate another Gaussian with mean 0 and standard deviation 3 to serve as the 'independent' part. \n",
        "* Combine the two Gaussians to make a new prior by mixing the two Gaussians with mixing parameter $p_{common}$ = 0.75 so that the peakier \"common-cause\" Gaussian has 75% of the weight. Don't forget to normalize afterwards! \n",
        "\n",
        "Hints:\n",
        "* Code for the `my_gaussian` function from Tutorial 1 is available for you to use. Its documentation is below. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrtXxuSSz-Db"
      },
      "source": [
        "**Helper function(s)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grofj-UVz-Dc"
      },
      "source": [
        "help(my_gaussian)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmtADOLLz-De"
      },
      "source": [
        "## Exercise 1: Implement the prior "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "MYKE6i4Nz-De"
      },
      "source": [
        "def mixture_prior(x, mean=0, sigma_common=0.5, sigma_independent=3, p_common=0.75):\n",
        "\n",
        "  ###############################################################################\n",
        "  ## Insert your code here to:\n",
        "  #   * Create a second gaussian representing the independent-cause component\n",
        "  #   * Combine the two priors, using the mixing weight p_common. Don't forget\n",
        "  #      to normalize the result so it remains a proper probability density function\n",
        "  #\n",
        "  #   * Comment the line below to test out your function\n",
        "  raise NotImplementedError(\"Please complete Exercise 1\")\n",
        "  ###############################################################################\n",
        "\n",
        "  gaussian_common = my_gaussian(x, mean, sigma_common)\n",
        "  gaussian_independent = ...\n",
        "  mixture = ...\n",
        "\n",
        "  return gaussian_common, gaussian_independent, mixture\n",
        "\n",
        "\n",
        "x = np.arange(-10, 11, 0.1)\n",
        "\n",
        "# Uncomment the lines below to visualize out your solution\n",
        "# common, independent, mixture = mixture_prior(x)\n",
        "# plot_mixture_prior(x, common, independent, mixture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "371c906d-ddd0-42f4-bd7d-6170c8818f94",
        "id": "BTsTxG6mz-De"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial2_Solution_2c2af4a2.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial2_Solution_2c2af4a2_1.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjLgXGiNz-Df"
      },
      "source": [
        "\n",
        "# Section 3: Bayes Theorem with Complex Posteriors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DFhV2jqtz-Df"
      },
      "source": [
        "#@title Video 2: Mixture-of-Gaussians and Bayes' Theorem\n",
        "video = YouTubeVideo(id='LWKM35te0WI', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXYMyPl-yK4D"
      },
      "source": [
        "# Discussion of Video 2:\n",
        "\n",
        "- briefly want to emphasize that this is a case of marginalization. \n",
        "but the priors, some of the common and the not common model's predictions, and by adding them together\n",
        "we are effectively considering both possible values of an unknown variable.\n",
        "\n",
        "Now,\n",
        "given that we have the prior which now is just slightly different not like it's just the sum of two gaussians,\n",
        "we can calculate the posterior\n",
        "using Bayes rule. And this shows you the strength of the Bayesian approach,\n",
        "we have a completely different prior and yet we can use exactly the same\n",
        "mechanisms, now like the posterior again is the prior which now is different, times the likelihood and normalize so that it adds up to one.\n",
        "\n",
        "So, we now want to multiply Gaussians with a mixture of Gaussians,\n",
        "which just like before is proportional to the prior times the likelihood.\n",
        "\n",
        "Now, calculate the posterior of this case for the mixture of gaussians or in other words\n",
        "causal inference model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZMDt85mz-Df"
      },
      "source": [
        "Now that we have created a mixture of Gaussians prior that embodies the participants' expectations about sound location, we want to compute the posterior probability, which represents the subjects' beliefs about a specific sound's origin. \n",
        "\n",
        "To do so we will compute the posterior by using *Bayes Theorem* to combine the mixture-of-gaussians prior and varying auditory Gaussian likelihood. This works exactly the same as in Tutorial 1: we simply multiply the prior and likelihood pointwise, then normalize the resulting distribution so it sums to 1. (The closed-form solution from Exercise 2B, however, no longer applies to this more complicated prior). \n",
        "\n",
        "Here, we provide you with the code mentioned in the video (lucky!). Instead, use the interactive demo to explore how a mixture-of-Gaussians prior and Gaussian likelihood interact. For simplicity, we have fixed the prior mean to be zero. We also recommend starting with same other prior parameters used in Exercise 1: $\\sigma_{common} = 0.5, \\sigma_{independent} = 3, p_{common}=0.75$; vary the likelihood instead. \n",
        "\n",
        "Unlike the demo in Tutorial 1, you should see several qualitatively different effects on the posterior, depending on the relative position and width of likelihood. Pay special attention to both the overall shape of the posterior and the location of the peak. What do you see?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWWZVn1Fz-Dg"
      },
      "source": [
        "## Interactive Demo 1: Mixture-of-Gaussian prior and the posterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "kdpIU0aNz-Dg"
      },
      "source": [
        "#@title\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "fig_domain = np.arange(-10, 11, 0.1)\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def refresh(sigma_common=0.5, sigma_independent=3, p_common=0.75, mu_auditory=3, sigma_auditory=1.5):\n",
        "    _, _, prior = mixture_prior(fig_domain, 0, sigma_common, sigma_independent, p_common)\n",
        "    likelihood = my_gaussian(fig_domain, mu_auditory, sigma_auditory)\n",
        "\n",
        "    posterior = prior * likelihood\n",
        "    posterior /= posterior.sum()\n",
        "\n",
        "    plt.plot(fig_domain, prior, label=\"Mixture Prior\")\n",
        "    plt.plot(fig_domain, likelihood, label=\"Likelihood\")\n",
        "    plt.plot(fig_domain, posterior, label=\"Posterior\")\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "_ = widgets.interact(refresh,\n",
        "    sigma_common=widgets.FloatSlider(value=0.5, min=0.01, max=10, step=0.5, description=\"sigma_common\", style=style),\n",
        "    sigma_independent=widgets.FloatSlider(value=3, min=0.01, max=10, step=0.5, description=\"sigma_independent:\", style=style),\n",
        "    p_common=widgets.FloatSlider(value=0.75, min=0, max=1, steps=0.01, description=\"p_common\"),\n",
        "    mu_auditory=widgets.FloatSlider(value=2, min=-10, max=10, step=0.1, description=\"mu_auditory:\", style=style),\n",
        "    sigma_auditory=widgets.FloatSlider(value=0.5, min=0.01, max=10, step=0.5, description=\"sigma_auditory:\", style=style),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "17a71ae3-6fd7-47ba-ce59-34e72ad104b2",
        "id": "G8PCDJsDz-Dg"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial2_Solution_4d414185.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAW3IXXKz-Dg"
      },
      "source": [
        "# Section 3: Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6bOL751Mz-Dg"
      },
      "source": [
        "#@title Video 3: Outro\n",
        "video = YouTubeVideo(id='UgeAtE8xZT8', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3tAcLJkza3Z"
      },
      "source": [
        "# Summary of Video 3:\n",
        "\n",
        "There\n",
        "are now two different modes here if\n",
        "the\n",
        "auditory stimulus and the visual stimulus are really quite close to one another, then we have a region where we have linear\n",
        "integration, where the visual stimulus has a very strong influence on the auditory stimulus.\n",
        "\n",
        "And then, that's the other mode. If we are far away,\n",
        "it has very little influence and then there's a relatively steep\n",
        "transition between these two modes, and this is what's meant with causal inference.\n",
        "\n",
        "vary the disagreement between cues - measured how one \n",
        "influences the other and then they test the\n",
        "hypothesis that the impact is large if they're close to one another and small if they're far away.\n",
        "And again, we find this in many domains vision and audition, price and likelihood ,all kinds of cases in cognitive science.\n",
        "\n",
        "The Bayesian framework allows us to deal with arbitrary probability\n",
        "distributions.\n",
        "Gaussians give rise to linear combinations.\n",
        "Mixture of Gaussians give rise to cause the inference like effects where we have an\n",
        "interaction that breaks down as soon as the difference is too large. Behavior, then becomes very nonlinear. And\n",
        "there's a biological principle integration tends to be linear for small difference and\n",
        "tends to break down for large differences\n",
        "between cues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ZbmSHaz-Dg"
      },
      "source": [
        "In this tutorial, we introduced the ventriloquism setting that will form the basis of Tutorials 3 and 4 as well. We built a mixture-of-Gaussians prior that captures the participants' subjective experiences. In the next tutorials, we will use these to perform causal inference and predict the subject's responses to individual stimuli. "
      ]
    }
  ]
}