{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of W3D4_Tutorial1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJLwUhKX5X1K"
      },
      "source": [
        "# Neuromatch Academy: Week 3, Day 4, Tutorial 1\n",
        "# Deep Learning: Decoding Neural Responses\n",
        "\n",
        "**Content creators**: Jorge A. Menendez, Carsen Stringer\n",
        "\n",
        "**Content reviewers**: Roozbeh Farhoodi,  Madineh Sarvestani, Kshitij Dwivedi, Spiros Chavlis, Ella Batty, Michael Waskom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kw22Z0Q5X1T"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "In this tutorial, we'll use deep learning to decode stimulus information from the responses of sensory neurons. Specifically, we'll look at the activity of ~20,000 neurons in mouse primary visual cortex responding to oriented gratings recorded in [this study](https://www.biorxiv.org/content/10.1101/679324v2.abstract). Our task will be to decode the orientation of the presented stimulus from the responses of the whole population of neurons. We could do this in a number of ways, but here we'll use deep learning. Deep learning is particularly well-suited to this problem for a number of reasons:\n",
        "* The data are very high-dimensional: the neural response to a stimulus is a ~20,000 dimensional vector. Many machine learning techniques fail in such high dimensions, but deep learning actually thrives in this regime, as long as you have enough data (which we do here!).\n",
        "* As you'll be able to see below, different neurons can respond quite differently to stimuli. This complex pattern of responses will, therefore, require non-linear methods to be decoded, which we can easily do with non-linear activation functions in deep networks.\n",
        "* Deep learning architectures are highly flexible, meaning we can easily adapt the architecture of our decoding model to optimize decoding. Here, we'll focus on a single architecture, but you'll see that it can easily be modified with few changes to the code.\n",
        "\n",
        "More concretely, our goal will be learn how to:\n",
        "* Build a deep feed-forward network using PyTorch\n",
        "* Evaluate the network's outputs using PyTorch built-in loss functions\n",
        "* Compute gradients of the loss with respect to each parameter of the network using automatic differentiation\n",
        "* Implement gradient descent to optimize the network's parameters\n",
        "\n",
        "This tutorial will take up the first full session (equivalent to two tutorials on other days)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Q1dYIz_25X1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "ece7f548-e061-4dd5-ce96-7ed205f03256"
      },
      "source": [
        "#@title Video 1: Decoding from neural data using feed-forward networks in pytorch\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"SlrbMvvBOzM\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/SlrbMvvBOzM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/SlrbMvvBOzM?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fa7308e1908>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadpG-x3GaRP"
      },
      "source": [
        "#Summary of Video 1:\n",
        "\n",
        "- Decoding models \n",
        "are used to predict external correlates from neural activity.\n",
        "In this case you predicted the choice of a mouse in a\n",
        "decision-making task using neural activity from various brain areas by doing this you can determine how much\n",
        "information a brain area has about a given external correlate.\n",
        "- Encoding models on the other hand are used to predict neural activity from external correlates.\n",
        "You build a model such as this \n",
        "to predict retinal neural activity from image pixels.\n",
        "This type of model can help you understand how a brain area represents different external correlates such as visual stimuli.\n",
        "In both of these cases you used linear models.\n",
        "\n",
        "- use deep neural networks to build decoding and encoding models due to their ability to fit\n",
        "non-linear functions and their ability to be fit easily.\n",
        "\n",
        "- So we will be exploring neural activity in mice,\n",
        "while the mice are viewing oriented grading stimuli on a screen in front of it.\n",
        "We record this neural activity using a technique called two photon calcium imaging\n",
        "which allows us to record many thousands of neurons simultaneously.\n",
        "These neurons light up when they fire.\n",
        "We then convert this imaging data to a matrix of neural responses by stimuli presented.\n",
        "(bin the neural responses and compute the neuron's tuning curve.)\n",
        "We used bins of one degree.\n",
        "We will use the response of all neurons in a single bin\n",
        "in order to predict which stimulus was shown.\n",
        "\n",
        "- So we're going to be using the responses of 24,000 neurons\n",
        "to try to predict\n",
        "360 possible stimulus conditions corresponding to each of the degrees of these different orientations.\n",
        "So to do this\n",
        "we're going to build a deep neural network to predict stimuli from neural responses.\n",
        "Because we're now in a regime of having large amounts of data.\n",
        "\n",
        "- Let's start with a linear network with no hidden layers\n",
        "where the stimulus prediction y\n",
        "is a product of the weights w_out and neural responses r plus a bias term.\n",
        "When you fit a linear model such as this\n",
        "you likely minimized the squared error between the stimulus prediction and the true stimulus.\n",
        "This is called the loss function.\n",
        "The solution to minimizing this loss function in a linear model can be found in closed form\n",
        "\n",
        "- So if we use a simple linear model such as this\n",
        "we are able to predict the stimulus within two to three degrees.\n",
        "So, let's see if we can improve this prediction by using a deep neural network.\n",
        "\n",
        "- So let's start by adding a single hidden layer\n",
        "with m units to this linear model, where now the output y depends on the activity of these hidden units h\n",
        "and the hidden layer h - its activations depend on the neural responses r.\n",
        "This linear network with one hidden layer where m is less than n, where n is the number of neurons\n",
        "is equivalent to performing reduced rank regression, which is a technique that is actually useful for regularizing your regression model.\n",
        "Adding this hidden layer means that the network now has a depth of 1\n",
        "and a width of m.\n",
        "Increasing the depth and the width of this network can increase the expressivity of the model.\n",
        "In other words, how well it can fit complex non-linear functions.\n",
        "\n",
        "- so working with a model with a depth of one, and see if we can improve our prediction of the stimulus.\n",
        "\n",
        "Structure of code:\n",
        "1. import the PyTorch module\n",
        "2. we import the neural network module nn.\n",
        "3. create a class for this deep network called DeepNet.\n",
        "(method's first argument is self, which is the class itself).\n",
        "4. initialize with this init method - takes two inputs n_inputs ( number of neurons ) and n_hidden (number of hidden units).\n",
        "5. So we first call this super function\n",
        "in order to invoke the nn module, then we add our first hidden layer, which is going to be this in_layer here with nn.linear unit\n",
        "which is n_inputs by n_hidden.\n",
        "And then we're only adding one hidden layer here. \n",
        "6. So now let's add the output layer\n",
        "which is going to be nn.linear, which is n_hidden by 1 because we're only predicting 1 stimulus here\n",
        "which is the orientation here.\n",
        "7. add another method to this class called the forward method\n",
        "and this is the method that runs when you call this class as a function.\n",
        "It takes as input r which are our neural responses here,\n",
        "and they're passed through this first\n",
        "hidden layer to produce our activations h\n",
        "and then they're passed through this output layer to produce our final prediction of the stimulus y.\n",
        "8. And now that we've declared our class, let's create an instantiation of this class called net.\n",
        "And we're going to give it the number of inputs we have which is the number of neurons\n",
        "and then also the number of hidden units, which is 200.\n",
        "\n",
        "Now let's run our neural responses through this net\n",
        "So we're going to take our neural response to a single stimulus, istim = 0 in this case,\n",
        "and we're going to run the network\n",
        "run the response through the network using this call of net as a function here,\n",
        "and this call calls the forward pass.\n",
        "\n",
        "So every deep network that you build with PyTorch you need to make sure you have this init method and this forward method\n",
        "and then you can use it in this way in order to run your data through the network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxsi4Wjk5X1X"
      },
      "source": [
        "---\n",
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Hs85ATgq5X1X"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QYEa2iDZ5X1Z"
      },
      "source": [
        "#@title Data retrieval and loading\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "fname = \"W3D4_stringer_oribinned1.npz\"\n",
        "url = \"https://osf.io/683xc/download\"\n",
        "expected_md5 = \"436599dfd8ebe6019f066c38aed20580\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
        "      print(\"!!! Data download appears corrupted !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "WccvXU2G5X1a"
      },
      "source": [
        "#@title Figure Settings\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmv7-RlR5X1a"
      },
      "source": [
        "#@title Helper Functions\n",
        "\n",
        "def load_data(data_name=fname, bin_width=1):\n",
        "  \"\"\"Load mouse V1 data from Stringer et al. (2019)\n",
        "\n",
        "  Data from study reported in this preprint:\n",
        "  https://www.biorxiv.org/content/10.1101/679324v2.abstract\n",
        "\n",
        "  These data comprise time-averaged responses of ~20,000 neurons\n",
        "  to ~4,000 stimulus gratings of different orientations, recorded\n",
        "  through Calcium imaging. The responses have been normalized by\n",
        "  spontaneous levels of activity and then z-scored over stimuli, so\n",
        "  expect negative numbers. They have also been binned and averaged\n",
        "  to each degree of orientation.\n",
        "\n",
        "  This function returns the relevant data (neural responses and\n",
        "  stimulus orientations) in a torch.Tensor of data type torch.float32\n",
        "  in order to match the default data type for nn.Parameters in\n",
        "  Google Colab.\n",
        "\n",
        "  This function will actually average responses to stimuli with orientations\n",
        "  falling within bins specified by the bin_width argument. This helps\n",
        "  produce individual neural \"responses\" with smoother and more\n",
        "  interpretable tuning curves.\n",
        "\n",
        "  Args:\n",
        "    bin_width (float): size of stimulus bins over which to average neural\n",
        "      responses\n",
        "\n",
        "  Returns:\n",
        "    resp (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,\n",
        "        each row contains the responses of each neuron to a given stimulus.\n",
        "        As mentioned above, neural \"response\" is actually an average over\n",
        "        responses to stimuli with similar angles falling within specified bins.\n",
        "    stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation\n",
        "        of each stimulus, in degrees. This is actually the mean orientation\n",
        "        of all stimuli in each bin.\n",
        "\n",
        "  \"\"\"\n",
        "  with np.load(data_name) as dobj:\n",
        "    data = dict(**dobj)\n",
        "  resp = data['resp']\n",
        "  stimuli = data['stimuli']\n",
        "\n",
        "  if bin_width > 1:\n",
        "    # Bin neural responses and stimuli\n",
        "    bins = np.digitize(stimuli, np.arange(0, 360 + bin_width, bin_width))\n",
        "    stimuli_binned = np.array([stimuli[bins == i].mean() for i in np.unique(bins)])\n",
        "    resp_binned = np.array([resp[bins == i, :].mean(0) for i in np.unique(bins)])\n",
        "  else:\n",
        "    resp_binned = resp\n",
        "    stimuli_binned = stimuli\n",
        "\n",
        "  # Return as torch.Tensor\n",
        "  resp_tensor = torch.tensor(resp_binned, dtype=torch.float32)\n",
        "  stimuli_tensor = torch.tensor(stimuli_binned, dtype=torch.float32).unsqueeze(1)  # add singleton dimension to make a column vector\n",
        "\n",
        "  return resp_tensor, stimuli_tensor\n",
        "\n",
        "\n",
        "def plot_data_matrix(X, ax):\n",
        "  \"\"\"Visualize data matrix of neural responses using a heatmap\n",
        "\n",
        "  Args:\n",
        "    X (torch.Tensor or np.ndarray): matrix of neural responses to visualize\n",
        "        with a heatmap\n",
        "    ax (matplotlib axes): where to plot\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  cax = ax.imshow(X, cmap=mpl.cm.pink, vmin=np.percentile(X, 1), vmax=np.percentile(X, 99))\n",
        "  cbar = plt.colorbar(cax, ax=ax, label='normalized neural response')\n",
        "\n",
        "  ax.set_aspect('auto')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "\n",
        "def identityLine():\n",
        "  \"\"\"\n",
        "  Plot the identity line y=x\n",
        "  \"\"\"\n",
        "  ax = plt.gca()\n",
        "  lims = np.array([ax.get_xlim(), ax.get_ylim()])\n",
        "  minval = lims[:, 0].min()\n",
        "  maxval = lims[:, 1].max()\n",
        "  equal_lims = [minval, maxval]\n",
        "  ax.set_xlim(equal_lims)\n",
        "  ax.set_ylim(equal_lims)\n",
        "  line = ax.plot([minval, maxval], [minval, maxval], color=\"0.7\")\n",
        "  line[0].set_zorder(-1)\n",
        "\n",
        "def get_data(n_stim, train_data, train_labels):\n",
        "  \"\"\" Return n_stim randomly drawn stimuli/resp pairs\n",
        "\n",
        "  Args:\n",
        "    n_stim (scalar): number of stimuli to draw\n",
        "    resp (torch.Tensor):\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "\n",
        "  Returns:\n",
        "    (torch.Tensor, torch.Tensor): n_stim x n_neurons tensor of neural responses and n_stim x 1 of orientations respectively\n",
        "  \"\"\"\n",
        "  n_stimuli = train_labels.shape[0]\n",
        "  istim = np.random.choice(n_stimuli, n_stim)\n",
        "  r = train_data[istim]  # neural responses to this stimulus\n",
        "  ori = train_labels[istim]  # true stimulus orientation\n",
        "\n",
        "  return r, ori\n",
        "\n",
        "def stimulus_class(ori, n_classes):\n",
        "  \"\"\"Get stimulus class from stimulus orientation\n",
        "\n",
        "  Args:\n",
        "    ori (torch.Tensor): orientations of stimuli to return classes for\n",
        "    n_classes (int): total number of classes\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: 1D tensor with the classes for each stimulus\n",
        "\n",
        "  \"\"\"\n",
        "  bins = np.linspace(0, 360, n_classes + 1)\n",
        "  return torch.tensor(np.digitize(ori.squeeze(), bins)) - 1  # minus 1 to accomodate Python indexing\n",
        "\n",
        "def plot_decoded_results(train_loss, test_labels, predicted_test_labels):\n",
        "  \"\"\" Plot decoding results in the form of network training loss and test predictions\n",
        "\n",
        "  Args:\n",
        "    train_loss (list): training error over iterations\n",
        "    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    predicted_test_labels (torch.Tensor): n_test x 1 tensor with predicted orientations of the\n",
        "      stimuli from decoding neural network\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Plot results\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "  # Plot the training loss over iterations of GD\n",
        "  ax1.plot(train_loss)\n",
        "\n",
        "  # Plot true stimulus orientation vs. predicted class\n",
        "  ax2.plot(stimuli_test.squeeze(), predicted_test_labels, '.')\n",
        "\n",
        "  ax1.set_xlim([0, None])\n",
        "  ax1.set_ylim([0, None])\n",
        "  ax1.set_xlabel('iterations of gradient descent')\n",
        "  ax1.set_ylabel('negative log likelihood')\n",
        "  ax2.set_xlabel('true stimulus orientation ($^o$)')\n",
        "  ax2.set_ylabel('decoded orientation bin')\n",
        "  ax2.set_xticks(np.linspace(0, 360, n_classes + 1))\n",
        "  ax2.set_yticks(np.arange(n_classes))\n",
        "  class_bins = [f'{i * 360 / n_classes: .0f}$^o$ - {(i + 1) * 360 / n_classes: .0f}$^o$' for i in range(n_classes)]\n",
        "  ax2.set_yticklabels(class_bins);\n",
        "\n",
        "  # Draw bin edges as vertical lines\n",
        "  ax2.set_ylim(ax2.get_ylim())  # fix y-axis limits\n",
        "  for i in range(n_classes):\n",
        "    lower = i * 360 / n_classes\n",
        "    upper = (i + 1) * 360 / n_classes\n",
        "    ax2.plot([lower, lower], ax2.get_ylim(), '-', color=\"0.7\", linewidth=1, zorder=-1)\n",
        "    ax2.plot([upper, upper], ax2.get_ylim(), '-', color=\"0.7\", linewidth=1, zorder=-1)\n",
        "\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vEwb9eBB83P"
      },
      "source": [
        "# Summary of Helper functions:\n",
        "\n",
        "*load_data*: Load mouse V1 data from Stringer et al. (2019)\n",
        "\n",
        "\n",
        "    These data comprise time-averaged responses of ~20,000 neurons\n",
        "    to ~4,000 stimulus gratings of different orientations, recorded\n",
        "    through Calcium imaginge. The responses have been normalized by\n",
        "    spontanous levels of activity and then z-scored over stimuli, so\n",
        "    expect negative numbers. They have also been binned and averaged\n",
        "    to each degree of orientation.\n",
        "\n",
        "    This function returns the relevant data (neural responses and\n",
        "    stimulus orientations) in a torch.Tensor of data type torch.float32\n",
        "    in order to match the default data type for nn.Parameters in\n",
        "    Google Colab.\n",
        "\n",
        "    This function will actually average responses to stimuli with orientations\n",
        "    falling within bins specified by the bin_width argument. This helps\n",
        "    produce individual neural \"responses\" with smoother and more\n",
        "    interpretable tuning curves.\n",
        "\n",
        "\n",
        "\n",
        "*plot_data_matrix(X, ax)*: Visualize data matrix of normalised neural responses using a heatmap\n",
        "\n",
        "*identityLine()*: Plot the identity line y=x\n",
        "\n",
        "*get_data(n_stim, train_data, train_labels)*: Return n_stim randomly drawn stimuli/resp pairs\n",
        "\n",
        "*stimulus_class(ori, n_classes)*: Get stimulus class from stimulus orientation\n",
        "\n",
        "*plot_decoded_results(train_loss, test_labels, predicted_test_labels*: Plot decoding results in the form of network training loss and test predictions; \n",
        "- true stimulus orientation vs. predicted class:\n",
        "  - iterations of gradient descent (xaxis) vs negative log likelihood (yaxis)\n",
        "  - true stimulus orientation (xaxis) vs decoded orientation bin (yaxis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEV0gIci5X1b"
      },
      "source": [
        "---\n",
        "# Section 1: Load and visualize data\n",
        "\n",
        "In the next cell, we have provided code to load the data and plot the matrix of neural responses.\n",
        "\n",
        "Next to it, we plot the tuning curves of three randomly selected neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "65pHuTNr5X1c"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Execute this cell to load and visualize data\n",
        "\n",
        "# Load data\n",
        "resp_all, stimuli_all = load_data()  # argument to this function specifies bin width\n",
        "n_stimuli, n_neurons = resp_all.shape\n",
        "\n",
        "print(f'{n_neurons} neurons in response to {n_stimuli} stimuli')\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(2 * 6, 5))\n",
        "\n",
        "# Visualize data matrix\n",
        "plot_data_matrix(resp_all[:100, :].T, ax1)  # plot responses of first 100 neurons\n",
        "ax1.set_xlabel('stimulus')\n",
        "ax1.set_ylabel('neuron')\n",
        "\n",
        "# Plot tuning curves of three random neurons\n",
        "ineurons = np.random.choice(n_neurons, 3, replace=False)  # pick three random neurons\n",
        "ax2.plot(stimuli_all, resp_all[:, ineurons])\n",
        "ax2.set_xlabel('stimulus orientation ($^o$)')\n",
        "ax2.set_ylabel('neural response')\n",
        "ax2.set_xticks(np.linspace(0, 360, 5))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXXM0sCg5X1d"
      },
      "source": [
        "We will split our data into a training set and test set. In particular, we will have a training set of orientations (`stimuli_train`) and the corresponding responses (`resp_train`). Our testing set will have held-out orientations (`stimuli_test`) and the corresponding responses (`resp_test`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "z678Dir-5X1d"
      },
      "source": [
        "#@title\n",
        "#@markdown Execute this cell to split into training and test sets\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(4)\n",
        "torch.manual_seed(4)\n",
        "\n",
        "# Split data into training set and testing set\n",
        "n_train = int(0.6 * n_stimuli)  # use 60% of all data for training set\n",
        "ishuffle = torch.randperm(n_stimuli)\n",
        "itrain = ishuffle[:n_train]  # indices of data samples to include in training set\n",
        "itest = ishuffle[n_train:]  # indices of data samples to include in testing set\n",
        "stimuli_test = stimuli_all[itest]\n",
        "resp_test = resp_all[itest]\n",
        "stimuli_train = stimuli_all[itrain]\n",
        "resp_train = resp_all[itrain]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_1aQnlR5X1e"
      },
      "source": [
        "---\n",
        "# Section 2: Deep feed-forward networks in *pytorch* \n",
        "\n",
        "We'll now build a simple deep neural network that takes as input a vector of neural responses and outputs a single number representing the decoded stimulus orientation.\n",
        "\n",
        "To keep things simple, we'll build a deep network with **one** hidden layer. See the appendix for a deeper discussion of what this choice entails, and when one might want to use deeper/shallower and wider/narrower architectures.\n",
        "\n",
        "Let $\\mathbf{r}^{(n)} = \\begin{bmatrix} r_1^{(n)} & r_2^{(n)} & \\ldots & r_N^{(n)} \\end{bmatrix}^T$ denote the vector of neural responses (of neurons $1, \\ldots, N$) to the $n$th stimulus. The network we will use is described by the following set of equations:\n",
        "\\begin{align}\n",
        "    \\mathbf{h}^{(n)} &= \\mathbf{W}^{in} \\mathbf{r}^{(n)} + \\mathbf{b}^{in}, && [\\mathbf{W}^{in}: M \\times N], \\\\\n",
        "    y^{(n)} &= \\mathbf{W}^{out} \\mathbf{h}^{(n)} + \\mathbf{b}^{out},  && [\\mathbf{W}^{out}: 1 \\times M],\n",
        "\\end{align}\n",
        "where $y^{(n)}$ denotes the scalar output of the network: the decoded orientation of the $n$th stimulus. \n",
        "\n",
        "The $M$-dimensional vector $\\mathbf{h}^{(n)}$ denotes the activations of the **hidden layer** of the network. \n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/static/one-layer-network.png?raw=true\" width=\"450\" />\n",
        "</p>\n",
        "\n",
        "The blue components of this diagram denote the **parameters** of the network, which we will later optimize with gradient descent. These include all the weights and biases $\\mathbf{W}^{in}, \\mathbf{b}^{in}, \\mathbf{W}^{out}, \\mathbf{b}^{out}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyI84KG55X1e"
      },
      "source": [
        "### Section 2.1: Introduction to PyTorch\n",
        "\n",
        "Here, we'll use the **PyTorch** package to build, run, and train deep networks of this form in Python. There are two core components to the PyTorch package: \n",
        "\n",
        "1. The first is the `torch.Tensor` data type used in PyTorch. `torch.Tensor`'s are effectively just like a `numpy` arrays, except that they have some important attributes and methods needed for automatic differentiation (to be discussed below). They also come along with infrastructure for easily storing and computing with them on GPU's, a capability we won't touch on here but which can be really useful in practice.\n",
        "\n",
        "2. The second core ingredient is the PyTorch `nn.Module` class. This is the class we'll use for constructing deep networks, so that we can then easily train them using built-in PyTorch functions. Keep in my mind that `nn.Module` classes can actually be used to build, run, and train any model -- not just deep networks!\n",
        "\n",
        "  The next cell contains code for building the deep network we defined above using the `nn.Module` class. It contains three key ingredients:\n",
        "\n",
        "  * `__init__()` method to initialize its parameters, like in any other Python class. In this case, it takes two arguments:\n",
        "    * `n_inputs`: the number of input units. This should always be set to the number of neurons whose activities are being decoded (i.e. the dimensionality of the input to the network). \n",
        "    * `n_hidden`: the number of hidden units. This is a parameter that we are free to vary in deciding how to build our network. See the appendix for a discussion of how this architectural choice affects the computations the network can perform.\n",
        "\n",
        "  * `nn.Linear` modules, which are built-in PyTorch classes containing all the weights and biases for a given network layer (documentation [here](https://pytorch.org/docs/master/generated/torch.nn.Linear.html)). This class takes two arguments to initialize:\n",
        "    * \\# of inputs to that layer\n",
        "    * \\# of outputs from that layer\n",
        "\n",
        "    For the input layer, for example, we have:\n",
        "    * \\# of inputs = \\# of neurons whose responses are to be decoded ($N$, specified by `n_inputs`)\n",
        "    * \\# of outputs = \\# of hidden layer units ($M$, specified by `n_hidden`)\n",
        "    \n",
        "    PyTorch will initialize all weights and biases randomly.\n",
        "\n",
        "  * `forward()` method, which takes as argument an input to the network and returns the network output. In our case, this comprises computing the output $y$ from a given input $\\mathbf{r}$ using the above two equations. See the next cell for code implementing this computation using the built-in PyTorch `nn.Linear` classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "MpucL2bM5X1f"
      },
      "source": [
        "class DeepNet(nn.Module):\n",
        "  \"\"\"Deep Network with one hidden layer\n",
        "\n",
        "  Args:\n",
        "    n_inputs (int): number of input units\n",
        "    n_hidden (int): number of units in hidden layer\n",
        "\n",
        "  Attributes:\n",
        "    in_layer (nn.Linear): weights and biases of input layer\n",
        "    out_layer (nn.Linear): weights and biases of output layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n",
        "\n",
        "  def forward(self, r):\n",
        "    \"\"\"Decode stimulus orientation from neural responses\n",
        "\n",
        "    Args:\n",
        "      r (torch.Tensor): vector of neural responses to decode, must be of\n",
        "        length n_inputs. Can also be a tensor of shape n_stimuli x n_inputs,\n",
        "        containing n_stimuli vectors of neural responses\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: network outputs for each input provided in r. If\n",
        "        r is a vector, then y is a 1D tensor of length 1. If r is a 2D\n",
        "        tensor then y is a 2D tensor of shape n_stimuli x 1.\n",
        "\n",
        "    \"\"\"\n",
        "    h = self.in_layer(r)  # hidden representation\n",
        "    y = self.out_layer(h)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOmmNhEQ5X1f"
      },
      "source": [
        "The next cell contains code for initializing and running this network. We use it to decode stimulus orientation from a vector of neural responses to the very first stimulus. Note that when the initialized network class is called as a function on an input (e.g. `net(r)`), its `.forward()` method is called. This is a special property of the `nn.Module` class.\n",
        "\n",
        "Note that the decoded orientations at this point will be nonsense, since the network has been initialized with random weights. Below, we'll learn how to optimize these weights for good stimulus decoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di6etFgb5X1f"
      },
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize a deep network with M=200 hidden units\n",
        "net = DeepNet(n_neurons, 200)\n",
        "\n",
        "# Get neural responses (r) to and orientation (ori) to one stimulus in dataset\n",
        "r, ori = get_data(1, resp_train, stimuli_train)  # using helper function get_data\n",
        "\n",
        "# Decode orientation from these neural responses using initialized network\n",
        "out = net(r)  # compute output from network, equivalent to net.forward(r)\n",
        "\n",
        "print('decoded orientation: %.2f degrees' % out)\n",
        "print('true orientation: %.2f degrees' % ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LgzwYXm5X1f"
      },
      "source": [
        "---\n",
        "### Section 2.2: Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ixpzbYAX5X1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "18ae7836-c216-4d24-d3e0-daaf4ba188d6"
      },
      "source": [
        "#@title Video 2: Nonlinear activation functions\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"JAdukDCQALA\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/JAdukDCQALA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/JAdukDCQALA?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fa730095da0>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLRXXYPKHPbQ"
      },
      "source": [
        "# Summary of Video 2:\n",
        "\n",
        "- So the power of neural networks comes with their ability to fit complex functions,\n",
        "and this ability is possible by using non-linear activation functions.\n",
        "\n",
        "- These activation functions are non-linearities\n",
        "that are added to our hidden layer such as this function phi.\n",
        "The most common non-linearity used is the rectified linear unit, or ReLU,\n",
        "which is the function max(0,x) - where x is the input to our hidden layer here.\n",
        "\n",
        "- At the beginning of neural network development researchers experimented with various different\n",
        "non-linearities such as sigmoid and tanh functions.\n",
        "But, in the end they found that ReLU activation functions work the best\n",
        "and that's because in ReLU activation functions the gradient\n",
        "is one for all values of x greater than zero and zero for all values of x less than zero.\n",
        "\n",
        "So the gradient is able to back-propagate through the network as long as the input is greater than zero,\n",
        "whereas with this saturating non-linearity the gradients become very small and these saturating regimes which reduces the effective computing regime of this unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WCyi3KZ5X1g"
      },
      "source": [
        "Note that the deep network we constructed above comprises solely **linear** operations on each layer: each layer is just a weighted sum of the elements in the previous layer. It turns out that linear hidden layers like this aren't particularly useful, since a sequence of linear transformations is actually essentially the same as a single linear transformation. We can see this from the above equations by plugging in the first one into the second one to obtain\n",
        "\\begin{equation}\n",
        "    y^{(n)} = \\mathbf{W}^{out} \\left( \\mathbf{W}^{in} \\mathbf{r}^{(n)} + \\mathbf{b}^{in} \\right) + \\mathbf{b}^{out} = \\mathbf{W}^{out}\\mathbf{W}^{in} \\mathbf{r}^{(n)} + \\left( \\mathbf{W}^{out}\\mathbf{b}^{in} + \\mathbf{b}^{out} \\right)\n",
        "\\end{equation}\n",
        "In other words, the output is still just a weighted sum of elements in the input -- the hidden layer has done nothing to change this.\n",
        "\n",
        "To extend the set of computable input/output transformations to more than just weighted sums, we'll incorporate a **non-linear activation function** in the hidden units. This is done by simply modifying the equation for the hidden layer activations to be\n",
        "\\begin{equation}\n",
        "    \\mathbf{h}^{(n)} = \\phi(\\mathbf{W}^{in} \\mathbf{r}^{(n)} + \\mathbf{b}^{in})\n",
        "\\end{equation}\n",
        "where $\\phi$ is referred to as the activation function. Using a non-linear activation function will ensure that the hidden layer performs a non-linear transformation of the input, which will make our network much more powerful (or *expressive*, cf. appendix). In practice, deep networks *always* use non-linear activation functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwm5AL4F5X1g"
      },
      "source": [
        "#### Exercise 1: Nonlinear Activations \n",
        "\n",
        "Create a new class `DeepNetReLU` by modifying our above deep network model to use a non-linear activation function. We'll use the linear rectification function:\n",
        "\\begin{equation}\n",
        "  \\phi(x) = \n",
        "  \\begin{cases}\n",
        "    x & \\text{if } x > 0 \\\\\n",
        "    0 & \\text{else}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "which can be implemented in PyTorch using `torch.relu()`. Hidden layers with this activation function are typically referred to as \"**Re**ctified **L**inear **U**nits\", or **ReLU**'s.\n",
        "\n",
        "Initialize this network with 20 hidden units and run on an example stimulus.\n",
        "\n",
        "**Hint**: you only need to modify the `forward()` method of the above `DeepNet()` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXPGxj_45X1g"
      },
      "source": [
        "class DeepNetReLU(nn.Module):\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n",
        "\n",
        "  def forward(self, r):\n",
        "\n",
        "    ############################################################################\n",
        "    ## TO DO for students: write code for computing network output using a\n",
        "    ## rectified linear activation function for the hidden units\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Student exercise: complete DeepNetReLU forward\")\n",
        "    ############################################################################\n",
        "\n",
        "    h = ...\n",
        "    y = ...\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Get neural responses (r) to and orientation (ori) to one stimulus in dataset\n",
        "r, ori = get_data(1, resp_train, stimuli_train)\n",
        "\n",
        "# Uncomment to test your class\n",
        "\n",
        "# Initialize deep network with M=20 hidden units and uncomment lines below\n",
        "# net = DeepNetReLU(...)\n",
        "\n",
        "# Decode orientation from these neural responses using initialized network\n",
        "# net(r) is equivalent to net.forward(r)\n",
        "# out = net(r)\n",
        "\n",
        "# print('decoded orientation: %.2f degrees' % out)\n",
        "# print('true orientation: %.2f degrees' % ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaXeaexl5X1h"
      },
      "source": [
        "# to_remove solution\n",
        "\n",
        "class DeepNetReLU(nn.Module):\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n",
        "\n",
        "  def forward(self, r):\n",
        "\n",
        "    h = torch.relu(self.in_layer(r))\n",
        "    y = self.out_layer(h)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Get neural responses (r) to and orientation (ori) to one stimulus in dataset\n",
        "r, ori = get_data(1, resp_train, stimuli_train)\n",
        "\n",
        "\n",
        "# Initialize deep network with M=20 hidden units and uncomment lines below\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Decode orientation from these neural responses using initialized network\n",
        "# net(r) is equivalent to net.forward(r)\n",
        "out = net(r)\n",
        "\n",
        "print('decoded orientation: %.2f degrees' % out)\n",
        "print('true orientation: %.2f degrees' % ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELCCZVos5X1h"
      },
      "source": [
        "You should see that the decoded orientation is 0.13 $^{\\circ}$ while the true orientation is 139.00 $^{\\circ}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7m5V65z5X1i"
      },
      "source": [
        "---\n",
        "# Section 3: Loss functions and gradient descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Uaj3ZYEf5X1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "e2cbcd0c-77d2-4ec6-8901-0e494e8ff844"
      },
      "source": [
        "#@title Video 3: Loss functions & gradient descent\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"aEtKpzEuviw\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/aEtKpzEuviw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/aEtKpzEuviw?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fa730095e48>"
            ],
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAUDBAgICAgICAgICAgICAgKCggICwoICAgICAgICAgICAgIDRALCAgOCQgIDRUNDhERExMTCAsWGBYSGBASExIBBQUFBwcHDgkJDxIPEA4SEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhISEhUSEv/AABEIAWgB4AMBIgACEQEDEQH/xAAdAAEAAgMBAQEBAAAAAAAAAAAABgcEBQgDCQIB/8QAWhAAAQQBAQMGBwgPAwoFBAMAAQACAwQFEQYSIQcTFjFBkQgUIlFSYdIyNlNxgZKy0RUXGCM0QlRWc3WUlaGx0wlytCQmM0NigrXB4fA3Y3R2szWixfFkg5P/xAAZAQEAAwEBAAAAAAAAAAAAAAAAAgMEAQX/xAAnEQEAAgICAgEDBAMAAAAAAAAAAQIDESExBBJBIlFhEzJxgRQjkf/aAAwDAQACEQMRAD8A4yREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERARb/opY9OHvf7CdFLHpw97/AGEGgRb/AKKWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/wBhBoEW/wCilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v8AYQaBFv8AopY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/AGEGgRb/AKKWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/wBhBoEW/wCilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v8AYQaBFv8AopY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/AGEGgRb/AKKWPTh73+wnRSx6cPe/2EGgRb/opY9OHvf7CdFLHpw97/YQaBFv+ilj04e9/sJ0UsenD3v9hBoEW/6KWPTh73+wnRSx6cPe/wBhBoEW/wCilj04e9/sJ0UsenD3v9hBM0REBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREHjcm5uN8hGoY1ztOrXdGumq0HS1nwLvnD6luc1+DT/opPolVyglvS1nwLvnD6k6Ws+Bd84fUokiCW9LWfAu+cPqTpaz4F3zh9SiSIJb0tZ8C75w+pOlrPgXfOH1KJIglvS1nwLvnD6k6Ws+Bd84fUokp5sJyO7S5yr47isVNbq86+Lng+CJpkjDS8NE8jS4DeA3gNNdRrqDoGB0tZ8C75w+pOlrPgXfOH1KKzROY5zHtcx7HFrmuBa5rmnRzXNPEOBBGh8yl3J5yYZ7aBk8mHx0t1lZ0bZXsdFG1j5Q4sbrO9oc7Rjjo3XThrpqNQ8+lrPgXfOH1J0tZ8C75w+paTafBW8ZbnoXoHVrdZ+5NC8tLo36B2hLCWu8lwOoJBBC2XJ/sLls/PLWxFKS7PDCZpGRujZuRB7I94vmc1vupGjTXU8eHA6Bk9LWfAu+cPqTpaz4F3zh9S1e2ey2Qw1yShk6slS3E2NzoZN1xDZWNkY4PjJY9pa4cWk8dR1ghaZBLelrPgXfOH1J0tZ8C75w+pfvH8lu01iPnYNns3LEW7wkjo23McD2scI9H/wC7qo1l8ZZpzPr2689WePTfgsxvgmZqNRvxygObwI6wgkXS1nwLvnD6k6Ws+Bd84fUoktxsjsxkMvabSxtSa7ac1zhDAN5+4zi9514NaNRqSe0INr0tZ8C75w+pOlrPgXfOH1KZV/Bm25eNW4GQD/btUIz3SWAVrNouQLbGhGZbGAvFjes1RHeIHaS2i+RwaO06aBBoOlrPgXfOH1J0tZ8C75w+pRSRjmktcC1zSQWuGha4HQgg8QQexflBLelrPgXfOH1J0tZ8C75w+pRJEEt6Ws+Bd84fUnS1nwLvnD6lrcxsjlqUXP3MXkakO8G89ZqzwRbzvct5yVgbvHQ6DXsWkQS3paz4F3zh9SdLWfAu+cPqUSRBLelrPgXfOH1J0tZ8C75w+pRJEEt6Ws+Bd84fUs3DZ1tmQxiNzCGl2pIPUQNNAPWoKt7sR+Eu/RO+kxBNUREBERAREQEREBERAREQEREBERAREQEREBERAREQEREGHmvwaf8ARSfRKrlWNmvwaf8ARSfRKrlAREQEREBERAXbf9mxtJv083iHaDmLFe7Fx4uFmM1rGjexrTVr/wD+vfxIrz8BnaP7H7ZUozoI8nXtUXuJ03ecYLMOnnLrFWBmnD3fyIIr4T+zv2L2vz1UAhjrz7LOGgEeQa281rfO1vjG5/uecFdq+Alsz9jtj607wWy5W1YuO3uBDC8VK4H+y6KsyQD/AM71qkf7Q/ZOR20OEtV2b8mWp+JsiboHSWqlkAcSdN5zb1dnH0Arq8IbamLYzZ7ZqlC4iOHKYKuQ0kPNDEPhtWXaDyn73ikLHDrPjB+UOb/7QPZvxPasXGtIZlaFacv/ABTYrh1KRo8xEVeu4/pdeslW3/ZubNc1jMxlnNcHXLkNSPeGg5qlFzr3xkjymukubpPEa19Owr2/tHtnhPhsRlWDedSvSV3ObxAgvw7++4jraJacQB883rUpxEnQ7krbLq6KyzCmUEj74zI5l29CHADiY7F2NvqEXmGqDiLl52pGa2lzOSa4PisXpWwvbxD6tfSrUcD668MR+Vb3wXdv8bs3n25HK1PGqwq2I2ubFHPYqznckhsVmykbk2sZi3g5pDbD+KqtEHZF/wAOJ4sHmNnWOqhx0E1wssSMB4OJZAWROI08ny9POetWn4RuAxe1+w8mcbDuTQYk5ajYka1tquxkHjU1WRzCdWvia+NzN5zd8NcNSxpVdbIeBNT1gnyGensQOZG91erVZUeS5ocWCzJNN5PHTXmwfiUn8NPambZ7ZiLA4rGWYqVuvFSN9jd6lTpRBsZpc6S53jMsbNz74Bqx0hBc4EtDgJdA+AF78ov1de+jGufl0D4AXvyi/V176MaC8vC68IDObK5qrjsZFj3QTYuG259qKWWXnZLd2BzQ5krGiPcrMOm7rqXceoCB7BeGxkGTNbnMXVnrEgOlxnOV7Mbe14isyvjsO/2d6L41of7R331UP/b9X/iOVXMyD6F+ETyT4nbXB9IcG2J+SNTxqrart5s5OFrN41LLSATOWtLGmQB7HsDHaDeA+ei+gP8AZ1ZOabZe3BI4uZUy87IQeqOKWtVndG31c9JM/wCOUriXlapR1toM7WiaGxV8xk4o2t4NbHFdnjY0AdQDWhBGF094DHI39lbw2hyMR+xuNmHiscgG5dyDOIeQeLoK53Xk8AZObGpDJGql+RPk7tbUZmtiqvkNf99s2NNRVpRuYJ7BHa4b7Wtbw3nyRjUa6jrTwteUapsjgq2x+A0r2ZqbYnGIguo406te5z+s3LJ5wb3F2jpnktc5jiE68OKVr9hsg9jg5j5cY5rmnVrmuvVy1wI4EEEHVfNtfRXwwf8Aw6l/R4X/ABNRfOpAREQEREBb3Yj8Jd+id9Ji0S3uxH4S79E76TEE1REQEREBERAREQEREBERAREQEREBERAREQEREBERAREQYea/Bp/0Un0Sq5VjZr8Gn/RSfRKrlAREQEREBERAWz2UzEmOv0chEAZaNytaYCdAZK0zJmAnsG8wLWIg+qvKHshBtDLsteYGyQ4/LwZLnNdNazaNmaHd091vWxjzp1aA9a5I/tFNqRa2gpYxjmuZiqO88A6llrIObNI1w7P8mipO/wB/4l0J4LHKliZdkcMy9lcbUtVK5pvgs2oIJWtpSOr1yWSvDtHVmQO6vxlwZy0bUfZraHMZMO347V+d0J6v8ljdzNQce0V44h8iDvXA41m3HJ1jK0kjnPt1MdHLMdGv8Yxl2CK2/iPJLn05+IHVIdOsKBf2jm04r4nE4WI7pu2pLMgZoGivQjEccbm+i6Wy1w4ddY+bj5+AJyjY+vgLuNyORp1H08g6SFtyeGtrWtxMduxc85vOaTxWHHTXTnRr1hUV4a+2kWZ2ssGtNFYp4+tWpwTQSCWGXdYbM72OZ5J/yixKzUE6iIcewBSKIr38ECtsdLaycW1klNvjFaKCmy+Xw1wHukdblFsbrKk4DIA2QvY4B8m6QgqrD7dZqm6N9XL5KuYg0M5m1OwNawANaGtfpuAADd6tOGi+iHINmJNtdiY/s9Gyd12K5StP3BGLIilfC2yxreEc2gY7eZppJGXN3dABB/uW9gJJRcjyNrxXUO5iPI1nUnN83POjM+56xLrw615ctfLzs7sxgzgNlZa09sVXVq4x7ufqY2OQOa6zJa1cye0C57w3ee4yeVJ1+UHBsrN1zm6h26SN5p1adDpq09oKv7wAvflF+rr30Y1z8rz8B7M06G1sU963WpQeI3Gc9bljrxGR4jDI+cmcG77j1DXUoJN/aO++qh/7fq/8Ryq5mX0o5XOTLYzay5DksllGGaGpHWaal+tHEYI5p526gh2rt+xJx16tPMo1hcVyU7HSeOtuYyW7XLd2R9k5i7FJqCx0VSAycxLqQd9sbSNddQEEh8F3ZnofsW6zlgasjxay11kgIfWYYWbkTmHRwlFWvDrGRqHve3rXzr2lyr7125ekAEly1YsPA4gPsSvmeAe0bzyr48KTwkZdp4zisZFLUwwkY6QykCzkHxuD4zMxhLYa7ZAHCPVxJYxxIIDW87oO5vAdz+ymG2dM9nLYqllb1mY223bdavYEcEjo6sQjmc1zYBH5Y69XTPOvYN7tRsTyU5O5ZyF/NYuzctyulmmkzo1c93YALIaxjWgNaxoDWta1oAAAXz8RB9YeU/CYC5gjUzc8EWFIqayz2fFIdI3xmrrb326avbHp5Xlajr1XzX5esXh6W0WRq4CaOfExOreLSxTeNxu36deScMsgnnWtsOmbrqfc6anRdfeFZtph7ewD69XK46xYkZh9yvBaglncWTVpHgRRvL9QxjieHANK4IQEREBERAW92I/CXfonfSYtEt7sR+Eu/RO+kxBNUREBERAREQEREBERAREQEREBERAREQEREBERAREQEREGHmvwaf8ARSfRKrlWNmvwaf8ARSfRKrlAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAW92I/CXfonfSYtEt7sR+Eu/RO+kxBNUREBERAREQEREBERAREQEREBERAREQEREBERAREQEREGHmvwaf8ARSfRKrlWNmvwaf8ARSfRKrlAREQEREF+eDLJFV2e25yop0LN3G1cO+rLdrQXPF3TT3WS82LDXboc0N1A013Ga67oWy2btUNtsBtKbuJxlDN4HHSZSvk8VWZR8agrse+xXuww6MleRGGh3/mDQN3Dvxnwe9psJXw21uIzGSdi/s5Bi4oLIqz3w3xWW3JMXRVuP+sjHEj3fqWXY222d2cwWVw+zdi5l8jnYW17uZs1/EKsVHWQPrU6kjnTb7mPe1xfw++A7x3Q0BDeS/k1GWp5DK3sjDh8NjDCyfITRS2nOsWDuw1q1WDR9iU6tJAI3Q5pPWvDld5OpMA+hJHchyWNytRtuhkYGPhbZgOge2SvN98rzsLm7zHa6B7dSDvNbYvg38q9bGYXLYKfL2NnZ7VqO7TzUFd19kUzWRRT1rNaJrpHMfHEwAtB03nk6boDoXy+7VTZKzTY/ah+1EVeu9zbLqcuObWmnk0nrxxTta+VpZDXdzhA1JI0G7qQleO8HgPmrYqxtBRq7UXKXjUOBfBO8AOifPHWs5Jn3mvbMTHEx6HTTtBaXQrkY5LbO0uVs4iOxHQs16lqcmy1xZzlZ8cboJN3jEd+Ti7Q6bp4FdBbTcudfLur5Cpt5c2XMtWIWsK/FzZLxe5EzdldUswM5qSGQgEBzmnrcd3e3GwLwILfjG1OTnuPe/n8HlpLEjdOcfzktd87x2b53nHzalBF9qORqszD38xg9oae0EWImijyUNevPVlqCZ4jZPEJtfG6pfvffWgNLWPcCQx27UKvyPaDZrZvZzaGhh83Jn8jtC2vVB8Rs4+CjRhfI575Ra05ydzJZG+QXAO3OwEmg0F4bR4+Acl+HsiCEWX7UWY3WAxgndGKtwiN0oG+5moB3SdOA8yqvYPZa3m8lTxVFjX2rsojjDzuxt4F8ksjgCWxMja97iATow6Angrh2ZzezOR2IobPZTPuw1upmbF5xGPt5EOjfFNExn3jdYNRNva7x03dNOKivJ1tLjNktr6GSpXJM5jKMh3rTK76Es0VulJWtGOrZcXCSLxmXQOcA8xDi0O1AZm1fIzWix2Vv4baCrnHYKWNmTrQ1rFV1dskjoeerSTEtvVxKx4MjNBusLgSNNdFyV7DYnLMAv7RwYizNb8Xr1DStX5pjuRu5+Q19I69fek3d9zv9XISABqrP5Y+UnxjH5NlLlEtZWCz5EOFlxU9eWWtPM0SQWb0sbI2mKFztXt15wxcA3e4ORflCwlPZeLHNz0+y+TblHz5CzVpTWrmWogOMcFa3XafFnAFjAHuaA5hOmjjqFQbZcn1rE7QybO25I/GI7lauZ49XRFtoQvhnaDo7dMU8b93r46KQ8uvJPW2VcKr89UyGUEzRLjq0MrH1q0kbpYbE07iWNkc0RnmfdATMdxBBOZy6baY3J7dTZulYM2Okt4iUTGOWNwZUqUIp9YZWiTVr4JOzjoNNdVpvCW2opZnanLZPHSmelafWMUpY+IvEdGtC883M1r26SRvHEDqQTbFeDxVczAm9tTQoTbSUaU+OqurWJp5bFuNrzWm3CI4IwZYGNmc774972hoLRvQjYnZa5j9sqOHmjpOvV83BUc25GblB0wstia+WHVjrFV2oeBq0lpHUVMts+UPEWL3J1NDZc+PAY7BQ5F3NTN8Xlo2IH2WtDmAz7rWOOse8Dpw1WtzO3OMk5Rm59lgnFDP1LfjPNyg+LRSwufJzBbzvAMd5O7rw6kGnv7HzZjbe5hHSU6c9nO36jpKkDoqMUkVidjnV6m+XRwExHRheSA4ak9ake0XIDFDUzb8ftFQymS2cYZMpjYIJ4fF4mGQTGG5N97tPjEb99rR5JY5pIdoHa7ZrbXGw8oj89JOW4o7R5C6LPNyk+KWLVmSKTmA3ndSyRh3d3eGvELcbGcoeIrXuUWaay5kefx2dhxx5qZ3jEt6xO+s1wawmDea9p1k3QNeOiCC8kXJs/PDI2Zr1fF4rD12z5DJWGvmFdkpe2COKtD98s2JHRvDWDTXccAS4sa7ccp/JHDicJSz9LN1svj8jddWruhgmrSaMile980c53oZGyQyMMZ1PuTrx0GdyD7VYxuI2k2dzUlupjsxFSlGUqwOtjHWqVgPgktQR+XJWfMYG8COLQ0ac5vNm3Lvj8dR5OtmKmPtT24JMxcmitWYHUX3W83Z56zBUkJkjqh80bWl3EjdPU4ahzMuh+RWtg72xu1cbsHWOTxWJfO/LTu8Znlmnntcx4rHI3doNihjjbrGdXnUlc8K2uRHbPHY3CbZU7k5isZfFQ16bBHJIJpmGzvML42lsX+kZxeQOKCpUREBERAW92I/CXfonfSYtEt7sR+Eu/RO+kxBNUREBERAREQEREBFnY3Fyz8WjdZ6buDfk7XfIpLj9n6sPlWN6Y+bXm29w4nvVV81Kdyvx+PkydR/aGIrJr3Ma0hvidfTq8oa/wAyva7jMLO3/R8w704TwB+LqIVMeZTetSvnwMmuJhWCKY9Hsa15Y67IfMdGtBHcdV+JdjWytL6VuKfT/VuO674gfP8AGAFbHkY5+VP+Nl10iKL2uVZIXujlY6N7etrhoR9Y9a8VcziIiAiIgIiICIiAiIgw81+DT/opPolVyrNt05rEb4K8Uk88zHMjhhY6WWWRwIayONgLnvJ7ANVHvtZ7Sfm/m/3fc/pIImiln2s9pPzfzf7vuf0k+1ntJ+b+b/d9z+kgiaKWfaz2k/N/N/u+5/ST7We0n5v5v933P6SCJopZ9rPaT8383+77n9JPtZ7Sfm/m/wB33P6SCJopZ9rPaT8383+77n9JPtZ7Sfm/m/3fc/pIImt1shtVkMRNJYxtp9SeWCSu+SMNJdBNu85Gd8EaHdb6+C2X2s9pPzfzf7vuf0k+1ntJ+b+b/d9z+kgiaKWfaz2k/N/N/u+5/ST7We0n5v5v933P6SCJopZ9rPaT8383+77n9JPtZ7Sfm/m/3fc/pIImiln2s9pPzfzf7vuf0k+1ntJ+b+b/AHfc/pIImiln2s9pPzfzf7vuf0k+1ntJ+b+b/d9z+kgiaKWfaz2k/N/N/u+5/ST7We0n5v5v933P6SCJopZ9rPaT8383+77n9JPtZ7Sfm/m/3fc/pIPHk/29y+AmlnxF2SlJPGI5SxscjJYwd4Nkina5jwDr1jhqfOvxt7t1l89Oyxl7896WNhZGZd1rImEglsUMYbHECQCd1o10GuuiyftZ7Sfm/m/3fc/pJ9rPaT8383+77n9JBE0Us+1ntJ+b+b/d9z+kn2s9pPzfzf7vuf0kETRSz7We0n5v5v8Ad9z+kn2s9pPzfzf7vuf0kETRSz7We0n5v5v933P6Sfaz2k/N/N/u+5/SQRNb3Yj8Jd+id9Jiz/tZ7Sfm/m/3fc/pLOwuxuXoSGe9islSgLCznrdWxXi5xxaWs5yZgbvkNdoNdToUG4REQEREBERAW32fxfPayyAiFh0/vu7G/F51rqFZ00rIme6kcGj1a9ZPqA4/IpzZ5qnXLv8AVxDSNunGR+unOlvWSTroPWfUs/kZfSNR3LV4uGL23PUP5duR1WakgHTyWn3I8x4fjepRW/l5ZCSNdOvV3DQeodi8MhYcXc9YJ33+4i6y0eYD0uPE9iwI4HzHyiGN16tTqfj04n4ljjHHcvSm3xHTcYqdjj5biTr19ehPZosrJzDd0aZnH0WndH8BoFi46SnXIDpdXeYauIP91vDvWVcttlLWsa4g9Qdq3X4gOC7MOVmWO+EuqmVoP3qUauJ3iGnr3u0BemItu3mujed4djSA7Qeo8T8ikmzGLL4LUJj3WyxkcSDo7iR69OvuVfPpPjlLNQHNd7l3ku4Hhoexd4nhCszW3CzZKsWVhEcpDLDRoyQDiDr1EniWHtHcq7yVKSvK+GVu69h0I7D5iD2tI46qU7K3nFwa8FsnYere3fj/ABv5racoVIWa4tMb9+g4SafjR9p9YB8r5XKfj5Zi3rPXwr8zDEx7x38q6REW95YiIgIiICIiAiIgmfIV75sF+s630l9E187OQr3zYL9Z1vpL6JoCKBcvuy2VzOCs0MLdGPyEslZ0dl009VrWRTsklaZ6rXSt3mNI4A68AfOuS9uORPlDxGNu5SztSH16NeSeRkGVyzpnMjGrhG18LWl+nYXAetB3ii+c3IlsXtxtdWtWsbtLPDHUnbC8Xcnkonue+PnAWCBkgLdD2kfEuovBh5MNqcBZyEu0OZZk4rEELII2XLt3m5GSPc95bdjY2PySBq3UnUoL3Rfx50BOhOgJ0Gmp07BroNfjXCfKx4Ve0eRbYfs9SnxOMpyMbNefC2zaDnvLImWZHtdWpb7tBzY1dq06PI1CDu1FVHgl7U38zspQyGTsOtXJZbrXzubGxzmxXJ44wWxNa3gxrR1ditdAREQEREBERAREQEREBERARFzp4TfhCXtl8xRw9HH1J32qtWy61afK5rWT3LFZ0Ta8JYd4NrEh5f1yDyeHEOi0REBERARU54RfL5S2ONeCShavXLcL5YWMIr1dxj+bdztt4do/U+5Yx5A0Lt3ebvUlF4at+KSJ9vZljK02jmbtmWOR8fDV8UksG5LoCDwAB1HEa6oO0EUU5KOUDHbS42LKY2RzoXudHJFIA2etYYGmSvOxpIbK0PYeBILXtcCQ4FStAVIeGt72W/rOn9GdXeqQ8Nf3st/WdP6M6DihERAREQERfqNhc4NaNS4gAeck6BBL+TjGbxltP0DIwWhx7OovcPXoWt/3z5lj7X5LWTXTe3DuxxjhrI4b3X6LG6anz6qV5KJmPx8FYcH6B0mnW6Rw1a0/7x1VWbT3N6xzDDxY3V7vRDjvEA+cleZNv1Mu/iHsY6xjxRHzLybJ5biXBztDvyn3IAJO6zXqaP4rOx9Ge4Cyu1zWHgXng53n49bQfN1rVYiA25RCwHmmuG8exzuzj2tHYFffJ9g2RNaN3q/ifOuZL+vELKV2g2znJfIPKIBJ7/Xx7VM8TyaSE9TGgdvEu/3QOpWnjq7BpwW9qsA0Kz82lZ7ajhAYNipK8J3QT5OmvW7Q9eipnlY2Vkifzoa7tIdx14dY18668Y0FvUFCtv8AZqOzCRug8SeA8/WrfX15hXW8X4lytstmBI4Vpf8ATMOsbzwc7T8UqaMvcAHDUPYAWO6nN6nNJ9RUC5QdnX07TizVrmHfbw09yQVtfsgZqsUzPdAbxb69dJG6f3ge9Xa3zCMz3EtZnKfMTvjGu7qHMJ7WOGrfj4HT5CsJSDM7tmrHZb7qJ267z7jz2+pr+H+8o+t9LbjbyMlPW2hERSQEREBERAREQTPkK982C/Wdb6S+ia+dnIV75sF+s630l9E0BV34S/vQ2j/VVr/4yrEVd+Ev70No/wBVWv8A4ygpT+zZ/wDpGb/WUP8AhWrq9cof2bP/ANIzf6yh/wAK1dXoC588N/G16uxFyKrBDWjOQpyGOCNkLDLNc5yWQsjABkfI5znO6yXEniV0GqG8PP3l3P8A1mP/AMQ1BkeAv7yMZ+nyP+PsK8VR3gL+8jGfp8j/AI+wrxQEREFL+Gll7lDZC5ZoW7NKwyzRAnqSyV5g19ljHNbLCQ9oIOh0PFczcktvlB24pw4ujl7FTHYsPFrKT2bLJbU1ixNO2OxbaXWbszYpd1sIIjayGLe3SWk9HeHX7ycj/wCpx3+NhWB4AEUbdjoyxrQ6TJXnSlumrpAYow5/ndzTIh8TWoOe9t623vJtcq3H5eW7SsSFrHumsXMbYe3y31blWyQYZ3MaSHN0du7+5Jq1273ByW7ZV9oMPQzFUFkV2HfMZO86GZjnRWIHO0G8Y5mSM3tBru69RVXeHg2E7FXudDd8W8eYN7TUTeNRhxZr+NzBn6uwuWJ4AAl6HR85ruHJXua1+C1iB09XPCb+KC2+VavlpcNkGYKZtfLmuTUkcInDnmua4x6WPvTXPY18Yc/g0yB3YuYH+DZt3bidaubZSDIODntgNrISxNedS1jrILeZAOg+9xuDezUAK+vCR5U2bJYR+QELbFuaZlanXeXNjfYka95fMWeVzMcccjyBoXFrW6t394c47EbNcpm3FdmWm2ikw+OsOkMDY5pqPOxB5aHQ08c1u/AHNLQ6d4cQ3e8oHeIbzwOOVzNfZu1shtDYktTxeOMgmsyc9ahuUHuFqk6c6usx83HO8Oc5xb4uQNWuG718vnl4PeEsY3lUrUbVp12zVu5iOa24vLrMoxWQ35nGQl5c4kklxJ4r6GoOJ/DW5RMzg9sMa6hkb0NaHHY+06hHZsQ0rD4790vZYrwva2RkjYWsdqNS3h2BbfkD2H2+yeZxW1eZyMjaEsrrDqk9qaN76ksEnNczjoWmCGB2+wiM7nA6kcdTBv7QSESbY42N3uX4eg06cDuuyORadD2HQrvljQ0BrQAAAABwAA4AADqGiDzuMc6ORrHbj3MeGv8ARcWkNdw8x0PyL5f8v2xm0OHzVOln8r9k8jNTryw2/Grd3moJLVmKKLn7rGys3Zopn7rQQOc1HElfUVcGeH979MR+qMd/xTJILu8Hfkt22w2Xkt7Q7RfZSg6lNEK32QyN/wDyh8sDo5eZvRMjbo1kg3wd4b2mmhK6FREBERBqszgcfZmrW7lSpPPQ511exYjjkfUMoYJXwySA8yXCNmpGnuQqk8L7bDZ+HZfJ079mpPatVnNpU2vjlsm4fwexHE0l0bInkPMh0ADSNSXBpszlO2TjzuIv4iWZ9eO9DzRmjAe+Py2vDg13B3Fo4fyXHPKv4HVrGUbGQxGT+yJqROmkoz1xFPJFE0vkNeRj3smk3RqInMbvaEAkkNIWL/Zx4W3Bg8ncma9tW9fZ4sHAgSeLRuisTx6+6YZHCPeH41dw/FXUq548Cnljl2ioT427FBFexEVcNfXjjrwWKTg6ONza8IEcEkbow1zWNazSSPdA4gdDoCpDw1/ey39Z0/ozq71SHhr+9lv6zp/RnQcUIiICIiApPydYrn7POO0EcHlFx6g78Xu61GFYdQChjmt1AmnG8/zhrupvy8O5Z/JyetOO54afExxfJz1HLC21y7XSvkJ+912EtB/HdoQ3e+UdnmKqa9ZduufqTNYeST2ne7fiAW+2uv6xiNpJMj+Pbw6h/Ef/AGrSYmDxi81v4kQA9WunX3rNhr6123Zre1tLR5J8B5DS5vE6H/8AavLA1A0NGmirLZzLNpta2KtNZeAP9GNGA+YuJ4lbN3Ksa8m7PjLMQ87tB8o14LLETaZlpi0VrELiqVwO1bGJumirvZjlHpXdBEXxv9CQaH5COBU0rXg8A6jqXaxESik9YcBw715XIt4KGZzlCq0B99D36djACdflK1tDljx1hwYIp2OPud5uocfNq3VaNxMKNTEoZ4Quy29C61G3y2jytO0HXVUPsreAY6N3UHdX946H+I1+VdX53MVcjBNXcHRSujOkcoLC8aHiwu4H4lyPfrGnkpYSNGvLwPkJ/j9SYe5hPLMaiUowQa2WSsSebsNc3Q/ilw8lwHx6LSTRljnMcNHMcWkeYtOh/iFsaUm9uu/Hhc0k+duvux/DX4177Y19yzvjTdnjZKNOrVw0f/8AcD3rXinU6YfJrxFmlREV7GIiICIiAiIgmfIV75sF+s630l9E187OQr3zYL9Z1vpL6JoCgXhD05rGymfgrxSTzSYu01kMLXSSyOMZ0ayNgLnu9QCnqIOYf7PXA3qGKzDb1K3SdJkIixtuGWu57RWaCWNmaC4AkcQunkRAVJeG3irV3ZC3BTrWLcxt0XCGtG+eUtbYbvOEcQLiB2nRXaiCmvAuxdmnsbjoLdeerO2bIF0NmN8ErQ69Yc0ujlAcAQQRqOIKuVEQEREFGeHX7ycj/wCpx3+NhXLng9bdbW7J477IUcS/K7P5CaZz2BskkcVqvpBK/na286lMQ2IEyNLXtYNNS3VvUfh1j/MnI+qxjvk/y6EcVjeAQP8AMyt671/T1jntOHyj+CDnrbfNba8p9qpSgxDqOMgl5xoLZoqEMhbzZtXb8zdJpWMfIGtjbvbrnhrHEkntzkv2Or4DEUMRVJdFShDOcI3XTSuc6SedzdTuukmfI/TU6b+nYpKiCifDY5N720WAi+xkbrFzG2xZbVbpzlmB0T4Z2QhxAdM0PbIG66uEbmgFzmg0byWeEBtVicTX2cg2VsXchSidBXlfFbbIxgLuZbYoRxb8pZru8Hx6ho146k90Ig4D8H/YraWjyi0Leax17n32L09y5zLpK3P5DF25Xufagb4vrzlkB26d0O3mjqXfiIg4i8OLZTKXdr8XPTxuQtwNxlBhmq1p7ETXtyN5zmGSJhaHhr2EjXXRzfOF26iIC45/tBOTbJ27mOz+PrWbcUNMU7LazHSyVTBPNZrzlkYMnNu8ZlBeODTE3XTeGvYyIOfPBi5csztRekpZHCClFXxzpnX422GxS245q0RiAlbuRb7ZZXhm+SOaPXxI6DREBERBSPKV4SuD2fz0mEyEVoiKCB8tysGzMgmnaZRBNBqJABAYX7zN8/fgN0aaqK8qvhb7NxYy0zDSz5HITwSxQNNeavBBJIxzWzWX2WsLo2a67rA4uIA8kEuF2becm2BzoH2WxVS64N3BNIzcssZ6LLURbNG31NcFFMH4OOxNOUTQ4Cs947Lctq/F8sF6WSI/K1BSv9nRsJbrsyWfsRPhgtxR1Ke+C02GMlMtmdoI4wh7IWNcNQSJR+KuwV+Yo2saGtaGtaA1rWgBrWtGga0DgABw0X6QFSHhr+9lv6zp/RnV3qkPDX97Lf1nT+jOg4oREQEREGw2cqCa1DG73JcC7+63iVsOUDMavcxjvIaNN71DgOrt9SwNnX7s4IOhLXDXzatK0G01jelcwHhvEcO0+f1rLmr7Xhv8aYrjn8y016zvO3+oMaSPkGjetbXYan45bkjBMbOZEmrNN4e5bpx7TrxUdvu0Y4nXyv5dTf8Akp9yV4wxCnkf9TrJXsH0WyndjkP+yCBr8a5kn1pKVd2v/HbK2jp06LfcWpJXnXQTSfKdGaBvxcFr4bsvOMrA3o+cALN2c2I9XaboLJQ4cSdPVxV9HZGOYa+SHadZAcP++35QsQbAuYdW8wzQnymx6O17dPMVipl+8NlsG+p4Vdh8hPVcXuYybmpHMfuNEc8bgeLXNB3XHTtHX1hW5sxyr4FtUvntujla0/eHMfzpIHU0AacfWVB9tMS6k2SZztSG6NaBoXyO8lrfj1U65OuSvGzYbmbVaN9qWJ2/YP8ApWSvbqHNd+LuuI4duiVisyjb2iNRKA7U7VW7hE9SlzdWYnmprYJMw14PZC3juetxCimK2uy5nfDDNKx0Wu8yrDDEXBrS5zmmRrvJGmnX2q8Nl9lmXsZXied2alvVZWnjuzVjuO1B6tWhrvicsijycxxucS2E73B2oOrhw4EjrClF617h2aTfUxZV2z+as5VhH2dyMMkZ4xzxQP3SOo8G6kfEoJtNQsG2fHZBYLWvcy0xpifvMIIEg8x8/rXSd7YatBuyMjYw9WrBp8QVD8uTj4xMyu14gr83E+Rp3Y3zvBeWadT3AAcdeGil49930j5OPWPbR4m5qyOQDXUd7SNHNPyalSPaKMPp13jjzTnNDuvWN4Bbx9RGirzZi1o3mz1A6j4+xT/APM1O1XPFzGiRgPo9unxPA+ctc8WhntHtj1+EeREWl5oiIgIiICIiCZ8hXvmwX6zrfSX0TXzs5CvfNgv1nW+kvomgKF8pHKps/s6GfZjJwVJJG7zIAHz2XsJIDxWrNfKIy5rhvlobqDx4KV5W42vBPYcCWwQySkDrLYmF5A9ejVzV4GmyVfMQXts8zFDkMxlcjZMc1hombThhLY92qyXe5h29vsBHuYoomN0G8CFs8mfLbs3tHafSxF99i2yF87oXVrUBEEb443yc5NE2PQPljGm9r5XV1qxViRYusybxhleBtjmzHz7Y2Nm5pzmvdFzoG9zZcxhLddNWN8wUQ5WLO1LBVZs1DhiXmc2reZkmbDVYxsfNBkVby3ueXSHe4hvNcR5WoCdIqO5EeVfLXM9kNmNoIsYcjUqi5Bewz3yULNbWBj2ESvLxMHTtIOjeDXgtaWgv3PhS8pV3ZXDV8nRirTPdk61eVlpsj2+LSw2ZJDEIpIy2behZoXEjQu4HhoFsIq25Dc3tTkY7V3aKhRxteyK0uNqVnOfaigk8YdIy+5ziDNzfih4Bh1dJq1h8hsI2l5T9qMltNlNndl4sFAcLFC6xNm3z87bfIxjyKsVbyhG0yBpO6eoEubvNBDoBFqtkJL76FR2UZXiyLoIzajqFxrMsFoMjIS9znFgdqOLj1dZ61S0nKZtVtBkcpW2PrYWLHYa06pLks26yReuR/wCljqMqe4Y068XB2rTG7ebvbqC/iEAVX+D3ynzbRV8hBkK0VLNYW6+lkasLt+FszHSME0OrnFsL3wztALnaGB/lOGhUR5Y+V/aDG7W09mcNjqF+TJYpk1YWTLE6O2+W4HzTytkDXUooKj5HRtaHndOjhwCC/kVB8nHKttDX2pZsntZUxrLd2m61QuYoy8xK1jJpHMeyZzjulta0A482Q6vpuuD2uF+ICqfazwjNjsXbko2su02IXlkra8Nm2yF7To5j5q0boy9pBBa0kggggHgrOzFeSWvPFFIYpZYZWMlHXHI9jmskGna1xB+Rch+D1tziti4ZNltrcU/EXTancclNXE9LIxve1rHSTtBc5gDgxr2h8W6zUuYdQg622azdXJU69+lLz1S3EyWGXdfHzkTxq125K1r26jscAVli1FznM84zndzf5rebzm5qBv7mu9uakDXTTiFj4IVRVrCiK4pCCLxYVNwVRW5tvMeLcz975jm93d3PJ3dNOC4y2Jl24HKDtI6rDs2doDjqgusmNz7GNr8zi+aNQsfz3O7gq675I1MnqQdtIotysbQ2MTgctk6zYX2aFCxYjbMHOhdJDGXgSNY5rizUdQcD61Wng7coe1m0zquTuY7HUdn5Kr2c4xzzbtXYt2OWeBjpHc3TNhkzQ1w1Ab7p/ukFobUbd4nF3MdQv3Y69vLTGGlC5sjjYlDo2bu8xpbFq+WNoMhaC54A1Kkio7wgdphS2m2FqHGYq6chkp2C1frme3QLJsc0S4+UPaK8v37e1Id5UUR01arH5Xdop8Rgctk6rYn2KNGexG2cOdC58TC4CRsbmuLeHY4fGglSLlavyw8oVrAN2prYbBxYmvXEs0EzrDrtyKDybt2swSAQVN5srmteS8NjJ++gtLuhuTLa2HO4jH5eBhjjvV2y80TvmGTUsmhLwBvlkrJGb2g13ddB1IJGioHP8qu0mYz+RwWxlTFuZhCGZDJ5czGuLTnOYK0LKx32kPinZqWu3jDJ7kNBc5GOV3aLJbWXtmc3jsdRkxmLfPP4qZZXS2mzUgyaGR8haynLBcY9sZa541bq7raAv5FWnL3ymybPw0K9CoMhm8za8UxtJztyN83kB8879RpXjMsWvFuplbxaN5zYU3lL2twGVxFba2tg5sdm7QpxXcI6yDRuSECFlhtv3bC5wB0HBu+7fO5ukOgEUP5atp7OG2fyuVqNhfZo1XTRssNe+FzmuaNJGxuY4jQnqcOxQLwdtvNrdo3Q5PI47H0MDPR+8mMyeO2bsfMRyWWMfI7m6T5W2yxrgCGiPi/g9wXaqQ8Nf3st/WdP6M6u9Uh4a/vZb+s6f0Z0HFCIiAiIgyMbJuysP+0O48FGL9lrbUu+NTvu0B8zuIAPnUigOjmnzOH81pdqccW5Hc46Pe0/w/lwVVp1b+pasPNJ/mGmz7CN3UabwLg0fitB0APnV9cglFsuGYHNa4OfK0tcAWuGuhBBVLbew7s5A6gxgA/3QT/FXt4OsumEicSP9NZHc8/8iFlzz7Y4n8tmGPXKlFfH5Spo2hPFJA3qgugybjexkc7CJAwceB1WRNf2hA408W3h7szTOA9e6Wg/xX7lzXNuOvHQ9Q/gFifZeWwSDoG+YevznzrD7PS9Y0j8mOt3LbJr8kU3MODmwwNLK0b9NA47x3pXjznh5ldOxI0h07XNJ0+LsVOZSC9EHeKTxtDnBx5yPfcPO1vEag/wWy2Ry+RJbG953xqC8tIGnZoNVKJ5R/Tj4S3P1btK1LdxrYXCx+FVZSWRzvj0aydj2683Y3eB7DoNV+ae1WSeQPsHK49pjswmPX43aFbLZXFWmB5t3Dba92rWOY2MMGnV5Hux8a120VezSfz9cc7DxLovxmjzt+pSm89ofp1mdbZF+HNZBrWOhr4qDtk5zxu2R27jGgRsOn4x1VUeEti4KeIpwV26NZbBcTxfI9zHF0kjvxnk8dVb2A2mZZjGh4jgQRoQfWOsKrvCkcDiWOP5YzTz8I5NdFKkxN40hlp60nbnLFDTR3rU52Xslksbj1EmN3rbKOB+LeDVCMaev1aKTY15B16tAx3zXg/8it12PHHDKvRbkj29Wjj3HiP4LxW52yjDbcm71E69/lfyIWmWis7h5t41aYERF1EREQEREEz5CvfNgv1nW+kvomvnZyFe+bBfrOt9JfRNB52oGyxvieN5kjHMc3zteC1w+UErknku2xm5Mprmzu0tW4cLJdlmxmbgiM1dzJW8Ypd3qcWxB5Yzeex7n6tLHNeOul+ZGBwLXAOB6wRqD8YPWgrbk05cdn9o778fiJ7NmSOq+w+Y15YK7Gskij5svnDXc64y6gbuhEb+PAA094Xd6IbUbO1tpJLkWxL60r7Pi/jAr2MiDaIjs+J/fXlvN0CAAXNbJKWabzyOqYYWMG6xrWN8zQGjuCTRNeC17WvaetrgHA6dWoPBBx54PdjCv5R5X7P0ZKGIk2dlFTnI5oBdDZ64kuwss/fDE97JGhx6+YOuh1AnH9of70of1zT/AMNdXRoaPMOrT5PN8S/pAPWg/MXuR8Q/kuQ/CgzvJ/btZZ89i7jNrcW18cFqlDahns3K8DXVQ6SJpglj3ubj5yR0bw1mjXhoaV18vN9eNzmvcxhe33Li0Fzf7rjxCCC+Dvay82zGHlzolGTdWdz3PjdndGJ5W1Xzg8RO6qIHO3vK3nO3tDqFyJhNkdj8Dlczi9vsfdil8fnmx2V1yHit2hIWljWDHu8tw1D94NdoZZGOLTHurvlec8DJBuyMa9uuujwHDUduh7UFQ+C9jNj21chd2QhnjrWLMVexLMbmk8lON0kbom33FwjAuyDUBup3gfchRHa7/wAYsB/7am//ADS6Pa0AAAAADQAcAAOoAdgX907e1Bzfyn/+LOxn6pv/AOHzK6QTQIgxM3fbUrWLT2SyMrQTTOjhY6WZ7YY3SOZDEzypZSGkBo4kkAda5o5UPCM2MzWBu0X17d67bryRQYienILLb8jTHXHO6OihlZM5p343uc3dJaHHQHqJeQrx7/ObjOc00390b+nm3uvRBWfgqbO38VsjiKWTY+K2yOeR0EuvOV47FqaeCCQO4se2KRmrDxaSW/iqncptnS2T5UNochnTNTo5TEUxVsiKWdszo4MXGd1sDXOLecq2WE6cDFx6wV1mvOWFj90vY1xadWlwDt0+duvUUEE8I33o7R/qe/8A4d6w/Ba95uz36uj+k9WWUCDm/wAKX348mv62tf4nEKz/AAkfejtH+qLv/wALlYJAQhBQOyv/AISSf+zsn/gLS3vgXn/MbA/3L/8AxS8rh0QBByZshtbHyebSbT1toYLMWMzmRdkKGVhhfPA/nJLEroJHR6l0gbM1paBq10LyRuva5frkH2mOY5Us7kRUtU4bGzusENyMwWXVo5cLFFYkidxY2UMMjf8AYkZ19ZtTlAk5Q4clYfg49mrmKk5nmIsh4yy3XLYWNl5x0Lo2vDpw9w4u8kgcNF5cg/JdksbkcttFtDbrXM/mdxknibSKtOpHuBlaF72tc/yYq7TqOArMGrzq4hCfDh2QdZfs/mpqE2UxOJsWG5SnWMgseI2TWc6eMxOa5oaIJNXbwALo9SG7xEX2Aocj1zJ4yDFQ2pMlPaiNePezDTFPGeeYZXTuEW6HRjXQuHyaldgLyhrRsJLI2MLustaGl3xkDigrnwpPebtD+rpPpMWZ4OQ/zR2c/U9D/wCBin5QICpDw1/ey39Z0/ozq71SHhr+9lv6zp/RnQcUIiICIiAFmbQxiXJQuHA+LOePMdIXBw+PXTvWGtjK0OdUm0OsTJWuPnY4cdNewf8ANUZviWvxZ7j+J/41G3tUOLJdPdMZ1+ZzBoe9WB4N+SDqFqpr5dezzm7281O0AEDzbzNPlUb2nrb9KJ3buPYfjjeS0/NP8FEthto3YjIMsjUwvHNzsH40TuJ0/wBpp0I+IrPWPfHMfZsv9N4l0vZrs3hvs1EnEOHnb1tPrVb7Vz5XETiauGT0JXjg8EvhLneVq70dVZtG/HNCxzHCSN+7JG4cQQ7tB7x8iyZK8ckbo3tDo3DqOhGh7Vip9NuW6s7hFqzc3MGyQR1po3RNlHkgktLg0jXX3QKmeFx+fD2xux9LeI1bJvgN4aa6kdvEKGvdZxera8s0cJDg0xkENDjvFu64EDipFjuUaw50W9NputLXfeg7eJ08o8eB4dinEV3trjBnmu6+swk8rM4wb0goxNDHO/GdoAdNCRoAeKh+Agy2Xe+W7KyOlGZA1kDXRvme06RlshOu6ANT1cTopI11jIkME0zoPx3yeTrqd4tDG/ia/wAlMYKrI4hGwBrGADhw4D612aqLx+nH1a3+ELw2NbDJIGNIJ3dS7j5WnYf4qpfCxyjWx46kD5b5ZZnDzMa3m2E/G5zu5XftBfhqxyzyOEcbGlznO4ANaCSVxXylbVvzGTmuHUR67kDD+LCzUN4ekeLj8as8XHO9/Zg8rLqIj7sXEAbxHmb/ACKk9AaMkcepkI+dv8Ao9imeVr52n+OhW5uuczdb/q3tYNfXqdTqtVlVeki2onZK6GRn40TdfWQANf4fwWnWVfA0j09zzbQO0eTw6/i0PyrFWinTzsv75ERFJWIiICIiDP2ey89C3Xu1nBlirKyWJzmh7WyMOrSWO4OHqKs37o7az8trfssHsqoidOJ4AdvmCn/Ixsxi8tJlK9+W2y1Fjp56EVXd1mnrRzS2Gu32kOka2OPdjJbvB0vHVo0DffdHbWfltb9lg9lPujtrPy2t+yweyqz2awN7JO3MfStXnDTe8UiknazeG8OcfG0tiBHa4gLcZ3k7z1GMy28PkYYgCXS8w+SNgA1LpJIg5sTdO1xCCafdHbWfltb9lg9lPujtrPy2t+yweyqiaQRqOIPaOpf1Bbn3R21n5bW/ZYPZT7o7az8trfssHsqo0QW590dtZ+W1v2WD2U+6O2s/La37LB7KqNEFufdHbWfltb9lg9lPujtrPy2t+yweyqjRBbn3R21n5bW/ZYPZT7o7az8trfssHsqo0QW590dtZ+W1v2WD2U+6O2s/La37LB7KqNEFufdHbWfltb9lg9lPujtrPy2t+yweyqjRBbn3R21n5bW/ZYPZT7o7az8trfssHsqo0QW590dtZ+W1v2WD2U+6O2s/La37LB7KqNEFufdHbWfltb9lg9lPujtrPy2t+yweyqjRBbn3R21n5bW/ZYPZT7o7az8trfssHsqrsNjLN2eOrUgls2ZiRHBC0vkeWtc9260dgY1zieoBpJ4BeFqB8UkkUrHRyxPfHJG8Fj45I3Fkkb2O4se1zS0g8QQQgtj7o7az8trfssHsp90dtZ+W1v2WD2VUaILc+6O2s/La37LB7KfdHbWfltb9lg9lVGiC3PujtrPy2t+yweyn3R21n5bW/ZYPZVRogtz7o7az8trfssHsrQbe8r2dzlTxLI2IZa4lZLusgihdzkYcGnfYNdPKPBQJEBERAREQF62cgI2Dt0DWEeqR+p+XdC8lg5CJ7nkNBOrouoa6Ag6AedxOvBU5tajbV4vc6+yWukbLRLdeLZHNGvHjGGl38HDuKrDOQ7r3DqGuvr4/9VMs/cFSGtA3jJXk++6HrdONZAdPNqB8ii20PlaOGmg+j2fxJWfDGpmfu25Z3XSV8jm3TqjxQsuJrPcebeeuB56wP/KPm7F0bh5WOaOIIIB17CD2g+ZcZ4bhZh7NZGj53D/mugNgs3LBpC8l0YHk68TGeojj1tVPlUiLbg8a8zGvst0Y8EkHQt8x4jRZNLAQa6iKPs6gFosdtIwab3lD+IUhr5qA6FrtOrUFU118vSi+ukopUBG0ADsB4fyXhmLrYxprppxPxdQWsv7WRMG6w78jhwY3i7Xs4DqWNjIHyu5yfievc6wD2a+f4lK0xrUMu53yg/LLBJPiLxdvBroH6NOuu6OOrlyEdOc6uxx/hwXcvKjA12PtA9Rgk182m4T/AMlwtCfKdr2tdotPidTDJ5MxwkeJ9wwntGmvr0BH8j3KRQFsrRG7Q6B2nrGvEj4lG8Yf8mJ9HUjT1OB/7+NZtSY6BzTxHlAjsOv/AOlOe0460krWfejE8jVh1Y/zjTt9fq+pYJGnBZWNtCYNGmuujXNHXx4b7fWvzeYWniQSCRqBpvbp0B07D2fIrcVvhk8mnHsx0RFexiIiAiIgy8LkH1LVa3EI3S1LEFiNsrd+J0leVkzGyM1G/GXMAI1Go14jrXUfJPtXHdtT7TbSYjD4ZukbaOdlaaTrD7DHQGMSXJT4y7xaNzRYaBpHvN13SVyirK2tp5S5sdichazFeahWtyY+rjObYyesWNnibvSsAM8ogr7wjfxbC5rg7iQguvlPydK9lnYGrtZNsuKgPjNSOsKdWad7PGnysyTJINJHNmi3o3P3X7ri3U6688Y3b/OYy1I6nnL7+alkY2Qzy2K1hrJC1svi1ovjex4aHDebqA7sWRyxba1s9cr3IMc3HyMqQw2CJOefaliaGNkkfut3gyNrI2udq4tY3U8GgQuKNz3NYxrnve5rWsaC5z3uIa1rWji5xJAAHXqgtfP16e1OMuZipWio57FRifKU6rdypkaRJ5zJ1YifvU0ZDnSNBJ011Li6MmplbPI1Vk2d2prxZ9zMRG6lbbYbd3eamq2a0gjifOxxjjjdKxjt8kjerlh0ceFW5GCKKaaKCbxiCKaWOGxpu+MQRyOZDY3fxecja1+nZvaIJZgOS/N5DH18lSqCzBatuqxMjkZz7pGGQOkMbyGsgDopGl7nDTdJIDfKW92o5BdpMfUkuy1688ULHPlZUm56eKNgLpHuic1u+GgEkRl54E6aAqc4zL2aXJcJKsz68st2aAyxEskbFNlpGzNY8cWFzAWEjjo92mh4rW+BFae3N36jXaVp8XLPJD/q3zw3KMTJCzqL+bsTN17Q71IKf2Q2Zv5e02njqz7VhwLt1m61scYIDpZZXkMiiBcBvOI4kAakgGbbTchW0VCtLadBVtx1wXTMoTixNA1oLnOfC5rXO0AOoZvHrOmgJE/5CKdarsdtPaF44t770lOTJxwTW56dWKGmyINirETvcPHZyHMILTPvfilR7kfs7NbO5WHIxbYOljDJY7FVmDydcW4pI3hrHyavA3JjHKDunjFpw1KCt9mdichkaGSyVVkTqmJi52y58gY/c5t8ruZZ/rC2ONzj1cNNNTwWDshs/Zyt6tjqYY6zae9sYkdzcf3uKSd7nv0O60RxSO6ifJ4AnQK9eS6Wu/Z3lIfT4U3/AGVdVG6Y9Kjqt91Uc24Ax/eTH5JA06uxV34Mo/zuwn6S/wD8JyCDU7PcmuWv5a5ha8UPj9Fth0zZJWsiDa0scLyyXQh28+aIN8++CdADpmP5Is6zDyZyetHVoxQCwRZkEVp0Dt0iQV9CWEhwO5IWO9XEa3TyMjXlH2nH/wDGyP8AxHFKntlshNn9rcecrNJZZYzEbnRzOL4WsZMZI60cbvJjg8hsW40DyXHzoMrZTkH2kyNZlplaCrFKGui8el5iSVjgC17YWNe9gOvDnAwnr00IJh+3ex2SwdgVsnWdXkc1z436tkhnjadHPgmjJbIAS3Vvum77d4N3hrNfCvy1i3tPer2HudBQFSKvC4kxxNkpV7L5Gxk7olfJYeS8DUgMBJDApXtRYkyXJhXt3nuns0L4ZXsSkulcxt59MNMh4v0ryuZx115hpOpGqCG1OQraWWWtEypFpZqts88ZmCCvE7TRlqT8SbiPIYH68SNQ1xbj7Q8im0lK3VpuoeMyXXObDJUeJa7nsaXyNklfueLlrAXayhgIB0J0OljeGDk5xR2boiRwqz05ZpoQdGTSwx0WQGUfjhgklIB4av16wNP1QztyDkt34rEzJPGnVGyh7udZVflSx0LHk6sZzRdEAPcsdujQAaBXe2/IltBiKb79mCvLWiAMzqk3PPrs10MksbmtdzYOgLmbwb1nQAkRrYDYfJ52w6vjKxmdGGulle4RV67XahhmlfwBcWu0a3Vzt12gIaSLb8EY79Da2m7jVOOid4uf9EHTQZKKZwZ1Bz2NY1x7RGzXqC/WAty47kwfaovdBZyF97LFiE7soa68ajtHjizer1o49RxAmOmhOqCIZXkD2lr2Klc1q8otyuibZgm5yrDI2N8pFp5aHwN3I3+UWbpIDQS5zWmR8g3I9kHZuOxkaNWfG427fq22TPhnjdZhqTMj0ru156MWJa7gXDzHTgtD4JeVsVdpqdWBzm177LcdmFpIie2KlPajmcweTzjZa8YDyNQHvaD5Z1lOxhI5UpgCQDkcxqB1HTF3dNR2oIry38k+SxM2Synilevh3ZGbmOZliAigsTv8Wa2uCDHHulrQ1o8kacABw8NlOQjaTI1mW2VoKsMoa6Lx6XxeSVjgC17YWNe9gOvDnAwnrA0IJyxj4rvKFJWsjfgftNaLmP4seIrU0ojIPAte6MMI7Q8jtXn4WOXsW9pr1Ww5zq9AVY68DtTExslKCzJKIz5POvksSavA1LQxvU0IN3yGbHZLCbaY2rk6zq8j4Mi+N282SKaMUbLS+GaMlrwDpqNQ5u83eA1C88/yJ7QZjLZu7WrwwVpc1ljDJdlNc2Gi/ZG/DG1rpDHw4OcGhw0LSRxWN4Nmeu3dqcKy5bsWm1K+SjgFiR0vMxvo2HOYxzySGktb29TGjqaAIhy75qzb2jy8000pkpZG5BWcHuBqRU53wQ+LEHWu7SFr9WaHfJd1nVBo9sdmb2Itvo5CA17LGtdu6teySJ5cI5opGEtkiduu0I7WuB0LSBp1f/hkOMh2andxlmxtkvf2u08RkGvqDppD/vlUAgIiICIiAiIgIiICL+EryFmPh5QOp0GnHq9aD2W+2bhDo5nRxg2WaOjc/eMW+ODXafCgE6dnWtC+eGNhe92p6g0cOPXq7zNW62Eyhfxc4NjJc5kfAdQ0Gh7dNTr8ap8j9jR4s/7OECkrWHzWGycXh7eA6t7f6x3/AMUtVnboB46bzT8h1UqyEbIX2JOGhLn9fFxA1AHykLChqlsAe7T3MhPxuWeLfTEtvr9UwgVaUieMj8WRpHyOCv8AxDR5Lx+MNe9UDjG707f72v8AFXvsvMHRMHboP4da55fUQr8X5bqYnXgdPi4LaYSg+UgOe/TXsJHyLChaC4dqlOEGmnAfGsL0d8JDhcdFABo0A+fTVx+Ure1JtDoP+/8AqtXC8aedZcD+K7CtoeWzINr4TISuOhFZ7W+t8nkNHeVxDE06j18O8afzXTHhX7QbtCCi0+VPK179PQjB0BHx/wAlzQ46aL0vEr9Ey83y7fXEJJs5xiIPaXDT42u/5tX5pyji3XgOPyLPw8QjhDj5ie/eP/fxrQ1pPK17O/r7E7lfM6iISXAWWtmj3uHxeccQdfj0W0tv3nuPnPx/KocJD8W7rr8R8/rW1gyThug+U0Dq6j8fmB0VuOvLLnvvht0XlWsNk6jo70TwK9Aderj/ANOtWsz+oiICIiArQ5HpIcpj8nspPLHBLkJYr2Lnk0bG3MVmCM15HnXdFiBjIwQNQGyAaucwGr0BIIIJBBBBHAgg6ggjqIPagysvjrFOeWrbgkrWYHFksEw3ZI3Dzjqc0jiHNJa4EFpIIJm2y+xuLt4mO4NpcfjswLTwKeQmFKGKKIkskNloMkMhAbI2bTcBIZwcC4ZVPlclngjrZ/FUNo44GbkM9zerZOJhPFgyUILy3TTiW7xI1c5xX7q8pGFp/fMXsbja1lpDo58jct5tsUg4h7IbTWbrmniCHDQgFBeW2fJxJFJS2pybpcvkMPh6zbGKrxNlZksjUY5rZ2ySAlkPOzOlcxsROsQc0a6sdy9ygX5bmRs35aH2N8ek55lVsb4omt0EZMW+1vOAuY5zngAF7nnQa6C5MbtvtBnNlMzbfmTWuYfIQWxNA9lCeanzUj30j4o1rtznHNMfwjohG4kb2tT8oPKDk88KIycsczsfDJFFIyMRySc9zPOyzkHSSZ/MRakBrfJ4NGp1CRScoFM7FR7O81Y8ebfMpk3WeLcybb7e8H7+/veWGbu71gnXReXg7bd09nsvNevR2JIJMdYrgVmsfIJX2KdhhLZHsG4RVc3XXgXt7NSK4RBY3JHymjDuv1btMZDDZUPFulqN8F7XMMkJfo15MbixzXFu8BGQ5pZx38ea5O6bhar4rNZGUcY6N57G1I3aHRs7jIecZrw8rn/iPWqaRBaXI7yn1cRZy8d6gZcPmw9s9KtuuFZj3ThsUMcrmtkr8zZkiLS5pLWxkHVuhk+yPKDsZs/kIbGHxuUmMrjHYu3HB76lR7XbzKMDpPvkpkbEHF+6dzf0c4ndNDIgubk75VsfQ2tzGenhtmnkIrrIo4mxvsNMtqpYh5xhkDBq2q5p0cdHSN6xq4VDWuyxTsswuMU8U7J4nt0JimjlE0T26jQlr2tI1H4qx0QXpmOUHZDaIw29ocdkqmUihZHLNjHNdXtNYXbo8p+8OsnRzN5ocG848NBUU5XeUivkqNLCYelJjcJQ8qOGV29YsS7r2tksbr3jQc7K7QveXvkc9zidNK2RBZ3LzyhU883CCpFYjOPoyRT+MNY0c9KKwLYix7t9rfFz5R01328OvT8/bAp9CujvN2PHvshz3ObrPFuZ8a8a3t/f3978Td3evjroqzWdg8PbvTCvSq2Lc7gSIa0b5pN0EAvc2MEtjBc3Vx0A1GpCCwOQjlBp4GPONtxWJDkaEcUHMNY4CaIWmhspe9u413jIO8NdNx3Dq1/vJBylVMfj7mBzdOTIYS8d5zITpYrSnmw50Qc9mrC6OOQFr2OjfHvt1LlX+Xwl2nYFS3UtVrRLA2vPFJFM/nHbkfNxvaHSBzwWtLQQ48BqvfaHZnJY4RnIY+5SEw1jNqCWBr+GpDXSNALwOtvWOGoCC4tj+UPY7Z/IQz4jHZSfnSY7V665jpa1R7XEx0IN8B8hmbBvOfunca4AuJ0UCm5QOZ2rl2jpwlzfsjYsRwTkMe+vOySCSKQs3hHI+vLINRvBpcD5WnHQZbY/LU67bdvF5CtVfppYnrTRRDeIazfe9oEe8SAN7Te1GmqydktisjfNaZlC+7Hy3K8Et6GvJJBEySwyGaQShpYRHvO3ne5aWne0QTTlN2u2ZuyOzWIhzNDaB12vZAl5g0mzxSskkneN+Tj5G8NzTV+hLQC4LfZzlD2P2i5m3tBjclUykULI5Jca5robLWEkNBLw7TUkgPYHNDg3fcBqqqm2QuTZPIY7G1beQdRuXIPvETpn81WszQMlm5obsW8Ius6DU6BaxmFuunlqtpXHWoATNWbXmdZga0sDnTQNZzkTQZIwS4ADnGecILTwPKRs/S2hxF7H4aXHYvHQW4ZC3dlyFs2q74mz2QZCJHRuI65Xu3XvOp8lgrLbjJsv5LJ3YmuZHev3rEbZNOcZHasyzRtkDSQHhrxqASNddCetbH7X2e59tb7C5Tn3R86IvFZ97mvTPk6BuvDj2kDr4LST4u1GySSSrajjhnNeWSSGVkcNoAuNWV7mhsVndBPNOIdoCdOCCw+XflBp55mDbUisRnHUHxT8+1jQZpRVBbEWPdvtb4ufKOmu+3h16ViveelNHHFLJDNHFYDzDLJG9kVhsbtyR0EjgGzNa/ySWE6HgdCvBAREQEREBERAREQabaK5ujcB09Y69fV/DvWpjsHgG9fV6gF/M/Lq/XXrcf5rDZLuhx146KcDLsWt7SMHgDu6nj63vcV+8blJGyksJDN3cb2kNPm8xOnWtNXl1LhrxcNAfl1K9Y3bnDXQjqPxqNoi3btZ1PCWW8sJd1rzpx8vT3Pk6cB8iyMrk2FkrA4bkdfhp6bzx4KEyTcAA4jh2LwMp48esaH18dVROGJX1z2hssDWPONd3d/Wra2dk5ssb+K4Aj4/xlG9lMGXMjOmp3W9+mqnVDEuMLtPdRkEefdPB2g9X/NZM94mW3DjmIb6GUaArf4e0AfiUfxVVz2DgdQANPWFuadJ0Z8oEf8AfUse2rSWVLOoHbwWTLbEbC8nqBWBjYfJ10Wq21vGGB3rB4eddjmXLdKB5ecsbN0En3PADsA86rcjXTTt4d6kvKE5xmaXnV0he4+oAgAdyjfVxHX/ANP5r1sNdY3j5p3kS3xoGu5gPuIgNfO48D3ALQteGkadmmgPAcO0heZtkRNZ8p83q19a8WO14k8V2lOeU7ZtxwyxMXb28eo6geYjj2LIhdw1HZ6+0rX+mfUD/BfurJwHrV0Rpm3921bZ00Pm7f8Ar1r8bKXT4zNGSd2QvcNfTDiT3t17gsNzl47O/h0f9+T6D12RPERFEEREBERAREQflzASCQCR1EjiNevQ9i/SIgIiICIiAiIgIiICIiArC5M8UZsVnp3OyVisz7Gw2MVieabavNmmmdE6zNJBM+vQY+M73NsO+XaHg3jXqycbkLFZ/O1rE9aXdLecrSyQSbjtN5m/E4O3ToOGunAIOiMPUdDY2O1oT4yf7DbSR0K1yWSzJWyUjp34yGWzOyPcndG4uZE9rDGZGMDQWgKtthcBnDBBXnldi6VzaHEsD8pE9lg5Yvkc21Tgts1knjYXc6XFoeXRscXcQK/kvzuYInTzuibK+YROkkdG2eQl0k7Yyd0TuJJLwN468Sv1k8lZtOa61Zs2nMaWsdZlksOYw6EtY6ZzixuoHAcOAQXfTxT9zbUxYrPCZ+JyTJsrlpi6TI3BbhLWMoQVYYS883LICx0piY0AbokGvhlaGRm2j2Vs4mO2/Gx08D4jYgbJ4rWpRiIZJksw+9wkOjtc8xxDiNA4HVoNPSZ++6Rkrr950scb4mSuszuljhkAEkLJC/eZE4NaCwHQ6DULxrZW1FEa8dqzHXLw8145pWQGRpBa8wtcGF4LWkO011A8yC685VpWMVmoXVcxc3dtsy69BhpIo5+Mkwx77sc1axztHdEgb5IaJQ78ZZE9+WG7dLI7lK5S5OLjC+1ZbYyjd18T6sl6WGOLmMi2F7NRuh7RzROh00oyllLUErp4LVmCd+9vTwTSwzP3zvP35Y3B7t53E6nieteQtSh0jxLLvzB4lfvu3pmyHWRsrtdZGuIBIdrr2oJztJalbsTiImySNi+zOXfuNc4N34Y60sTgAfdNklkeD2OeT18VPdrmvvbRbY4Bo3pctUpz02Odu7+YxeNp36zGl3ktM0fjTHOOmurdfVQz5nljYy95jaXObGXExtc8APc1hO61xAGpA46DzKS7D7Usx1t+TmisXMlC3WlLJOBBFYMM0Bnutex8tsRh8LmMa+PjDoSQRuhnctNyM5PxCu/fq4OrWxULgSQ80WFtuYjq333H2SSOsNb6tISjnEklznOcSSXOJc5zjxLnOPFzidSSevVEBERAREQEREBERB5GvGetjD8bQf8Akni0fwcfzW/UvVEHj4rF8FH81v1L++LR/Bx/Nb9S9UQePisXwcfzW/UnisXwUfzW/UvZEHpFM9nuXvb/AHXFv8l6NvzjqnmGvmkeOHm4HqWOi5qHfaWUzI2W+5sTj4pZB/Ir9HK2z12rJ/8A7pPaWGiesHtLObmbg6rlsfFPKP5OXnPkbEn+ksTyf35ZH/SJWKiesHtL8TRtedXta8+dwDj3lfjxaL4OP5rfqXsi648fFovg4/mt+pf3xaP4OP5rfqXqiDy8Wj+Dj+a36k8Wj+Dj+a36l6og8vF4/g2fNb9SMrxg6iNgI7Q0Aj5QF6ogIiICKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toJmihnSux6EPc/206V2PQh7n+2gmaKGdK7HoQ9z/AG06V2PQh7n+2gmaKGdK7HoQ9z/bTpXY9CHuf7aCZooZ0rsehD3P9tOldj0Ie5/toNAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiD/2Q==\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6kxIuMjK54o"
      },
      "source": [
        "# Summary of Video 3:\n",
        "\n",
        "**How do we train this neural network so that it optimally predicts stimuli from neural responses?**\n",
        "\n",
        "- First, we need to define a loss function that we are optimizing the network to minimize.\n",
        "In this case, we want to minimize the squared error between the prediction and the stimulus.\n",
        "First, we're going to run the neural responses through the network 'net', to get this out.\n",
        "Then, we're going to declare our loss function - to create a mean squared error loss function.\n",
        "- This loss function takes two inputs -\n",
        "which is the output of the network and in this case ori which is the original,\n",
        "true orientation that the mouse saw on the screen.\n",
        "Once we've defined this loss function we minimize this loss function using gradient descent.\n",
        "- In gradient descent we compute the gradient of the loss function with respect to each parameter,\n",
        "which in this case is all of our w's and our b's.\n",
        "We then update the parameters by subtracting a learning rate alpha times our gradient\n",
        "with respect to the loss function L.\n",
        "- Let's visualize this loss function with respect to a weight w -\n",
        "if the gradient is positive, then we want to move in the opposite direction\n",
        "to minimize this loss, which is in the negative direction. So we accordingly update\n",
        "w in this negative direction - in the opposite direction of the gradient.\n",
        "Once the iterations of the training are complete, ideally the weight will be at a value that minimizes this cost function.\n",
        "In reality these cost functions are not convex \n",
        "and they depend on hundreds and thousands or millions of parameters.\n",
        "- There are tricks to help navigate this rocky cost landscape such as adding momentum or changing the optimizer.\n",
        "There's also another way that you can change the architecture of the network to improve optimization such as including skip connections,\n",
        "and these connections are used in residual networks to allow for the optimization of these really deep networks.\n",
        "In our case we're working with\n",
        "a single hidden layer network. So we're going to stick with\n",
        "standard gradient descent in PyTorch. You can create this optimizer called optim.sgd\n",
        "or stochastic gradient descent to do this optimization,\n",
        "and the optimizer object takes as input the parameters net.parameters\n",
        "and it takes also as input the learning rate.\n",
        "- So on each iteration, we're going to run the training data\n",
        "through our network 'net' and compute the loss function.\n",
        "Then, once we have the loss function, clear our gradients\n",
        "and compute our gradients using this loss.backward method.\n",
        "This loss.backward method is also known as automatic differentiation,\n",
        "and this is an incredibly powerful tool.\n",
        "You can use this backward step for any model and any loss function that you can define in PyTorch, not just deep networks.\n",
        "So I highly recommend using it when you're fitting complex models rather than computing gradients using pen and paper.\n",
        "It's much less error prone to do it this way and it will save you a lot of time.\n",
        "- Finally, once you've computed the gradients using this automatic differentiation that's built into PyTorch,\n",
        "we update the parameters using this step method of the optimizer\n",
        "and then we repeat this method or this loop\n",
        "for many iterations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo67r6l15X1i"
      },
      "source": [
        "### Section 3.1: Loss functions\n",
        "\n",
        "Because the weights of the network are currently randomly chosen, the outputs of the network are nonsense: the decoded stimulus orientation is nowhere close to the true stimulus orientation. We'll shortly write some code to change these weights so that the network does a better job of decoding.\n",
        "\n",
        "But to do so, we first need to define what we mean by \"better\". One simple way of defining this is to use the squared error\n",
        "\\begin{equation}\n",
        "    L = (y - \\tilde{y})^2\n",
        "\\end{equation}\n",
        "where $y$ is the network output and $\\tilde{y}$ is the true stimulus orientation. When the decoded stimulus orientation is far from the true stimulus orientation, $L$ will be large. We thus refer to $L$ as the **loss function**, as it quantifies how *bad* the network is at decoding stimulus orientation.\n",
        "\n",
        "PyTorch actually carries with it a number of built-in loss functions. The one corresponding to the squared error is called `nn.MSELoss()`. This will take as arguments a **batch** of network outputs $y_1, y_2, \\ldots, y_P$ and corresponding target outputs $\\tilde{y}_1, \\tilde{y}_2, \\ldots, \\tilde{y}_P$, and compute the **mean squared error (MSE)**\n",
        "\\begin{equation}\n",
        "    L = \\frac{1}{P}\\sum_{n=1}^P \\left(y^{(n)} - \\tilde{y}^{(n)}\\right)^2\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf2kaAH25X1i"
      },
      "source": [
        "#### Exercise 2: Computing MSE \n",
        "\n",
        "\n",
        "Evaluate the mean squared error for a deep network with $M=20$ rectified linear units, on the decoded orientations from neural responses to 20 random stimuli."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xogNRxcI5X1i"
      },
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize a deep network with M=20 hidden units\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Get neural responses to first 20 stimuli in the data set\n",
        "r, ori = get_data(20, resp_train, stimuli_train)\n",
        "\n",
        "# Decode orientation from these neural responses\n",
        "out = net(r)\n",
        "\n",
        "###################################################\n",
        "## TO DO for students: evaluate mean squared error\n",
        "###################################################\n",
        "\n",
        "# Initialize PyTorch mean squared error loss function (Hint: look at nn.MSELoss)\n",
        "loss_fn = ...\n",
        "\n",
        "# Evaluate mean squared error\n",
        "loss = ...\n",
        "\n",
        "# Uncomment once above is filled in\n",
        "# print('mean squared error: %.2f' % loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrNDMqjk5X1i"
      },
      "source": [
        "# to_remove solution\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize a deep network with M=20 hidden units\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Get neural responses to first 20 stimuli in the data set\n",
        "r, ori = get_data(20, resp_train, stimuli_train)\n",
        "\n",
        "# Decode orientation from these neural responses\n",
        "out = net(r)\n",
        "\n",
        "# Initialize PyTorch mean squared error loss function (Hint: look at nn.MSELoss)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Evaluate mean squared error\n",
        "loss = loss_fn(out, ori)\n",
        "\n",
        "print('mean squared error: %.2f' % loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJtVJh4f5X1j"
      },
      "source": [
        "You should see a mean squared error of 42943.75."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdLKIRx75X1j"
      },
      "source": [
        "---\n",
        "### Section 3.2: Optimization with gradient descent\n",
        "\n",
        "Our goal is now to modify the weights to make the mean squared error loss $L$ as small as possible over the whole data set. To do this, we'll use the **gradient descent (GD)** algorithm, which consists of iterating three steps:\n",
        "1. **Evaluate the loss** on the training data,\n",
        "```\n",
        "out = net(train_data)\n",
        "loss = loss_fn(out, train_labels)\n",
        "```\n",
        "where `train_data` are the network inputs in the training data (in our case, neural responses), and `train_labels` are the target outputs for each input (in our case, true stimulus orientations).\n",
        "2. **Compute the gradient of the loss** with respect to each of the network weights. In PyTorch, we can do this with one line of code:\n",
        "```\n",
        "loss.backward()\n",
        "```\n",
        "This command tells PyTorch to compute the gradients of the quantity stored in the variable `loss` with respect to each network parameter using [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). These gradients are then stored behind the scenes (see appendix for more details).\n",
        "3. **Update the network weights** by descending the gradient. In Pytorch, we can do this using built-in optimizers. We'll use the `optim.SGD` optimizer (documentation [here](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)) which updates parameters along the negative gradient, scaled by a learning rate (see appendix for details). To initialize this optimizer, we have to tell it\n",
        "  * which parameters to update, and\n",
        "  * what learning rate to use\n",
        "\n",
        "  For example, to optimize *all* the parameters of a network `net` using a learning rate of .001, the optimizer would be initialized as follows\n",
        "  ```\n",
        "  optimizer = optim.SGD(net.parameters(), lr=.001)\n",
        "  ```\n",
        "  where `.parameters()` is a method of the `nn.Module` class that returns a [Python generator object](https://wiki.python.org/moin/Generators) over all the parameters of that `nn.Module` class (in our case, $\\mathbf{W}^{in}, \\mathbf{b}^{in}, \\mathbf{W}^{out}, \\mathbf{b}^{out}$).\n",
        "  \n",
        "  After computing all the parameter gradients in step 2, we can then update each of these parameters using the `.step()` method of this optimizer,\n",
        "  ```\n",
        "  optimizer.step()\n",
        "  ```\n",
        "  This single line of code will extract all the gradients computed with `.backward()` and execute the SGD updates for each parameter given to the optimizer. Note that this is true no matter how big/small the network is, allowing us to use the same two lines of code to perform the gradient descent updates for any deep network model built using PyTorch.\n",
        "\n",
        "Finally, an important detail to remember is that the gradients of each parameter need to be cleared before calling `.backward()`, or else PyTorch will try to accumulate gradients across iterations. This can again be done using built-in optimizers via the method `zero_grad()`, as follows:\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "Putting all this together, each iteration of the GD algorithm will contain a block of code that looks something like this:\n",
        "```\n",
        "Get outputs from network\n",
        "Evaluate loss\n",
        "\n",
        "# Compute gradients\n",
        "optimizer.zero_grad()  # clear gradients\n",
        "loss.backward()\n",
        "\n",
        "# Update weights\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "In the next exercise, we'll give you a code skeleton for implementing the GD algorithm. Your job will be to fill in the blanks.\n",
        "\n",
        "For the mathematical details of the GD algorithm, see the appendix. Note, in particular, that here we using the gradient descent algorithm, rather than the more commonly used *stochastic* gradient descent algorithm. See the appendix for a more detailed discussion of how these differ and when one might need to use the stochastic variant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9KseDde5X1j"
      },
      "source": [
        "#### Exercise 3: Gradient descent in PyTorch\n",
        "\n",
        "Complete the function `train()` that uses the gradient descent algorithm to optimize the weights of a given network. This function takes as input arguments\n",
        "* `net`: the PyTorch network whose weights to optimize\n",
        "* `loss_fn`: the PyTorch loss function to use to evaluate the loss\n",
        "* `train_data`: the training data to evaluate the loss on (i.e. neural responses to decode)\n",
        "* `train_labels`: the target outputs for each data point in `train_data` (i.e. true stimulus orientations)\n",
        "\n",
        "We will then train a neural network on our data and plot the loss (mean squared error) over time. When we run this function, behind the scenes PyTorch is actually changing the parameters inside this network to make the network better at decoding, so its weights will now be different than they were at initialization.\n",
        "\n",
        "\n",
        "**Hint:** all the code you need for doing this is provided in the above description of the GD algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccGh06mO5X1j"
      },
      "source": [
        "def train(net, loss_fn, train_data, train_labels, n_iter=50, learning_rate=1e-4):\n",
        "  \"\"\"Run gradient descent to opimize parameters of a given network\n",
        "\n",
        "  Args:\n",
        "    net (nn.Module): PyTorch network whose parameters to optimize\n",
        "    loss_fn: built-in PyTorch loss function to minimize\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    n_iter (int): number of iterations of gradient descent to run\n",
        "    learning_rate (float): learning rate to use for gradient descent\n",
        "\n",
        "  Returns:\n",
        "    (list): training loss over iterations\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize PyTorch SGD optimizer\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Placeholder to save the loss at each iteration\n",
        "  track_loss = []\n",
        "\n",
        "  # Loop over epochs (cf. appendix)\n",
        "  for i in range(n_iter):\n",
        "\n",
        "    ######################################################################\n",
        "    ## TO DO for students: fill in missing code for GD iteration\n",
        "    raise NotImplementedError(\"Student exercise: write code for GD iterations\")\n",
        "    ######################################################################\n",
        "\n",
        "    # Evaluate loss using loss_fn\n",
        "    out = ...  # compute network output from inputs in train_data\n",
        "    loss = ...  # evaluate loss function\n",
        "\n",
        "    # Compute gradients\n",
        "    ...\n",
        "\n",
        "    # Update weights\n",
        "    ...\n",
        "\n",
        "    # Store current value of loss\n",
        "    track_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
        "\n",
        "    # Track progress\n",
        "    if (i + 1) % (n_iter // 5) == 0:\n",
        "      print(f'iteration {i + 1}/{n_iter} | loss: {loss.item():.3f}')\n",
        "\n",
        "  return track_loss\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize network\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Initialize built-in PyTorch MSE loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Run GD on data\n",
        "#train_loss = train(net, loss_fn, resp_train, stimuli_train)\n",
        "\n",
        "# Plot the training loss over iterations of GD\n",
        "#plt.plot(train_loss)\n",
        "plt.xlim([0, None])\n",
        "plt.ylim([0, None])\n",
        "plt.xlabel('iterations of gradient descent')\n",
        "plt.ylabel('mean squared error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2dEpsLz5X1j"
      },
      "source": [
        "# to_remove solution\n",
        "\n",
        "def train(net, loss_fn, train_data, train_labels, n_iter=50, learning_rate=1e-4):\n",
        "  \"\"\"Run gradient descent to opimize parameters of a given network\n",
        "\n",
        "  Args:\n",
        "    net (nn.Module): PyTorch network whose parameters to optimize\n",
        "    loss_fn: built-in PyTorch loss function to minimize\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    n_iter (int): number of iterations of gradient descent to run\n",
        "    learning_rate (float): learning rate to use for gradient descent\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize PyTorch SGD optimizer\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Placeholder to save the loss at each iteration\n",
        "  track_loss = []\n",
        "\n",
        "  # Loop over epochs (cf. appendix)\n",
        "  for i in range(n_iter):\n",
        "\n",
        "    # Evaluate loss using loss_fn\n",
        "    out = net(train_data)  # compute network output from inputs in train_data\n",
        "    loss = loss_fn(out, train_labels)  # evaluate loss function\n",
        "\n",
        "    # Compute gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store current value of loss\n",
        "    track_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
        "\n",
        "    # Track progress\n",
        "    if (i + 1) % (n_iter // 5) == 0:\n",
        "      print(f'iteration {i + 1}/{n_iter} | loss: {loss.item():.3f}')\n",
        "\n",
        "  return track_loss\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize network\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Initialize built-in PyTorch MSE loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Run GD on data\n",
        "train_loss = train(net, loss_fn, resp_train, stimuli_train)\n",
        "\n",
        "# Plot the training loss over iterations of GD\n",
        "with plt.xkcd():\n",
        "  plt.plot(train_loss)\n",
        "  plt.xlim([0, None])\n",
        "  plt.ylim([0, None])\n",
        "  plt.xlabel('iterations of gradient descent')\n",
        "  plt.ylabel('mean squared error')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMDgTGfo5X1k"
      },
      "source": [
        "---\n",
        "# Section 4: Evaluating model performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zfNsgiu5X1k"
      },
      "source": [
        "## Section 4.1: Generalization performance with test data\n",
        "\n",
        "Note that gradient descent is essentially an algorithm for fitting the network's parameters to a given set of training data. Selecting this training data is thus crucial for ensuring that the optimized parameters **generalize** to unseen data they weren't trained on. In our case, for example, we want to make sure that our trained network is good at decoding stimulus orientations from neural responses to any orientation, not just those in our data set.\n",
        "\n",
        "To ensure this, we have split up the full data set into a **training set** and a **testing set**. In Exercise 3, we trained a deep network by optimizing the parameters on a training set. We will now evaluate how good the optimized parameters are by using the trained network to decode stimulus orientations from neural responses in the testing set. Good decoding performance on this testing set should then be indicative of good decoding performance on the neurons' responses to any other stimulus orientation. This procedure is commonly used in machine learning (not just in deep learning)and is typically referred to as **cross-validation**.\n",
        "\n",
        "We will compute the MSE on the test data and plot the decoded stimulus orientations as a function of the true stimulus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "p_0DmrSE5X1k"
      },
      "source": [
        "#@title\n",
        "#@markdown Execute this cell to evaluate and plot test error\n",
        "\n",
        "out = net(resp_test)  # decode stimulus orientation for neural responses in testing set\n",
        "ori = stimuli_test  # true stimulus orientations\n",
        "test_loss = loss_fn(out, ori)  # MSE on testing set (Hint: use loss_fn initialized in previous exercise)\n",
        "\n",
        "plt.plot(ori, out.detach(), '.')  # N.B. need to use .detach() to pass network output into plt.plot()\n",
        "identityLine()  # draw the identity line y=x; deviations from this indicate bad decoding!\n",
        "plt.title('MSE on testing set: %.2f' % test_loss.item())  # N.B. need to use .item() to turn test_loss into a scalar\n",
        "plt.xlabel('true stimulus orientation ($^o$)')\n",
        "plt.ylabel('decoded stimulus orientation ($^o$)')\n",
        "axticks = np.linspace(0, 360, 5)\n",
        "plt.xticks(axticks)\n",
        "plt.yticks(axticks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGXJwrE5X1l"
      },
      "source": [
        "**PyTorch Note**:\n",
        "\n",
        "An important thing to note in the code snippet for plotting the decoded orientations is the `.detach()` method. The PyTorch `nn.Module` class is special in that, behind the scenes, each of the variables inside it are linked to each other in a computational graph, for the purposes of automatic differentiation (the algorithm used in `.backward()` to compute gradients). As a result, if you want to do anything that is not a `torch` operation to the parameters or outputs of an `nn.Module` class, you'll need to first \"detach\" it from its computational graph. This is what the `.detach()` method does. In this hidden code above, we need to call it on the outputs of the network so that we can plot them with the `plt.plot()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4huCPa7E5X1l"
      },
      "source": [
        "---\n",
        "## (Bonus) Section 4.2: Model criticism\n",
        "\n",
        "Please move to the Summary and visit this section only if you have time after completing all non-bonus material! \n",
        "\n",
        "Let's now take a step back and think about how our model is succeeding/failing and how to improve it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GBi9TYQT5X1m"
      },
      "source": [
        "#@title\n",
        "#@markdown Execute this cell to plot decoding error\n",
        "\n",
        "out = net(resp_test)  # decode stimulus orientation for neural responses in testing set\n",
        "ori = stimuli_test  # true stimulus orientations\n",
        "error = out - ori  # decoding error\n",
        "\n",
        "\n",
        "plt.plot(ori, error.detach(), '.')   # plot decoding error as a function of true orientation (make sure all arguments to plt.plot() have been detached from PyTorch network!)\n",
        "\n",
        "# Plotting\n",
        "plt.xlabel('true stimulus orientation ($^o$)')\n",
        "plt.ylabel('decoding error ($^o$)')\n",
        "plt.xticks(np.linspace(0, 360, 5))\n",
        "plt.yticks(np.linspace(-360, 360, 9))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-LsTqcR5X1m"
      },
      "source": [
        "### Think \n",
        "\n",
        "In the cell below, we will plot the *decoding error* for each neural response in the testing set. The decoding error is defined as the decoded stimulus orientation minus true stimulus orientation\n",
        "\\begin{equation}\n",
        "  \\text{decoding error} = y^{(n)} - \\tilde{y}^{(n)}\n",
        "\\end{equation}\n",
        "\n",
        "In particular, we plot decoding error as a function of the true stimulus orientation.\n",
        "\n",
        "\n",
        "  * Are some stimulus orientations harder to decode than others?\n",
        "  * If so, in what sense? Are the decoded orientations for these stimuli more variable and/or are they biased?\n",
        "  * Can you explain this variability/bias? What makes these stimulus orientations different from the others?\n",
        "  * (Will be addressed in next exercise) Can you think of a way to modify the deep network in order to avoid this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk5xmZ1BLqzu"
      },
      "source": [
        "**HINTS**:\n",
        "\n",
        "1. How does orientation bias error direction?\n",
        "2. To prevent this, think of trigoalgorithnometric solutions. \n",
        "3. How can you convert this classification problem into regression?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqwmsL95X1m"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "\"\"\"\n",
        "It appears that the errors are larger at 0 and 360 degrees. The errors are biased\n",
        "in the positive direction at 0 degrees and in the negative direction at 360 degrees.\n",
        "This is because the 0 degree stimulus and the 360 degree stimulus are in fact the\n",
        "same because orientation is a circular variable. The network therefore has trouble\n",
        "determining whether the stimulus is 0 or 360 degrees.\n",
        "\n",
        "We can modify the deep network to avoid this problem in a few different ways.\n",
        "One approach would be to predict a sine and a cosine of the angle and then taking\n",
        " the predicted angle as the angle of the complex number $sin(\\theta) + i cos(\\theta)$.\n",
        "\n",
        "An alternative approach is to bin the stimulus responses and predict the bin of the stimulus.\n",
        "This turns the problem into a classification problem rather than a regression problem,\n",
        "and in this case you will need to use a new loss function (see below).\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIEnKW7H5X1m"
      },
      "source": [
        "### (Advanced Bonus) Exercise 4: Improving the loss function \n",
        "As illustrated in the previous exercise, the squared error is not a good loss function for circular quantities like angles, since two angles that are very close (e.g. $1^o$ and $359^o$) might actually have a very large squared error.\n",
        "\n",
        "Here, we'll avoid this problem by changing our loss function to treat our decoding problem as a **classification problem**. Rather than estimating the *exact* angle of the stimulus, we'll now aim to construct a decoder that classifies the stimulus into one of $C$ classes, corresponding to different bins of angles of width $b = \\frac{360}{C}$. The true class $\\tilde{y}^{(n)}$ of stimulus $i$ is now given by\n",
        "\\begin{equation}\n",
        "  \\tilde{y}^{(n)} =\n",
        "  \\begin{cases}\n",
        "    1 &\\text{if angle of stimulus $n$ is in the range } [0, b] \\\\\n",
        "    2 &\\text{if angle of stimulus $n$ is in the range } [b, 2b] \\\\\n",
        "    3 &\\text{if angle of stimulus $n$ is in the range } [2b, 3b] \\\\\n",
        "    \\vdots \\\\\n",
        "    C &\\text{if angle of stimulus $n$ is in the range } [(C-1)b, 360]\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "We have a helper function `stimulus_class` that will extract `n_classes` stimulus classes for us from the stimulus orientations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvP8HAMG5X1m"
      },
      "source": [
        "To decode the stimulus class from neural responses, we'll use a deep network that outputs a $C$-dimensional vector of probabilities $\\mathbf{p} = \\begin{bmatrix} p_1, p_2, \\ldots, p_C \\end{bmatrix}^T$, corresponding to the estimated probabilities of the stimulus belonging to each class $1, 2, \\ldots, C$. \n",
        "\n",
        "To ensure the network's outputs are indeed probabilities (i.e. they are positive numbers between 0 and 1, and sum to 1), we'll use a [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to transform the real-valued outputs from the hidden layer into probabilities. Letting $\\sigma(\\cdot)$ denote this softmax function, the equations describing our network are\n",
        "\\begin{align}\n",
        "    \\mathbf{h}^{(n)} &= \\phi(\\mathbf{W}^{in} \\mathbf{r}^{(n)} + \\mathbf{b}^{in}), && [\\mathbf{W}^{in}: M \\times N], \\\\\n",
        "    \\mathbf{p}^{(n)} &= \\sigma(\\mathbf{W}^{out} \\mathbf{h}^{(n)} + \\mathbf{b}^{out}),  && [\\mathbf{W}^{out}: C \\times M],\n",
        "\\end{align}\n",
        "The decoded stimulus class is then given by that assigned the highest probability by the network:\n",
        "\\begin{equation}\n",
        "  y^{(n)} = \\underset{i}{\\arg\\max} \\,\\, p_i\n",
        "\\end{equation}\n",
        "The softmax function can be implemented in PyTorch simply using `torch.softmax()`.\n",
        "\n",
        "Often *log* probabilities are easier to work with than actual probabilities, because probabilities tend to be very small numbers that computers have trouble representing. We'll therefore actually use the logarithm of the softmax as the output of our network,\n",
        "\\begin{equation}\n",
        "    \\mathbf{l}^{(n)} = \\log \\left( \\mathbf{p}^{(n)} \\right)\n",
        "\\end{equation}\n",
        "which can be implemented in PyTorch together with the softmax via an `nn.LogSoftmax` layer. The nice thing about the logarithmic function is that it's *monotonic*, so if one probability is larger/smaller than another, then its logarithm is also larger/smaller than the other's. We therefore have that\n",
        "\\begin{equation}\n",
        "  y^{(n)} = \\underset{i}{\\arg\\max} \\,\\, p_i^{(n)} = \\underset{i}{\\arg\\max} \\, \\log p_i^{(n)} = \\underset{i}{\\arg\\max} \\,\\, l_i^{(n)}\n",
        "\\end{equation}\n",
        "\n",
        "See the next cell for code for constructing a deep network with one hidden layer that of ReLU's that outputs a vector of log probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2bOBnZ05X1m"
      },
      "source": [
        "# Deep network for classification\n",
        "class DeepNetSoftmax(nn.Module):\n",
        "  \"\"\"Deep Network with one hidden layer, for classification\n",
        "\n",
        "  Args:\n",
        "    n_inputs (int): number of input units\n",
        "    n_hidden (int): number of units in hidden layer\n",
        "    n_classes (int): number of outputs, i.e. number of classes to output\n",
        "      probabilities for\n",
        "\n",
        "  Attributes:\n",
        "    in_layer (nn.Linear): weights and biases of input layer\n",
        "    out_layer (nn.Linear): weights and biases of output layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden, n_classes):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden)  # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, n_classes)  # hidden units --> outputs\n",
        "    self.logprob = nn.LogSoftmax(dim=1)  # probabilities across columns should sum to 1 (each output row corresponds to a different input)\n",
        "\n",
        "  def forward(self, r):\n",
        "    \"\"\"Predict stimulus orientation bin from neural responses\n",
        "\n",
        "    Args:\n",
        "      r (torch.Tensor): n_stimuli x n_inputs tensor with neural responses to n_stimuli\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: n_stimuli x n_classes tensor with predicted class probabilities\n",
        "\n",
        "    \"\"\"\n",
        "    h = torch.relu(self.in_layer(r))\n",
        "    logp = self.logprob(self.out_layer(h))\n",
        "    return logp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2ZF7om55X1n"
      },
      "source": [
        "What should our loss function now be? Ideally, we want the probabilities outputted by our network to be such that the probability of the true stimulus class is high. One way to formalize this is to say that we want to maximize the *log* probability of the true stimulus class $\\tilde{y}^{(n)}$ under the class probabilities predicted by the network,\n",
        "\\begin{equation}\n",
        "  \\log \\left( \\text{predicted probability of stimulus } n \\text{ being of class } \\tilde{y}^{(n)} \\right) = \\log p^{(n)}_{\\tilde{y}^{(n)}} = l^{(n)}_{\\tilde{y}^{(n)}}\n",
        "\\end{equation}\n",
        "To turn this into a loss function to be *minimized*, we can then simply multiply it by -1: maximizing the log probability is the same as minimizing the *negative* log probability. Summing over a batch of $P$ inputs, our loss function is then given by\n",
        "\\begin{equation}\n",
        "  L = -\\sum_{n=1}^P \\log p^{(n)}_{\\tilde{y}^{(n)}} = -\\sum_{n=1}^P l^{(n)}_{\\tilde{y}^{(n)}}\n",
        "\\end{equation}\n",
        "In the deep learning community, this loss function is typically referred to as the **cross-entropy**, or **negative log likelihood**. The corresponding built-in loss function in PyTorch is `nn.NLLLoss()` (documentation [here](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html)).\n",
        "\n",
        "In the next cell, we've provided most of the code to train and test a network to decode stimulus orientations via classification, by minimizing the negative log likelihood. Fill in the missing pieces.\n",
        "\n",
        "Once you've done this, have a look at the plotted results. Does changing the loss function from mean squared error to a classification loss solve our problems? Note that errors may still occur -- but are these errors as bad as the ones that our network above was making?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGt2X5Di5X1n"
      },
      "source": [
        "def decode_orientation(n_classes, train_data, train_labels, test_data, test_labels):\n",
        "  \"\"\" Initialize, train, and test deep network to decode binned orientation from neural responses\n",
        "\n",
        "  Args:\n",
        "    n_classes (scalar): number of classes in which to bin orientation\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    test_data (torch.Tensor): n_test x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "\n",
        "  Returns:\n",
        "    (list, torch.Tensor): training loss over iterations, n_test x 1 tensor with predicted orientations of the\n",
        "      stimuli from decoding neural network\n",
        "  \"\"\"\n",
        "\n",
        "  # Bin stimulus orientations in training set\n",
        "  train_binned_labels = stimulus_class(train_labels, n_classes)\n",
        "\n",
        "  ##############################################################################\n",
        "  ## TODO for students: fill out missing pieces below to initialize, train, and\n",
        "  # test network\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: complete decode_orientation function\")\n",
        "  ##############################################################################\n",
        "\n",
        "  # Initialize network\n",
        "  net = ...  # use M=20 hidden units\n",
        "\n",
        "  # Initialize built-in PyTorch MSE loss function\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  # Run GD on training set data, using learning rate of 0.1\n",
        "  train_loss = ...\n",
        "\n",
        "  # Decode neural responses in testing set data\n",
        "  out = ...\n",
        "  out_labels = np.argmax(out.detach(), axis=1)  # predicted classes\n",
        "\n",
        "  return train_loss, out_labels\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "n_classes = 12  # start with 12, then (bonus) try making this as big as possible! does decoding get worse?\n",
        "\n",
        "# Uncomment below to test your function\n",
        "\n",
        "# Initialize, train, and test network\n",
        "#train_loss, predicted_test_labels = decode_orientation(n_classes, resp_train, stimuli_train, resp_test, stimuli_test)\n",
        "\n",
        "# Plot results\n",
        "#plot_decoded_results(train_loss, stimuli_test, predicted_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cktM55Og5X1n"
      },
      "source": [
        "# to_remove solution\n",
        "\n",
        "def decode_orientation(n_classes, train_data, train_labels, test_data, test_labels):\n",
        "  \"\"\" Initialize, train, and test deep network to decode binned orientation from neural responses\n",
        "\n",
        "  Args:\n",
        "    n_classes (scalar): number of classes in which to bin orientation\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    test_data (torch.Tensor): n_test x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "\n",
        "  Returns:\n",
        "    (list, torch.Tensor): training loss over iterations, n_test x 1 tensor with predicted orientations of the\n",
        "      stimuli from decoding neural network\n",
        "  \"\"\"\n",
        "\n",
        "  # Bin stimulus orientations in training set\n",
        "  train_binned_labels = stimulus_class(train_labels, n_classes)\n",
        "\n",
        "  # Initialize network\n",
        "  net = DeepNetSoftmax(n_neurons, 20, n_classes)  # use M=20 hidden units\n",
        "\n",
        "  # Initialize built-in PyTorch MSE loss function\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  # Run GD on training set data, using learning rate of 0.1\n",
        "  train_loss = train(net, loss_fn, train_data, train_binned_labels, learning_rate=0.1)\n",
        "\n",
        "  # Decode neural responses in testing set data\n",
        "  out = net(test_data)\n",
        "  out_labels = np.argmax(out.detach(), axis=1)  # predicted classes\n",
        "\n",
        "  return train_loss, out_labels\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "n_classes = 12  # start with 12, then (bonus) try making this as big as possible! does decoding get worse?\n",
        "\n",
        "# Initialize, train, and test network\n",
        "train_loss, predicted_test_labels = decode_orientation(n_classes, resp_train, stimuli_train, resp_test, stimuli_test)\n",
        "\n",
        "# Plot results\n",
        "with plt.xkcd():\n",
        "  plot_decoded_results(train_loss, stimuli_test, predicted_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFX4215l5X1o"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "We have now covered a number of common and powerful techniques for applying deep learning to decoding from neural data, some of which are common to almost any machine learning problem:\n",
        "* Building and training deep networks using the **PyTorch** `nn.Module` class and built-in **optimizers**\n",
        "* Choosing and evaluating **loss functions**\n",
        "* Testing a trained model on unseen data via **cross-validation**, by splitting the data into a **training set and testing set**\n",
        "\n",
        "An important aspect of this tutorial was the `train()` function we wrote in exercise 6. Note that it can be used to train *any* network to minimize *any* loss function (cf. advanced exercise) on *any* training data. This is the power of using PyTorch to train neural networks and, for that matter, **any other model**! There is nothing in the `nn.Module` class that forces us to use `nn.Linear` layers that implement neural network operations. You can actually put anything you want inside the `.__init__()` and `.forward()` methods of this class. As long as its parameters and computations involve only `torch.Tensor`'s, and the model is differentiable, you'll then be able to optimize the parameters of this model in exactly the same way we optimized the deep networks here.\n",
        "\n",
        "What kinds of conclusions can we draw from these sorts of analyses? If we can decode the stimulus well from visual cortex activity, that means that there is information about this stimulus available in visual cortex. Whether or not the animal uses that information to make decisions is not determined from an analysis like this. In fact mice perform poorly in orientation discrimination tasks compared to monkeys and humans, even though they have information about these stimuli in their visual cortex. Why do you think they perform poorly in orientation discrimination tasks?\n",
        "\n",
        "See this paper for some potential hypotheses (https://www.biorxiv.org/content/10.1101/679324v2), but this is totally an open question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDOpMLiM5X1p"
      },
      "source": [
        "---\n",
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXOAD9g-5X1p"
      },
      "source": [
        "## Neural network *depth*, *width* and *expressivity*\n",
        "\n",
        "Two important architectural choices that always have to be made when constructing deep feed-forward networks like those used here are\n",
        "* the number of hidden layers, or the network's *depth*\n",
        "* the number of units in each layer, or the layer *widths*\n",
        "\n",
        "Here, we restricted ourselves to networks with a single hidden layer with a width of $M$ units, but it is easy to see how this code could be adapted to arbitrary depths. Adding another hidden layer simply requires adding another `nn.Linear` module to the `__init__()` method and incorporating it into the `.forward()` method.\n",
        "\n",
        "The depth and width of a network determine the set of input/output transformations that it can perform, often referred to as its *expressivity*. The deeper and wider the network, the more *expressive* it is; that is, the larger the class of input/output transformations it can compute. In fact, it turns out that an infinitely wide *or* infinitely deep networks can in principle [compute (almost) *any* input/output transformation](https://en.wikipedia.org/wiki/Universal_approximation_theorem).\n",
        "\n",
        "A classic mathematical demonstration of the power of depth is given by the so-called [XOR problem](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b#:~:text=The%20XOr%2C%20or%20%E2%80%9Cexclusive%20or,value%20if%20they%20are%20equal.). This toy problem demonstrates how even a single hidden layer can drastically expand the set of input/output transformations a network can perform, relative to a shallow network with no hidden layers. The key intuition is that the hidden layer allows you to represent the input in a new format, which can then allow you to do almost anything you want with it. The *wider* this hidden layer, the more flexibility you have in this representation. In particular, if you have more hidden units than input units, then the hidden layer representation of the input is higher-dimensional than the raw data representation. This higher dimensionality effectively gives you more \"room\" to perform arbitrary computations in. It turns out that even with just this one hidden layer, if you make it wide enough you can actually approximate any input/output transformation you want. See [here](http://neuralnetworksanddeeplearning.com/chap4.html) for a neat visual demonstration of this.\n",
        "\n",
        "In practice, however, it turns out that increasing depth seems to grant more expressivity with fewer units than increasing width does (for reasons that are not well understood). It is for this reason that truly *deep* networks are almost always used in machine learning, which is why this set of techniques is often referred to as *deep* learning.\n",
        "\n",
        "That said, there is a cost to making networks deeper and wider. The bigger your network, the more parameters (i.e. weights and biases) it has, which need to be optimized! The extra expressivity afforded by higher width and/or depth thus carries with it (at least) two problems:\n",
        "* optimizing more parameters usually requires more data\n",
        "* a more highly parameterized network is more prone to overfit to the training data, so requires more sophisticated optimization algorithms to ensure generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbooAewB5X1p"
      },
      "source": [
        "## Gradient descent equations\n",
        "\n",
        "Here we provide the equations for the three steps of the gradient descent algorithm, as applied to our decoding problem:\n",
        "\n",
        "1. **Evaluate the loss** on the training data. For a mean squared error loss, this is given by\n",
        "\\begin{equation}\n",
        "    L = \\frac{1}{P}\\sum_{n=1}^P (y^{(n)} - \\tilde{y}^{(n)})^2\n",
        "\\end{equation}\n",
        "where $y^{(n)}$ denotes the stimulus orientation decoded from the population response $\\mathbf{r}^{(n)}$ to the $n$th stimulus in the training data, and $\\tilde{y}^{(n)}$ is the true orientation of that stimulus. $P$ denotes the total number of data samples in the training set. In the syntax of our `train()` function above, $\\mathbf{r}^{(n)}$ is given by `train_data[n, :]` and $\\tilde{y}^{(n)}$ by `train_labels[n]`.\n",
        "\n",
        "2. **Compute the gradient of the loss** with respect to each of the network weights. In our case, this entails computing the quantities\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial L}{\\partial \\mathbf{W}^{in}}, \\frac{\\partial L}{\\partial \\mathbf{b}^{in}}, \\frac{\\partial L}{\\partial \\mathbf{W}^{out}}, \\frac{\\partial L}{\\partial \\mathbf{b}^{out}}\n",
        "\\end{equation}\n",
        "Usually, we would require lots of math in order to derive each of these gradients, and lots of code to compute them. But this is where PyTorch comes to the rescue! Using a cool technique called [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), PyTorch automatically calculates these gradients when the `.backward()` function is called.\n",
        "\n",
        "  More specifically, when this function is called on a particular variable (e.g. `loss`, as above), PyTorch will compute the gradients with respect to each network parameter. These are computed and stored behind the scenes, and can be accessed through the `.grad` attribute of each of the network's parameters. As we saw above, however, we actually never need to look at or call these gradients when implementing gradient descent, as this can be taken care of by PyTorch's built-in optimizers, like `optim.SGD`.\n",
        "\n",
        "3. **Update the network weights** by descending the gradient:\n",
        "\\begin{align}\n",
        "    \\mathbf{W}^{in} &\\leftarrow \\mathbf{W}^{in} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}^{in}} \\\\\n",
        "    \\mathbf{b}^{in} &\\leftarrow \\mathbf{b}^{in} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{b}^{in}} \\\\\n",
        "    \\mathbf{W}^{out} &\\leftarrow \\mathbf{W}^{out} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}^{out}} \\\\\n",
        "    \\mathbf{b}^{out} &\\leftarrow \\mathbf{b}^{out} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{b}^{out}}\n",
        "\\end{align}\n",
        "where $\\alpha$ is called the **learning rate**. This **hyperparameter** of the SGD algorithm controls how far we descend the gradient on each iteration. It should be as large as possible so that fewer iterations are needed, but not too large so as to avoid parameter updates from skipping over minima in the loss landscape.\n",
        "\n",
        "While the equations written down here are specific to the network and loss function considered in this tutorial, the code provided above for implementing these three steps is completely general: no matter what loss function or network you are using, exactly the same commands can be used to implement these three steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py4wsNWL5X1p"
      },
      "source": [
        "## *Stochastic* gradient descent (SGD) vs. gradient descent (GD)\n",
        "\n",
        "In this tutorial, we used the gradient descent algorithm, which differs in a subtle yet very important way from the more commonly used **stochastic gradient descent (SGD)** algorithm. The key difference is in the very first step of each iteration, where in the GD algorithm we evaluate the loss *at every data sample in the training set*. In SGD, on the other hand, we evaluate the loss only at a random subset of data samples from the full training set, called a **mini-batch**. At each iteration, we randomly sample a mini-batch to perform steps 1-3 on. All the above equations still hold, but now the $P$ data samples $\\mathbf{r}^{(n)}, \\tilde{y}^{(n)}$ denote a mini-batch of $P$ random samples from the training set, rather than the whole training set.\n",
        "\n",
        "There are several reasons why one might want to use SGD instead of GD. The first is that the training set might be too big, so that we actually can't actually evaluate the loss on every single data sample in it. In this case, GD is simply infeasible, so we have no choice but to turn to SGD, which bypasses the restrictive memory demands of GD by sub-sampling the training set into smaller mini-batches.\n",
        "\n",
        "But, even when GD is feasible, SGD turns out to be generally better. The stochasticity induced by the extra random sampling step in SGD effectively adds some noise in the search for local minima of the loss function. This can be really useful for avoiding potential local minima, and enforce that whatever minimum is converged to is a good one. This is particularly important when networks are wider and/or deeper, in which case the large number of parameters can lead to overfitting.\n",
        "\n",
        "Here, we used only GD because (1) it is simpler, and (2) it suffices for the problem being considered here. Because we have so many neurons in our data set, decoding is not too challenging and doesn't require a particularly deep or wide network. The small number of parameters in our deep networks therefore can be optimized without a problem using GD."
      ]
    }
  ]
}